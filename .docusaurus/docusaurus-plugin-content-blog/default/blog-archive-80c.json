{
  "blogPosts": [
    {
      "id": "/2022/07/10/mathjax-katex",
      "metadata": {
        "permalink": "/blog/2022/07/10/mathjax-katex",
        "source": "@site/blog/2022-07-10-mathjax-katex.md",
        "title": "How use math equations in Docusaurus",
        "description": "Learn how to convert MathJax for Jekyll to KaTeX for Docusaurus for using math equations in text.",
        "date": "2022-07-10T00:00:00.000Z",
        "formattedDate": "July 10, 2022",
        "tags": [
          {
            "label": "docusaurus",
            "permalink": "/blog/tags/docusaurus"
          },
          {
            "label": "jekyll",
            "permalink": "/blog/tags/jekyll"
          },
          {
            "label": "markdown",
            "permalink": "/blog/tags/markdown"
          },
          {
            "label": "tutorials",
            "permalink": "/blog/tags/tutorials"
          }
        ],
        "readingTime": 1.625,
        "truncated": true,
        "authors": [],
        "frontMatter": {
          "title": "How use math equations in Docusaurus",
          "description": "Learn how to convert MathJax for Jekyll to KaTeX for Docusaurus for using math equations in text.",
          "tags": [
            "docusaurus",
            "jekyll",
            "markdown",
            "tutorials"
          ]
        },
        "nextItem": {
          "title": "How to connect Google Sheets with MySQL",
          "permalink": "/blog/2022/07/07/connect-mysql-sheets"
        }
      },
      "content": "Learn how to convert MathJax for Jekyll to KaTeX for Docusaurus for using math equations in text.\n\n<!--truncate-->\n\nOne of the challenges I encountered when moving my website from Jekyll to Docusaurus was rendering math equations in some of my blog posts (for example, about [the apriori algorithm](blog/2021-10-17-market-basket-analysis-with-apriori-algorithm.mdx)). In Jekyll, I used [MathJax](https://docs.mathjax.org/en/latest/index.html#) with [Liquid](https://shopify.github.io/liquid/) to render them. This didn't work in Docusaurus, so instead I had to use [KaTeX](https://katex.org/).\n\nThe [Docusaurus docs](https://docusaurus.io/docs/markdown-features/math-equations) provide pretty clear instructions on how to use KaTeX in your project. This post is a more straightforward tutorial, and reflects my experience (and issues) specific to my website.\n\n\n1. Install the `remark-math` and `rehype-katex` plugins (in my case, with npm):\n   \n    `npm install --save remark-math@3 rehype-katex@5 hast-util-is-element@1.1.0`\n\n2. Open your project's configuration file `docusaurus.config.js`:\n   \n   - **At the top** (right after the lines importing the Docusaurus themes), add the two plugins:\n\n        ```javascript\n        const math = require('remark-math');\n        const katex = require('rehype-katex');\n        ```\n    \n    - **In presets**, under both the docs and blog categories, add the two plugins:\n\n        ```javascript\n        remarkPlugins: [math],\n        rehypePlugins: [katex],\n        ```\n\n        Make sure to add the plugins under all page categories where you want math equations to be rendered. I initially added them only under docs, and was confused why equations were rendered on my About page, but not in blog posts, until I figured out the reason.\n\n    - In stylesheets, add: the source of KaTeX:\n\n        ```javascript\n          stylesheets: [\n            {\n            href: 'https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css',\n            type: 'text/css',\n            integrity:\n                'sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM',\n            crossorigin: 'anonymous',\n            },\n        ],\n        ```\n\n3. Update the formulas in existing content, considering the differences between Jekyll vs. Docusaurus, and Mathjax vs. KaTeX.\n   \n   - In Jekyll, formulas are wrapped in double `$$`, whereas for Docusaurus they're wrapped in single `$`. Replace all instances of `$$` with `$`.\n   - If some equations are still not displayed correctly, it might be because they need a different syntax in KaTeX than in MathJax. Check this in the [KaTeX docs of suported functions](https://katex.org/docs/supported.html) and test the equations in their [online editor](https://katex.org/#demo)."
    },
    {
      "id": "/2022/07/07/connect-mysql-sheets",
      "metadata": {
        "permalink": "/blog/2022/07/07/connect-mysql-sheets",
        "source": "@site/blog/2022-07-07-connect-mysql-sheets.md",
        "title": "How to connect Google Sheets with MySQL",
        "description": "Discover two ways to connect MySQL with Google Sheets, and learn to build no-code workflows for automating it.",
        "date": "2022-07-07T00:00:00.000Z",
        "formattedDate": "July 7, 2022",
        "tags": [
          {
            "label": "n8n",
            "permalink": "/blog/tags/n-8-n"
          },
          {
            "label": "tutorials",
            "permalink": "/blog/tags/tutorials"
          }
        ],
        "readingTime": 4.64,
        "truncated": true,
        "authors": [],
        "frontMatter": {
          "title": "How to connect Google Sheets with MySQL",
          "description": "Discover two ways to connect MySQL with Google Sheets, and learn to build no-code workflows for automating it.",
          "tags": [
            "n8n",
            "tutorials"
          ],
          "canonical-url": "https://blog.n8n.io/google-sheets-to-mysql/"
        },
        "prevItem": {
          "title": "How use math equations in Docusaurus",
          "permalink": "/blog/2022/07/10/mathjax-katex"
        },
        "nextItem": {
          "title": "How to automatically import JSON into Google Sheets",
          "permalink": "/blog/2022/06/30/import-json-spreadsheet"
        }
      },
      "content": "Discover two ways to connect MySQL with Google Sheets, and learn to build no-code workflows for automating it.\n\n<!--truncate-->\n\nIf you've been struggling to connect your MySQL database with Google Sheets, you've come to the right place.\n\nEstablishing a connection from Google Sheets to MySQL allows you to avoid manually updating data in one source and importing/exporting data sets, thus reducing human errors.\n\nConnecting Google Sheets with MySQL is also a way to limit access to a data source, while giving access to that data in another source. For example, if your company stores data about sales orders, but the sales representatives don't feel comfortable with querying the database, you can connect the database to Google Sheets, so they read and analyze the data in the spreadsheet.\n\nIn this tutorial, we'll show you how to sync Google spreadsheet with MySQL database in two ways: the hard code way using Apps Script, and the no-code way using n8n workflows.\n\n\n## Can you connect Google Sheets to a database?\n\nYes, you can connect Google Sheets to a database like MySQL.\n\nEstablishing a connection between these two systems allows you to avoid manually updating data in one source and importing/exporting data sets, thus reducing human errors. You can also take it one step further and [synchronize data (one-way or two-way)](https://blog.n8n.io/how-to-sync-data-between-two-systems/) between a spreadsheet and a database. \n\nConnecting Google Sheets with MySQL is also a way to limit access to a data source, while giving access to that data in another source. For example, if your company stores data about sales orders, but the sales representatives don't feel comfortable with querying the database, you can connect the database to Google Sheets, so they read and analyze the data in the spreadsheet.\n\n## Method 1: Google Apps Script to connect MySQL to Google Sheets\n\n[**Apps Script**](https://developers.google.com/apps-script) is the cloud-based JavaScript platform from Google. With Apps Script, you can integrate with and automate tasks across Google products. For example, you can create scripts to connect MySQL to Google Sheets.\n\n1. Set up your MySQL database and get the connectivity data:\n\n  - Server\n  - Database name\n  - Username\n  - Password\n  - Port Number\n\n2. In your Google Sheet, select Extensions > Apps Script.  \n  This opens a new untitled project in Apps Script in your browser. The project includes a place-holder function in the code editor.\n3. In the code editor, replace the place-holder function with the JavaScript code for creating a connection with MySQL. For example, you can use this [public code](https://gist.github.com/mike-seekwell/c54c62e1ba7560583a84b9fe4c1fd157).\n4. Save the project under a descriptive name (e.g. *MySQL Connection*).\n5. Run the script by pressing the Run Script icon. Note: The first time you run the script you will be asked to provide authorization to Google.\n\n\n## Method 2: n8n automated workflows to connect MySQL to Google Sheets\n\nNext, we'll show you two workflows that automatically import data from Google Sheets into MySQL, and the other way around.\n\n![mysql workflows](./blog_images/n8n_mysql_workflows.png)\n\n**Prerequisites for building the workflows:**\n\n- [**n8n set up**](https://n8n.io/pricing). The easiest way to get started is to [download the free desktop app](https://docs.n8n.io/hosting/installation/desktop-app/), but you can also [sign up for n8n Cloud](https://docs.n8n.io/hosting/installation/cloud/) or [self-host n8n](https://docs.n8n.io/hosting/installation/docker/).\n- **A MySQL database and [credentials](https://docs.n8n.io/integrations/credentials/mysql/)**. You need these to use the [MySQL integration](https://n8n.io/integrations/109-mysql/) to access the MySQL API.\n- **A Google account and [credentials](https://docs.n8n.io/integrations/credentials/google/)**. You need these to use the [Google Sheets integration](https://n8n.io/integrations/18-google-sheets-/) to access the Google Sheets API. The API is free to use, but beware of [usage quotas and limitations](https://developers.google.com/sheets/api/limits).\n\n### Import data from Google Sheets into MySQL\n\n[This workflow](https://app.n8n.io/workflows/1752) automatically imports data from Google Sheets into a MySQL database every week.\n\n1. **Cron node** executes the workflow every Monday at 5am. You can change the time interval by tweaking the following parameters:\n\n- Mode: Every Week\n- Hour: 5\n- Minute: 0\n- Weekday: Monday\n   \n\n2. **Google Sheets node** reads data from a spreadsheet.\n\n3. **MySQL node** inserts the data from Google Sheets into a MySQL table.\n  Make sure that the column names in Google Sheets and the MySQL table match.\n\n### Import data from MySQL into Google Sheets\n\n[This workflow](https://app.n8n.io/workflows/1753) automatically imports data from a MySQL database into Google Sheets. The node configuration is similar to the previous workflow, \n\n1. **Cron node** executes the workflow at regular preset time intervals.\n2. **MySQL node** selects all book titles and prices from the books table in the database. You can change what data you want to extract from the table by tweaking the following parameters:\n   \n  - Query: `SELECT * FROM books;`\n\n3. **Google Sheets node** appends the data from the MySQL table to a sheet.\n  Alternatively, you can update data or create new records in the the sheet by selecting the operation *Create or Update*. Note that in this case, you need to have a key column (for example, a unique ID) based on which to compare the new and existing data.\n\n## What’s next?\n\nIn this post, you learned how to connect MySQL with Google Sheets in two different ways: using Google App Script and n8n workflows. Now you two workflow templates for connecting your database with spreadsheets, making your work more efficient. \n\nHere's what you can do next:\n\n- Read more tutorials about importing data into Google Sheets, for example [how to import JSON from an API and local files](https://blog.n8n.io/google-sheets-import-json/).\n- Check out the [workflow page](https://n8n.io/workflows/) for more automation ideas with Google Sheets and MySQL. For example, you can use add MySQL nodes to power up these workflows that [sync data between multiple Google Sheets](https://n8n.io/workflows/6/) or [get data via a REST API](https://n8n.io/workflows/226/).\n\n\n> This post was originally published on the [n8n blog](https://blog.n8n.io/google-sheets-to-mysql/)."
    },
    {
      "id": "/2022/06/30/import-json-spreadsheet",
      "metadata": {
        "permalink": "/blog/2022/06/30/import-json-spreadsheet",
        "source": "@site/blog/2022-06-30-import-json-spreadsheet.md",
        "title": "How to automatically import JSON into Google Sheets",
        "description": "Discover three ways to import JSON data into Google Sheets or a CSV file, and five no-code workflows for automating these tasks.",
        "date": "2022-06-30T00:00:00.000Z",
        "formattedDate": "June 30, 2022",
        "tags": [
          {
            "label": "n8n",
            "permalink": "/blog/tags/n-8-n"
          },
          {
            "label": "tutorials",
            "permalink": "/blog/tags/tutorials"
          }
        ],
        "readingTime": 7.69,
        "truncated": true,
        "authors": [],
        "frontMatter": {
          "title": "How to automatically import JSON into Google Sheets",
          "description": "Discover three ways to import JSON data into Google Sheets or a CSV file, and five no-code workflows for automating these tasks.",
          "tags": [
            "n8n",
            "tutorials"
          ],
          "canonical-url": "https://blog.n8n.io/google-sheets-import-json/"
        },
        "prevItem": {
          "title": "How to connect Google Sheets with MySQL",
          "permalink": "/blog/2022/07/07/connect-mysql-sheets"
        },
        "nextItem": {
          "title": "How to automate database activity monitoring and alerting",
          "permalink": "/blog/2022/06/28/database-activity-monitoring"
        }
      },
      "content": "Discover three ways to import JSON data into Google Sheets or a CSV file, and five no-code workflows for automating these tasks.\n\n<!--truncate-->\n\n[JavaScript Object Notation (JSON)](https://developer.mozilla.org/en-US/docs/Learn/JavaScript/Objects/JSON) is a standard text-based format that is used to store and represent structured data. JSON is commonly used in web applications, and is one of the most popular [API](https://blog.n8n.io/what-are-apis-how-to-use-them-with-no-code/) formats.\n\nData in JSON format is not practical if you want to share that data in a user-friendly form, or perform data analysis on a data set. Most commonly, you'll need to import JSON into a spreadsheet.\n\nIn this post, we'll show you three ways to import JSON data into Google Sheets or a CSV file, and five no-code workflows for automating these tasks.\n\n## How to get data from JSON to Google Sheets?\n\nThere are different ways to import JSON data into Google Sheets. You can use:\n\n- Custom code\n- Apps Script\n- No-code tools\n\n### Using custom code\n\nYou can write code in your preferred programming language to parse JSON data. This offers you freedom, flexibility, and control over your script. For example, you can extract certain data points and perform data analysis, along with transferring it into Google Sheets. The downside is that, depending on the use case, writing a custom script can be overly complicated.\n\nFor example, if Python is your go-to language, you can follow [this quickstart guide](https://developers.google.com/sheets/api/quickstart/python) for creating a command-line application that makes requests to the Google Sheets API, or use [this Python script](https://www.geeksforgeeks.org/convert-json-to-csv-in-python/) to export JSON data into a CSV file.\n\n### Using Apps Script\n\n[**Apps Script**](https://developers.google.com/apps-script) is the cloud-based JavaScript platform from Google. With Apps Script, you can integrate with and automate tasks across Google products. For example, you can create your own `=IMPORTJSON()` formula for Google Sheets to parse JSON data. However, this use is limited to the Google ecosystem, plus you need to know how to code in JavaScript.\n\nHere's **how to use a script in Google Sheets**:\n\n1. In your Google Sheet, select Extensions > Apps Script.  \n  This opens a new untitled project in Apps Script in your browser. The project includes a place-holder function in the code editor.\n2. In the code editor, replace the place-holder function with the code for creating a function to import JSON. For example, you can use [this public script](https://gist.github.com/paulgambill/cacd19da95a1421d3164).\n3. Save the project under a descriptive name, for example *Import JSON*.\n   Now you can use this function as a custom formula in Google Sheets.\n4. In Google Sheets, select a cell where you want to import JSON data, and type the formula you created `=IMPORTJSON()`.\n\n### Using no-code tools\n\nNo-code tools, such as n8n, require no coding skills. Compared to the previous two options, no-code tools are easier to use, yet allow you to build complex workflow automations. You can use [core nodes](https://docs.n8n.io/integrations/core-nodes/) for getting data from an API and converting data between binary and JSON format, and [integrations](https://n8n.io/integrations/) for transferring the data between apps and services, such as Google Sheets or Gmail.\n\n## No-code n8n workflows to import JSON to Google Sheets\n\nIn the following sections, we'll show you NUMBER workflows for importing JSON data into Google Sheets and CSV files, and the other way around. You can copy-paste these workflows and make minor changes to implement them for your use cases.\n\n**Prerequisites for building the workflows**\n\n- [**n8n set up**](https://n8n.io/pricing). The easiest way to get started is to [download the free desktop app](https://docs.n8n.io/hosting/installation/desktop-app/), but you can also [sign up for n8n Cloud](https://docs.n8n.io/hosting/installation/cloud/) or [self-host n8n](https://docs.n8n.io/hosting/installation/docker/).\n- **A Google account and [credentials](https://docs.n8n.io/integrations/credentials/google/)**. You need these to use the [Google Sheets integration](https://n8n.io/integrations/18-google-sheets-/) to access the Google Sheets API. The API is free to use, but beware of rate limits.\n- Basic understanding of **APIs and JSON**. This would help you understand how to make GET requests and interpret data in JSON format.\n\n### Workflow to import JSON from an API into Google Sheets\n\n[This workflow](https://app.n8n.io/workflows/1737) gets data from an API and stores it into Google Sheets. Feel free to copy-paste the workflow in your n8n editor, and continue reading the instructions to understand how it works and how you can tweak it.\n\n![workflow](./blog_images/importjson_workflow_apisheetscsv.png)\n\nThe workflow consists of three nodes:\n\n1. [**HTTP Request node**](https://docs.n8n.io/integrations/core-nodes/n8n-nodes-base.httprequest/) makes a GET request to the [Random User API](https://randomuser.me/api/).\n\n  If you access the API link in your browser, you should get information about a fictional person in JSON format, which looks like this:\n\n  ```\n  {\"results\":[{\"gender\":\"male\",\"name\":{\"title\":\"Mr\",\"first\":\"Giorgio\",\"last\":\"Jeuring\"},\"location\":{\"street\":{\"number\":9927,\"name\":\"Beckershagen\"},\"city\":\"Winneweer\",\"state\":\"Drenthe\",\"country\":\"Netherlands\",\"postcode\":59748,\"coordinates\":{\"latitude\":\"47.5171\",\"longitude\":\"-140.6882\"},\"timezone\":{\"offset\":\"-6:00\",\"description\":\"Central Time (US & Canada), Mexico City\"}},\"email\":\"giorgio.jeuring@example.com\",\"login\":{\"uuid\":\"553f1f08-61d6-48fc-b9d7-5f21f844f0ce\",\"username\":\"ticklishladybug384\",\"password\":\"mondeo\",\"salt\":\"yfYyG0YB\",\"md5\":\"7f53e13d5002fbc2005bea41767dc68e\",\"sha1\":\"255e2838819b92038ba4ccb04a38f2aa93aba868\",\"sha256\":\"5a3532ccc6fb3bde566b5e38023a509cefdd0fa9214508ac08141c7e759677dc\"},\"dob\":{\"date\":\"1997-01-01T03:37:12.559Z\",\"age\":26},\"registered\":{\"date\":\"2004-08-28T03:29:17.926Z\",\"age\":18},\"phone\":\"(952)-290-7836\",\"cell\":\"(367)-506-5505\",\"id\":{\"name\":\"BSN\",\"value\":\"65540109\"},\"picture\":{\"large\":\"https://randomuser.me/api/portraits/men/10.jpg\",\"medium\":\"https://randomuser.me/api/portraits/med/men/10.jpg\",\"thumbnail\":\"https://randomuser.me/api/portraits/thumb/men/10.jpg\"},\"nat\":\"NL\"}],\"info\":{\"seed\":\"6176dc854625bdf5\",\"results\":1,\"page\":1,\"version\":\"1.3\"}}\n  ```\n\n  In the n8n workflow, the execution of the HTTP Request node returns the result like this:\n\n  ![http request node result](./blog_images/importjson_httprequestnode.png)\n\n  This result includes a lot of information, but you probably don't need all of it, and want to save only the person's name and country.\n\n2. [**Set node**](https://docs.n8n.io/integrations/core-nodes/n8n-nodes-base.set/) sets the values you want to store from all the data you get from the API. In this case, it sets values for the name and country of the fictional person.\n\n  ![set node result](./blog_images/importjson_setnode.png)\n\n  This is the data that you have to store in Google Sheets.\n\n3. [**Google Sheets node**](https://docs.n8n.io/integrations/nodes/n8n-nodes-base.googlesheets/) appends the set data to a sheet.  \n   Before executing the node, go to the sheet you want to import data into and create two columns with the names of the values set in the Set node (*name* and *country*). Then, when you execute the node, you should see the name and country data being filled in under the column names in the sheet.\n\n### Workflow to export JSON data from an API to a local CSV file\n\nInstead of (or in addition to) importing JSON data into Google Sheets, you can  export the data to a local CSV file. To do this, you need to replace the Google Sheets node with the Spreadsheet File node (or connect a Spreadsheet File node to the Set node, if you want to save data in both Google Sheets and a local CSV).\n\nWhen you execute the Spreadsheet node, it should return a CSV file that you can view and download.\n\n![spreadsheet node](./blog_images/importjson_spreadsheetnode.png)\n\n### Workflow to convert a CSV file to a JSON file\n\n[This workflow](https://app.n8n.io/workflows/1732) converts a local CSV file to a JSON file. The catch here is to convert the data from JSON (the format in which the contents of the CSV file are returned) to binary (the format in which the JSON file is written).\n\n![workflow](./blog_images/importjson_workflow_csvfilejsonfile.png)\n\n1. [**Read Binary File node**](https://docs.n8n.io/integrations/core-nodes/n8n-nodes-base.readbinaryfile/) reads the local CSV file.\n2. [**Spreadsheet File node**](https://docs.n8n.io/integrations/core-nodes/n8n-nodes-base.spreadsheetfile/) reads the content of the CSV file.\n3. [**Move Binary Data node**](https://docs.n8n.io/integrations/core-nodes/n8n-nodes-base.movebinarydata/) converts the data from JSON to binary format.\n4. [**Write Binary Data node**](https://docs.n8n.io/integrations/core-nodes/n8n-nodes-base.writebinarydata/) creates a JSON file that contains the informations from the original CSV file. You can then download the JSON file locally.\n\n### Workflow to import a local JSON file into Google Sheets\n\nFor this example, you can copy the JSON code from the HTTP Request node and paste it in a local json file named `users.json`.\n\n```json\n{\n  \"results\": [\n    {\n      \"gender\": \"male\",\n      \"name\": {\n        \"title\": \"Mr\",\n        \"first\": \"Daniel\",\n        \"last\": \"Young\"\n      },\n      \"location\": {\n        \"street\": {\n          \"number\": 4689,\n          \"name\": \"Vimy St\"\n        },\n        \"city\": \"Woodstock\",\n        \"state\": \"Prince Edward Island\",\n        \"country\": \"Canada\",\n        \"postcode\": \"R6Q 7U7\",\n        \"coordinates\": {\n          \"latitude\": \"-60.5814\",\n          \"longitude\": \"-131.6670\"\n        },\n        \"timezone\": {\n          \"offset\": \"+10:00\",\n          \"description\": \"Eastern Australia, Guam, Vladivostok\"\n        }\n      },\n      \"email\": \"daniel.young@example.com\",\n      \"login\": {\n        \"uuid\": \"3ff7efb9-fdd1-42cd-b669-0d737530ad5f\",\n        \"username\": \"happymeercat108\",\n        \"password\": \"gofast\",\n        \"salt\": \"91ZgiW1o\",\n        \"md5\": \"9181752b52eca6ba4024a809506963d3\",\n        \"sha1\": \"35874f1666924b9f90403852154bbb3a4b5c784c\",\n        \"sha256\": \"8af1b73aed0890c75d76501317bdcda58cd8d82d3090f5fbbe88c4f34a0a24e8\"\n      },\n      \"dob\": {\n        \"date\": \"1991-09-16T14:12:51.147Z\",\n        \"age\": 31\n      },\n      \"registered\": {\n        \"date\": \"2010-09-10T04:22:34.603Z\",\n        \"age\": 12\n      },\n      \"phone\": \"431-068-3900\",\n      \"cell\": \"761-641-1531\",\n      \"id\": {\n        \"name\": \"\",\n        \"value\": null\n      },\n      \"picture\": {\n        \"large\": \"https://randomuser.me/api/portraits/men/24.jpg\",\n        \"medium\": \"https://randomuser.me/api/portraits/med/men/24.jpg\",\n        \"thumbnail\": \"https://randomuser.me/api/portraits/thumb/men/24.jpg\"\n      },\n      \"nat\": \"CA\"\n    }\n  ],\n  \"info\": {\n    \"seed\": \"a18f2fe366e8b0f6\",\n    \"results\": 1,\n    \"page\": 1,\n    \"version\": \"1.3\"\n  }\n}\n\n```\n\nTo import the JSON data into Google Sheets (or a CSV file), you can use [this workflow](https://app.n8n.io/workflows/1736) that consists of only three nodes.\n\n![workflow](./blog_images/importjson_workflow_jsonfilecsvfile.png)\n\n1. **Read Binary File node** reads the local CSV file.\n2. **Move Binary Data node** converts the data from binary to JSON format.\n3. a) **Google Sheets node** appends the data to a spreadsheet.\n   b) **Spreadsheet File node** writes the data to a CSV file that you can view and download.\n\n![workflow](./blog_images/importjson_workflow_jsonfilesheets.png)\n\n### Workflow to import a JSON file from Gmail into Google Sheets\n\nIt's also possible to import data into Google Sheets (or a CSV file) from JSON files sent as attachments via email.\n\n[This workflow](https://app.n8n.io/workflows/1734) does just that using the [**Gmail node**](https://docs.n8n.io/integrations/nodes/n8n-nodes-base.gmail/), which reads the contents of an email, including the attached JSON file. From there, the configuration of the Move Binary Data and Google Sheets nodes is similar to the one in the previous workflows.\n\n![workflow](./blog_images/importjson_workflow_gmailsheets.png)\n\n## What's next?\n\nIn this post, you learned how to import JSON into Google Sheets in different ways: using custom code, the App Script, and n8n workflows. Now you have several workflow templates for converting data between JSON and CSV, and importing it into Google Sheets automatically. \n\nHere's what you can do next:\n\n- Read more tutorials about importing data into Google Sheets, for example [example](https://blog.n8n.io//).\n- Check out the [workflow page](https://n8n.io/workflows/) for more automation ideas.\n- [Subscribe to our newsletter](https://n8n.io/blog/#subscribe) to get the latest news around workflow automation with n8n.\n- Join the [community forum](https://community.n8n.io/) and follow us on [Twitter](https://twitter.com/n8n_io).\n\n\n> This post was originally published on the [n8n blog](https://blog.n8n.io/google-sheets-import-json/)."
    },
    {
      "id": "/2022/06/28/database-activity-monitoring",
      "metadata": {
        "permalink": "/blog/2022/06/28/database-activity-monitoring",
        "source": "@site/blog/2022-06-28-database-activity-monitoring.md",
        "title": "How to automate database activity monitoring and alerting",
        "description": "Learn what database activity monitoring is, why it's important, and how to automatically monitor a Postgres database containing IoT data with n8n workflows.",
        "date": "2022-06-28T00:00:00.000Z",
        "formattedDate": "June 28, 2022",
        "tags": [
          {
            "label": "n8n",
            "permalink": "/blog/tags/n-8-n"
          },
          {
            "label": "tutorials",
            "permalink": "/blog/tags/tutorials"
          }
        ],
        "readingTime": 8.595,
        "truncated": true,
        "authors": [],
        "frontMatter": {
          "title": "How to automate database activity monitoring and alerting",
          "description": "Learn what database activity monitoring is, why it's important, and how to automatically monitor a Postgres database containing IoT data with n8n workflows.",
          "tags": [
            "n8n",
            "tutorials"
          ],
          "canonical-url": "https://n8n.io/blog/database-activity-monitoring"
        },
        "prevItem": {
          "title": "How to automatically import JSON into Google Sheets",
          "permalink": "/blog/2022/06/30/import-json-spreadsheet"
        },
        "nextItem": {
          "title": "Resources for technical writers",
          "permalink": "/blog/2022/06/26/resources-technical-writers"
        }
      },
      "content": "Learn what database activity monitoring is, why it's important, and how to automatically monitor a Postgres database containing IoT data with n8n workflows.\n\n<!--truncate-->\n\nDatabase activity monitoring (DAM) is increasingly valued in organizations that work with data, as it helps ensure that mission-critical processes run optimally and issues are detected on time. Established and new companies are offering a variety of DAM tools and solutions, tailored to the needs of customers across industries.\n\nThe global DAM software market is projected to grow [by 15% by 2029](https://www.fortunebusinessinsights.com/database-monitoring-software-market-106413), with DAM tools looking to [leverage machine learning](https://www.smartdatacollective.com/database-activity-monitoring-security-investment-that-pays-off/) in their solutions. These trends are fueled largely by a sharper focus on data privacy regulations, an increased risk of cyber-attacks. In this context, DAM is a worthwhile investment.\n\nIn this article, you'll learn about database activity monitoring: what it is, how it works, why it's important, and what to monitor in your database. Moreover, you'll learn how to build an n8n workflow that automatically monitors sensor activity data in a Postgres database and sends SMS via Twilio when it detects outliers.\n\n## What does database activity monitoring mean?\n\n**Database activity monitoring (DAM) is the process of observing activities of a database and reporting suspicious activity.** [Gartner defines DAM](https://www.gartner.com/en/information-technology/glossary/database-activity-monitoring-dam) as \"a suite of tools that can be used to support the ability to identify and report on fraudulent, illegal or other undesirable behavior, with minimal impact on user operations and productivity\".\n\nDAM tools (or workflows) can track user actions and changes made to a database continuously and in real-time, and then send notificaitions or alerts to a specified channel or person. Thus, DAM is a way of ensuring compliance in data processing, strengthening database security, and offering more visibility into database activities.\n\n## Why is database activity monitoring important?\n\nEssentially, database activity monitoring:\n\n- Increases productivity and efficiency in the responsible team.\n- Strengthens database security, along with data privacy and compliance.\n- Helps you improve and optimize the performance of your database.\n- Offers more insight into database performance and health metrics.\n\nEspecially is the context of remote work, which involves using personal devices, home connections, and maybe even unverified/insecure networks, [DAM is more important than ever](https://securityintelligence.com/posts/managed-data-activity-monitoring-dam-is-more-important-than-ever/).\n\n## What should be monitored in a database?\n\n**The most important issues you should monitor in a database boil down to database performance and user activity.** Specifically, these can be:\n\n- **Suspicious user activity:** Users with admin rights (for example, database administrators or system administrators) can abuse their privileges to access sensitive data or compromise the database. Abnormal database activity can also point to bad actors inside or outside of an organization who attempt data breaches.\n- **Database changes:** Whether intentional or accidental, changes to a database (for example, schema updates) can impact the performance, and it's important to review them on time.\n- **Query performance:** Slow or overly complicated queries can take a long time to execute, thus decreasing performance.\n- **Failures and errors:** System outages, user errors (for example, if a user accidentally deletes a row in a table), or failed queries are just some examples of situations that can (severely) impact the data and performance of a database.\n\n## How do you monitor a database?\n\nYou can monitor a database by looking into its [**transaction logs**](https://datatechnologytoday.wordpress.com/2014/02/10/the-log-is-the-database/#:~:text=The%20database%20log%2C%20sometimes%20referred,which%20changes%20to%20the%20database.), using **DAM tools**, or setting up **workflow automations** for specific issues you want to be keep an eye on.\n\nThe process of monitoring database activity usually consists of three main steps:\n\n1. Decide what you want to monitor in the database.\n2. Set up the tool/workflow for monitoring the database.\n3. Define the follow-up action to take when an issue is detected in the database.\n\n## Tools for database activity monitoring\n\nNowadays, many established and new companies are offering a variety of DAM tools and solutions, tailored to the needs of customers across industries.\n\nHere are 10 open-source and paid tools for DAM:\n\n- **DataDog** is an enterprise observability service that provides monitoring for databases through a data analytics platform.\n- **Dynatrace** is a software intelligence platform that provides database monitoring services based on artificial intelligence and automation.\n- **Icinga** is an open-source computer system and network monitoring application that covers all aspects of monitoring, including database functionality and alerting.\n- **ManageEngine** offers enterprise IT management software, including monitoring for both SQL and NoSQL databases.\n- **NewRelic** offers cloud-based software for website and application monitoring.\n- **Prometheus** is an open-source monitoring and alerting tool that allows capturing time-series data as metrics.\n- **Sensu** is an open source tool that delivers monitoring as code on any cloud.\n- **SigNoz** is an open-source tool for monitoring the entire software system, including the performance of database calls.\n- **Site24x7** offers both free and paid monitoring services for the entire IT environment.\n- **SolarWinds** offers performance and availability monitoring for websites, applications, and servers, including Database Performance Monitoring.\n\nAnother option is to use n8n for building your own custom database monitoring workflow. This way, you can take DAM one step further and send alerts to different channels, route them based on conditional logic, and integrate other popular apps or services, for example for [DevSecOps use cases](https://blog.n8n.io/devsecops-tools/).\n\n\n## n8n workflow for database activity monitoring\n\nNow that you know the basics of database activity monitoring, let's build an n8n workflow that automatically monitors a Postgres database and sends an alerts about outlier values. For this use case, we'll generate, store, and monitor sensor data (like humidity, temperature, pressure) in a Postgres database.\n\nThis automation use case consists of two workflows:\n\n1. [**Workflow 1**](https://n8n.io/workflows/356/) generates data and creates a database record every minute. This is a way to simulate sensor readings being ingested by a database.\n2. [**Workflow 2**](https://n8n.io/workflows/357/) checks for the threshold values every minute and, if a value that crosses the specified threshold, it triggers an alert with an SMS, and updates the value in the database.\n\n### Prerequisites for building the workflow\n\n- **n8n set up**. The easiest way is to [download the desktop app](https://docs.n8n.io/hosting/installation/desktop-app/), but you can also [sign up for n8n.cloud](https://docs.n8n.io/hosting/installation/cloud/) or [self-host n8n](https://docs.n8n.io/hosting/installation/docker/).\n- **PostgreSQL set up** and [credentials](https://docs.n8n.io/integrations/credentials/postgres/). You can create the database for this workflow with the following SQL statement: `CREATE TABLE n8n (id SERIAL, sensor_id VARCHAR, value INT, time_stamp TIMESTAMP, notification BOOLEAN);`\n- **Basic knowledge of SQL**. This is helpful for knowing how to create a table and query the database.\n\n### Workflow 1 - Generate data and insert it into a database\n\nThis workflow builds the initial setup for automatic database activity monitoring. It generates sensor data and inserts the values into a Postgres database.\n\n![workflow 1](https://blog.n8n.io/content/images/size/w1600/2022/06/dbmonitoring_workflow1.png)\n\nThe workflow consists of three nodes:\n\n1. [**Cron node**](https://docs.n8n.io/integrations/core-nodes/n8n-nodes-base.cron/) triggers the workflow at regular time intervals. The node has the following parameter:\n   - Mode: Every Minute\n2. [**Function node**](https://docs.n8n.io/integrations/core-nodes/n8n-nodes-base.cron/) generates sensor data (sensor id (preset), a randomly generated value, timestamp, and notification (preset as false)). In the JavaScript Code field, copy-paste the following code:\n   \n  ```javascript\n  var today = new Date();\n  var date = today.getFullYear()+'-'+(today.getMonth()+1)+'-'+today.getDate();\n  var time = today.getHours() + \":\" + today.getMinutes() + \":\" + today.getSeconds();\n  var dateTime = date+' '+time;\n\n  items[0].json.sensor_id = 'humidity01';\n  items[0].json.value = Math.ceil(Math.random()*100);\n  items[0].json.time_stamp = dateTime;\n  items[0].json.notification = false;\n\n  return items;\n  ```\n\n3. [**Postgres node**](https://docs.n8n.io/integrations/nodes/n8n-nodes-base.postgres/) inserts the data into a Postgres database. The node has the following parameters:\n   - Operation: Insert\n   - Schema: public\n   - Table: n8n\n   - Columns: sensor_id,value,time_stamp,notification\n   - Return Fields: *\n\n> Don’t forget to save and activate the workflow before moving on to the next workflow. Once you have done that, every minute a new record will be inserted into the database.\n\n### Workflow 2 - Query a database and send SMS alerts\n\nThis workflow builds the actual automation for database activity monitoring and alerting. It queries the Postgres database at regular time intervals for threshold values and, if a value that crosses the specified threshold, it triggers an alert with an SMS, then updates the value in the database.\n\n![workflow 2](https://blog.n8n.io/content/images/size/w1600/2022/06/dbmonitoring_workflow2_twilio.png)\n\nThe workflow consists of five nodes:\n\n1. [**Cron node**](https://docs.n8n.io/integrations/core-nodes/n8n-nodes-base.cron/) triggers the workflow every minute. The node has the following parameter:\n   - Mode: Every Minute\n\n2. [**Postgres node**](https://docs.n8n.io/integrations/nodes/n8n-nodes-base.postgres/) queries the database for values over 70 and notifications that haven't been sent (false). The node has the following parameters:\n   - Operation: Execute Query\n   - Query: `SELECT * FROM n8n WHERE value > 70 AND notification = false;`\n\n3. [**Twilio node**](https://docs.n8n.io/integrations/nodes/n8n-nodes-base.twilio/) sends an SMS with information about the queried values. The node has the following parameters:\n   - Resource: SMS\n   - Operation: Send\n   - From: phone number of the sender\n   - To: phone number of the receiver\n   - Message > Add Expression: `🚨 The Sensor ({{$node[\"Postgres\"].json[\"sensor_id\"]}}) showed a reading of {{$node[\"Postgres\"].json[\"value\"]}}.`\n\n  ![twilio message](https://blog.n8n.io/content/images/2022/06/dbmonitoring_twiliomsg.jpeg)\n\n  > You might want to turn off the other workflow or device at some point, otherwise you might end up using all your Twilio free credits during the first run.\n\n4. [**Set node**](https://docs.n8n.io/integrations/core-nodes/n8n-nodes-base.set/) sets the value of the notification to true for all the corresponding ids. Once you make this update in the database with the help of the next node, the SMS alerts will not go through twice for any given record. The node has the following parameters:\n   - Keep Only Set: toggle to true\n   - Add Value > Number:\n     - Name: id\n     - Value > Add Expression: `{{$node[\"Postgres2\"].json[\"id\"]}}`\n   - Add Value > Boolean:\n     - Name: notification\n     - Value: toggle to true\n\n\n5. [**Postgres node**](https://docs.n8n.io/integrations/nodes/n8n-nodes-base.postgres/) updates the information in the database based on the id. The node has the following parameters:\n   - Operation: Update\n   - Schema: public\n   - Table: n8n\n   - Update Key: id\n   - Columns: notification\n   - Return Fields: *\n\nAnd that was it! Now you have two workflows that automatically monitor activity in your Postgres database and send you SMS alerts when values exceed a specific threshold.\n\n## What's next?\n\nIn this post, you learned the basics of database activity monitoring and saw how to build a workflow that automatically monitors a Postgres database and sends SMS alerts.\n\nHere's what you can do next:\n\n- Read more tutorials about use cases related to databases and security, for example [How to manage database backups](https://blog.n8n.io/workflow-for-cratedb-backup-management/) or [How to automate an incident response playbook](https://blog.n8n.io/automated-incident-response-workflow/).\n- Check out the [workflow page](https://n8n.io/workflows/) for more automation ideas.\n- [Subscribe to our newsletter](https://n8n.io/blog/#subscribe) to get the latest news around workflow automation with n8n.\n- Join the [community forum](https://community.n8n.io/) and follow us on [Twitter](https://twitter.com/n8n_io).\n\n> This post was originally published on the [n8n blog](https://n8n.io/blog/database-activity-monitoring)."
    },
    {
      "id": "/2022/06/26/resources-technical-writers",
      "metadata": {
        "permalink": "/blog/2022/06/26/resources-technical-writers",
        "source": "@site/blog/2022-06-26-resources-technical-writers.md",
        "title": "Resources for technical writers",
        "description": "Books, podcasts, and courses to help technical writers hone their writing skills and collaborate with DevRel and Marketing teams.",
        "date": "2022-06-26T00:00:00.000Z",
        "formattedDate": "June 26, 2022",
        "tags": [
          {
            "label": "books",
            "permalink": "/blog/tags/books"
          },
          {
            "label": "writing",
            "permalink": "/blog/tags/writing"
          }
        ],
        "readingTime": 2.19,
        "truncated": true,
        "authors": [],
        "frontMatter": {
          "layout": "post",
          "title": "Resources for technical writers",
          "share-description": "Books, podcasts, and courses to help technical writers hone their writing skills and collaborate with DevRel and Marketing teams.",
          "tags": [
            "books",
            "writing"
          ]
        },
        "prevItem": {
          "title": "How to automate database activity monitoring and alerting",
          "permalink": "/blog/2022/06/28/database-activity-monitoring"
        },
        "nextItem": {
          "title": "How to scrape data from a website with n8n",
          "permalink": "/blog/2022/06/21/webscraping-workflow"
        }
      },
      "content": "Books, podcasts, and courses to help technical writers hone their writing skills and collaborate with DevRel and Marketing teams.\n\n<!--truncate-->\n\n![books technical writing](./blog_images/books_tw.jpeg)\n\nFor over a year now, I've been working as a technical writer in a tech startup developing a workflow automation tool. I've been part of a Developer Relations (DevRel) team, then a Marketing team, as well as some Product in-between. This organization means that I had to (re)learn how to **collaborate** efficiently across teams with different strategies, and **commnicate** the value of technical writing for achieving different goals.\n\nOne thing that helped me in this challenge was diving into different resources on these topics. In the spirit of sharing knowledge with other fellow technical writers, here's a **list of books, podcasts, and courses around technical writing**:\n\n## Books\n\n### Technical writing\n\n- [***Docs for Developers* by Jared Bhatti et al.**](https://docsfordevelopers.com/) offers a walk-through the entire software documentation process, from understanding users to publishing and maintaining docs. (Bonus points: it includes illustrations of corgies.)\n- [***The Product is Docs* by Christopher Gales and the Splunk Documentation Team**](https://www.splunk.com/en_us/blog/splunklife/the-product-is-docs.html) takes *Docs for Developers* a step further by including best practices from the Splunk team and chapters about how technical writers can work with different teams (Support, Marketing, Engineering, Product, UX/Design, and remote).\n- [***Docs like Code* by Anne Gentle**](https://www.docslikecode.com/book/)\n- [***Every Page is Page One* by Mark Baker**](https://everypageispageone.com/the-book/)\n\n### General writing\n\n- [***The Sense of Style* by Steven Pinker**](https://www.youtube.com/watch?v=3ZKTmsgqi0U) is a modern guide on how to read and write clearly, that gets into the nitty-gritty details of the English language from a linguistics and cognitive science perspective.\n- [***On Writing Well* by William Zinsser**](https://www.blinkist.com/de/books/on-writing-well-en) is an oldie but goldie among books dedicated to the art of writing non-fiction, and includes a chapter about technical writing.\n- [***The Elements of Style* by Strunk/White**](https://www.gutenberg.org/files/37134/37134-h/37134-h.htm) is, to any English major and (aspiring) writer, a reference book on grammar and style rules (though some are outdated by now), with the maxim \"Remove unnecessary words!\"\n\n### DevRel teams\n\n- [***The Business Value of Developer Relations* by Mary Thengvall**](https://www.oreilly.com/library/view/the-business-value/9781484237489/) is a manual on how to build, manage, and measure the success of a DevRel team.\n- [***Developer Marketing and Relations* by Caroline Lewko**](https://www.devrelx.com/book) is a practical guide on working at the intersection of DevRel and Marketing, with concrete good and bad examples from big tech companies.\n\n## Podcasts\n\n- [API the Docs](https://open.spotify.com/show/6QccUn9lMYwfnoO9s4shp1?si=222c36ae03774e60)\n- [Knowledgebase Ninjas](https://open.spotify.com/show/6FvQBMNBUhqe4VN2ZVybnj?si=a39ba13487734980)\n- [Content Components](https://open.spotify.com/show/7sfUcBaDWcaSMj2HrMBdtG?si=6dcaa5fbf5314a55)\n\n## Courses\n\n- [Technical Writing for Engineers (Google)](https://developers.google.com/tech-writing)\n- [Technical Writing Fundamentals (GitLab)](https://about.gitlab.com/handbook/engineering/ux/technical-writing/fundamentals/)\n- [Technical Writing: How to Write Software Documentation](https://www.udemy.com/course/start-your-career-as-user-assistance-developer/)\n- [Project Management for Technical Writers](https://www.udemy.com/course/project-management-for-technical-writers/)\n- [UX and Information Architecture Basics for Technical Writers](https://www.udemy.com/course/technical-writing-how-to-define-information-architecture/)"
    },
    {
      "id": "/2022/06/21/webscraping-workflow",
      "metadata": {
        "permalink": "/blog/2022/06/21/webscraping-workflow",
        "source": "@site/blog/2022-06-21-webscraping-workflow.md",
        "title": "How to scrape data from a website with n8n",
        "description": "Learn web scraping basics:\\ what it is, how it works, what tools you can use, and how to build your own low-code workflow to scrape an online store.",
        "date": "2022-06-21T00:00:00.000Z",
        "formattedDate": "June 21, 2022",
        "tags": [
          {
            "label": "n8n",
            "permalink": "/blog/tags/n-8-n"
          },
          {
            "label": "tutorials",
            "permalink": "/blog/tags/tutorials"
          }
        ],
        "readingTime": 9.335,
        "truncated": true,
        "authors": [],
        "frontMatter": {
          "title": "How to scrape data from a website with n8n",
          "description": "Learn web scraping basics:\\ what it is, how it works, what tools you can use, and how to build your own low-code workflow to scrape an online store.",
          "tags": [
            "n8n",
            "tutorials"
          ],
          "canonical_url": "https://blog.n8n.io/how-to-scrape-data-from-a-website/"
        },
        "prevItem": {
          "title": "Resources for technical writers",
          "permalink": "/blog/2022/06/26/resources-technical-writers"
        },
        "nextItem": {
          "title": "Webhooks explained",
          "permalink": "/blog/2022/06/14/webhooks-explained"
        }
      },
      "content": "Learn web scraping basics: what it is, how it works, what tools you can use, and how to build your own low-code workflow to scrape an online store.\n\n<!--truncate-->\n\nIt's no secret (anymore) that data is precious. Structured data is particularly valuable for companies and professionals in pretty much any industry, as it offers them the raw material for making predictions, identifying trends, and shaping their target audience.\n\nOne method for getting hold of structure data, that has been growing in popularity [in the past ten years](https://scrapeops.io/blog/the-state-of-web-scraping-2022/), is **web scraping**. In this post, we'll explain the basics of web scraping and show you a step-by-step tutorial for building an n8n workflow that scrapes data from an online store.\n\n## What is web scraping?\n\n**Web scraping (also called web harvesting) is the process of automatically extracting and collecting data from a website.** This is a useful approach if you need to get structured data from a website that doesn't have an [API](https://blog.n8n.io/what-are-apis-how-to-use-them-with-no-code/), or if the API offers limited access to the data.\n\nIn practical terms, let's say you want to have a spreadsheet with data about the top 50 bestselling products in an online store, including product name, price, and availability. One option is to manually copy-paste the data from the website into the spreadsheet – but this would be incredibly time-consuming and error-prone. Alternatively, you can automatically collect this data using web scraping.\n\n## Can you scrape data from any website?\n\nNot really. **Some websites explicitly prohibit web scraping, while others allow it, though they might also limit what data you can collect.** To check whether the website you want to scrape allows this, have a look at their `robots-txt` file. You can access it by appending this file name to the domain name. For example, to check whether [IMDb](https://imdb.com/) allows web scraping, go to [`https://imdb.com/robots.txt`](https://imdb.com/robots.txt).\n\nThe `robots.txt` file instructs search engine crawlers which URLs they can crawl. Restricting crawling on certain pages avoids overloading the website with requests. This is important, because when doing web scraping, you're basically sending lots of requests to the website in a short time interval, thus overloading it and impacting its performance.\n\nIn the case of IMDb, their `robots.txt` file doesn't allow scraping content from pages like TV schedule, movies, and many others.\n\nWhile web scraping is not illegal, it's a grey area. You can get sanctioned, banned or blocked from the website, or fined if you scrape data from certain websites and/or commercialize it. To make sure you're on the right side of the law, always check the `robots.txt` file of the website you want to scrape.\n\n## How does web scraping work?\n\nEssentially, the web scraping process involves 5 steps:\n\n1. **Select the URL** (website) you want to scrape.\n2. **Make a request** to the URL. The server responds to the request and returns you the data in HTML or XML page.\n3. **Select the data** you want to extract from the webpage.\n4. **Run the code** to extract the selected data.\n5. **Export the data** in a readable format (for example, as a CSV file).\n\n## How to do web scraping?\n\nNowadays, there are different options to do web scraping, from browser extensions to no-code tools and custom code in different programming languages. Here are **three ways to do web scraping**, with their advantages and downsides.\n\n### Using browser extensions or tools\n\nBrowser extensions are the easiest way to do basic web scraping. If you're not particularly tech-savvy or only need to scrape some data quickly from a static page, you can reach out for tools like [Parsehub](https://www.parsehub.com/), [Octoparse](https://www.octoparse.com/), or [Webscraper.io](https://webscraper.io/). \n\nThe downside of browser extensions like these is that they usually have limited functionality (for example, they might not work on pages with dynamic loading or websites with pagination). Moreover, some tools are paid or freemium, and it can get quite costly if you need to scrape lots of data.\n\n### Using custom code and web scraping libraries\n\nIf you know how to code, you can use libraries dedicated to web scraping in different programming languages, such as [rvest](https://rvest.tidyverse.org/) in **R**, or [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/), [ScraPy](https://scrapy.org/), and [Selenium](https://selenium-python.readthedocs.io/) in **Python**. For example, you can scrape and analyze [movie information using rvest](https://lorenaciutacu.com/web-scraping-imdb-with-r/) or [music lyrics using BeautifulSoup](https://github.com/lorenanda/lyrics-classification).\n\nThe advantage of using libraries like these is that you have more flexibility and control over what you scrape and how to process data – plus it's free. The downside to this approach is that it requires programming skills, and writing custom code can be time-consuming or become overly complicated for a basic scraping use case.\n\n### Using n8n workflows\n\nn8n combines the power of libraries with the ease of browser extensions –– and it's free to use. You can build automated workflows using [core nodes](https://docs.n8n.io/integrations/core-nodes/) like [HTTP Request](https://docs.n8n.io/integrations/core-nodes/n8n-nodes-base.httprequest/) and [HTML Extract](https://docs.n8n.io/integrations/core-nodes/n8n-nodes-base.htmlextract/) to scrape data from websites and save it in a spreadsheet.\n\nMoreover, you can extend the workflow however you like, for example by emailing the spreadsheet, inserting the data into a database, or analyzing and [visualizing it on a dashboard](https://blog.n8n.io/automatically-pulling-and-visualizing-data-with-n8n/).\n\n## Workflow for web scraping\n\nLet's build a web scraper with no code using n8n!\n\nThis workflow scrapes information about books from the fictive online book store `http://books.toscrape.com`, stores the data in Google Sheets and in a CSV file, and emails the file.\n\n![webscraping workflow](https://blog.n8n.io/content/images/size/w1600/2022/06/webscraping_workflow.png)\n\nThe workflow makes use of three core nodes and two Google nodes, and consists of five steps:\n\n1. Access the content of the website (**HTTP Request node**).\n2. Scrape the title and price of books (**HTML Extract node**).\n3. Order the scraped data alphabetically by title (**Items Lists node**).\n4. Write the scraped ordered data into a CSV file (**Spreadsheet node**) and append it to a Google Sheet (**Google Sheet node**).\n5. Send the CSV file via email (**Gmail node**).\n\nNext, you'll learn how to build this workflow with step-by-step instructions.\n\n### Prerequisites\n\n- **n8n set up**. The easiest way is to [download the desktop app](https://docs.n8n.io/hosting/installation/desktop-app/), but you can also [sign up for n8n.cloud](https://docs.n8n.io/hosting/installation/cloud/) or [self-host n8n](https://docs.n8n.io/hosting/installation/docker/).\n- Basic knowledge of **HTML and CSS**. This is helpful for knowing how to navigate the webpage and what elements to select for the data you want to extract.\n- A **Gmail** account and [credentials](https://docs.n8n.io/integrations/credentials/google/). Alternatively, you can use the [Send Email node](https://docs.n8n.io/integrations/core-nodes/n8n-nodes-base.sendemail/) or nodes for other email providers ([Mailcheck](https://docs.n8n.io/integrations/nodes/n8n-nodes-base.mailcheck/), [MailChimp](https://docs.n8n.io/integrations/nodes/n8n-nodes-base.mailcheck/), [MailerLite](https://docs.n8n.io/integrations/nodes/n8n-nodes-base.mailcheck/), [Mailgun](https://docs.n8n.io/integrations/nodes/n8n-nodes-base.mailcheck/), [Mailjet](https://docs.n8n.io/integrations/nodes/n8n-nodes-base.mailcheck/)).\n\n### Step 1 - Getting the website content\n\nFirst of all, you need to get data from the website in HTML format. To do this, use the **HTTP Request node** with the following parameters:\n\n- Request Method: GET\n- URL: http://books.toscrape.com\n- Response Format: String\n- Property Name: data\n\nWhen you execute the node, the result should look like this:\n\n![webscraping http request node](https://blog.n8n.io/content/images/size/w1600/2022/06/webscraping_httprequest.png)\n\n### Step 2 - Extracting data from the website\n\nNow that you have the HTML code of the website, you need to extract the data you want (in this case, all the books on the page). To do this, use the **HTML Extract node** with the following parameters:\n\n- Source Property: JSON\n- JSON Property: data\n  This is the property name set in the HTTP Request node.\n- Extraction Values:\n  - Key: books\n    CSS Selector: .row > li\n    To get the selector (the reference of the data you want to extract), [inspect the page in your browser](https://blog.hubspot.com/website/how-to-inspect).\n\nWhen you execute the node, the result should look like this:\n\n![webscraping html extract node 1](https://blog.n8n.io/content/images/size/w1600/2022/06/webscraping_htmlextract1.png)\n\nThe HTML Extract node returns one item that contains the HTML code of all the books on the page. However, you need each book to be an item, so that you can extract each title and price. To do this, use the **Item Lists node** with the following parameters:\n\n- Operation: Split Out Items\n- Field To Split Out: books\n  This is the name of the key in the extraction value set in the previous HTML Extract node.\n- Include: No Other Fields\n\nWhen you execute the node, the result should look like this:\n\n![webscraping item list node 1](https://blog.n8n.io/content/images/size/w1600/2022/06/webscraping_itemlist1.png)\n\nNow that you have a list of 20 items (books), you need to extract the title and price from their HTML code. This part is similar to the previous step. To do this, use another **HTML Extract node** with the following parameters:\n\n- Source Data: JSON\n- JSON Property: books\n- Extraction Values:\n  - Key: title\n    CSS Selector: h3\n    Return Value: Text\n  - Key: price\n    CSS Selector: article > div.product_price > p.price_color\n    Return Value: Text\n\nWhen you execute the node, the result should look like this:\n\n![webscraping html extract node 2](https://blog.n8n.io/content/images/size/w1600/2022/06/webscraping_htmlextract2.png)\n\n### Step 3 - Ordering data\n\nNow you have a nice table that contains the titles and prices of 20 books in store. Optionally, you can sort the books by their price in descending order. To do this, use another **Item Lists node** with the following parameters:\n\n- Operation: Sort\n- Type: Simple\n- Fields To Sort By:\n  - Field Name: price\n  - Order: Descending\n\nWhen you execute the node, the result should look like this:\n\n![webscraping item list node 2](https://blog.n8n.io/content/images/size/w1600/2022/06/webscraping_itemlist2.png)\n\n### Step 4 - Writing the scraped data to a spreadsheet\n\nNow that you scraped the data in a structured way, you'll want to export it as a CSV file. To do this, use the **Spreadsheet File node** with the following parameters:\n\n- Operation: Write to file\n- File Format: CSV\n- Binary Property: data\n\nWhen you execute the node, the result should look like this:\n\n![webscraping spreadsheet file node](https://blog.n8n.io/content/images/size/w1600/2022/06/webscraping_spreadsheet.png)\n\nFrom here, you can download the CSV file directly to your computer.\n\nAnother option is to store the scraped data in a Google Sheet. You can do this using the **Google Sheets node**. The result of this node would look like this:\n\n![webscraping google sheets node](https://blog.n8n.io/content/images/size/w1600/2022/06/webscraping_googlesheet.png)\n\n### Step 5 - Sending the data set via email\n\nFinally, you might want to send the CSV file with the scraped book data via email. To do this, use the **Gmail node** with the following parameters:\n\n- Resource: Message\n- Operation: Send\n- Subject: bookstore csv (or any other subject line you'd like)\n- Message: Hey, here's the scraped data from the online bookstore website.\n- To Email: the email address you want to send to\n- Additional Fields > Attachments > Property: data\n  This is the name of the binary property set in the Spreadsheet File node.\n\nWhen you execute the node, the result should look like this:\n\n![webscraping gmail node](https://blog.n8n.io/content/images/size/w1600/2022/06/webscraping_gmail-1.png)\n\nNow just check your inbox – you should receive an email with the CSV file!\n\n## What's next?\n\nNow you know the basics of web scraping: what it is, how it works, and what to keep in mind when scraping websites. You also learned how to build a no-code workflow that automatically scrapes data from an online store, stores in a spreadsheet, and emails it. \n\nHere's what you can do next:\n\n- Check out a more complex [workflow that scrapes data from a multi-page website](https://n8n.io/workflows/1073/), and read about the [use case behind it](https://n8n.io/case-studies/uproc/).\n- Check out the [workflow page](https://n8n.io/workflows/) for more automation ideas.\n- [Subscribe to our newsletter](https://n8n.io/blog/#subscribe) to get the latest news around workflow automation with n8n.\n- Join the [community forum](https://community.n8n.io/) and follow us on [Twitter](https://twitter.com/n8n_io).\n\n> This post was originally published on the [n8n blog](https://blog.n8n.io/how-to-scrape-data-from-a-website/)."
    },
    {
      "id": "/2022/06/14/webhooks-explained",
      "metadata": {
        "permalink": "/blog/2022/06/14/webhooks-explained",
        "source": "@site/blog/2022-06-14-webhooks-explained.md",
        "title": "Webhooks explained",
        "description": "Learn what webhooks are, how they work, when to use them, and how to automate your workflows using the webhook nodes in n8n.",
        "date": "2022-06-14T00:00:00.000Z",
        "formattedDate": "June 14, 2022",
        "tags": [
          {
            "label": "n8n",
            "permalink": "/blog/tags/n-8-n"
          }
        ],
        "readingTime": 6.57,
        "truncated": true,
        "authors": [],
        "frontMatter": {
          "layout": "post",
          "title": "Webhooks explained",
          "share-description": "Learn what webhooks are, how they work, when to use them, and how to automate your workflows using the webhook nodes in n8n.",
          "tags": [
            "n8n"
          ],
          "canonical_url": "https://blog.n8n.io/webhooks-for-workflow-automation/"
        },
        "prevItem": {
          "title": "How to scrape data from a website with n8n",
          "permalink": "/blog/2022/06/21/webscraping-workflow"
        },
        "nextItem": {
          "title": "13 tools to use for DevSecOps automation",
          "permalink": "/blog/2022/03/11/devsecops-tools"
        }
      },
      "content": "Learn what webhooks are, how they work, when to use them, and how to automate your workflows using the webhook nodes in n8n.\n\n<!--truncate-->\n\nSince being introduced in 2007, [webhooks have revolutionized the web](https://web.archive.org/web/20180630220036/http://progrium.com/blog/2007/05/03/web-hooks-to-revolutionize-the-web/). These user-defined callbacks made it possible to access real-time data from an app, creating new infrastructure and opening up endless possibilities for web developers.\n\nIn this post, you'll learn the basics about webhooks: what they are, how they work, how they differ from APIs and polling, and when to use them. You'll also see how the [Webhook node](https://docs.n8n.io/integrations/core-nodes/n8n-nodes-base.webhook/) and [Respond to Webhook node](https://docs.n8n.io/integrations/core-nodes/n8n-nodes-base.respondtowebhook/) can be used to build powerful workflow automations in n8n.\n\n\n## What are webhooks?\n\nEssentially, webhooks are a way for apps to communicate with each other, kind of like keeping each other informed about updates. Webhooks are used to set instant, real-time notifications for events that take place in an app, allowing you to then take specific actions based on those events.\n\nFor example, let's say you want to send a message as soon as information gets updated in a database. The dialogue between the two apps would go like this:\n\n– \"Hey Webhook, let me know when this happens.\"  \n– \"Ok, App, I will.\" [a while later...] \"Hey App, this thing you asked about just happened.\"  \n– \"Cool, thanks. I'll take it from here.\"\n\nThink of setting a webhook like putting yourself on the waitlist for an event: the date is not set yet, so you can't sign up; you also don't want to ask every 5 minutes when it will take place, because it would be a waste of time (and very annoying); so you only ask once to be kept in the loop, then you'll just wait to receive the news.\n\nWebhooks enable many automation and notification workflows, where it's needed to react to certain updates as they happen.\n\n## How do webhooks work?\n\nNow that you have a high-level understanding of webhooks, you might wonder what happens under the hood. The process can be summarized in three steps:\n\n1. The webhook receives a request to listen to an event in System A.\n2. When that event occurs in System A, the webhook makes an [HTTP POST request](https://developer.mozilla.org/en-US/docs/Web/HTTP/Methods/POST) to a URL. This method asks the web server to accept the data in the body of the request message.\n3. System B receives data from POST about the event that occurred in System A, and takes action. What this action is, is specified by the user.\n\n## Webhook vs. API vs. Polling\n\nWebhooks are not the only way to establish a communication between apps––[APIs](https://n8n.io/blog/what-are-apis-how-to-use-them-with-no-code/) and [polling](https://n8n.io/blog/creating-triggers-for-n8n-workflows-using-polling/) serve a similar purpose. However, there are some key differences between the three methods.\n\n- **Webhooks** are event-based, as you configure them to listen to a specific event. They are also automated, so they automatically respond to an event that was set once.\n- **APIs** are request-based, as you ask for a specific resource. APIs are not automated, as they give you the information only when you ask for it.\n- **Polling** functions like an API, but it requests data at regular time intervals. Think of polling like the donkey in Shrek, who keeps on asking: \"Do you have this info yet? How about now? But now?\".\n\nIn this sense, webhooks are simpler than APIs and faster than polling.\n\n## When to use webhooks\n\nKnowing the difference between webhooks, APIs, and polling helps you decide when to use each method. It's best to use webhooks in the following situations:\n\n- When you need to be informed when a specific event occurs (for example, to identify malfunctions, and act on them ASAP).\n- When you need to run a workflow based on data from an app (for example, to [sync data between two systems](https://n8n.io/blog/how-to-sync-data-between-two-systems/)).\n- When you need to run a workflow when an event occurs in a system (for example, to send a confirmation email when someone signs up for a course).\n\nWebhooks are fast and efficient, as they stay alert for specific events and pass on information about them right away. So when you have a use case where these two qualities are essentials, it's a job for a webhook.\n\n## Workflow automation with webhooks\n\nWebhooks open up many possibilities for automating all kinds of workflows. n8n provides two nodes for working with webhooks:\n\n- [**Webhook node**](https://docs.n8n.io/integrations/core-nodes/n8n-nodes-base.webhook/) allows several different services to connect to n8n and run a workflow when data is received. You can use this node when there is no [trigger node](https://docs.n8n.io/integrations/trigger-nodes/) available for the app you need, or when the trigger node doesn't have the operation you need.\n- [**Respond to Webhook node**](https://docs.n8n.io/integrations/core-nodes/n8n-nodes-base.respondtowebhook/) allows you to control the response to incoming webhooks.\n\nHere are some examples of workflows that use the (Respond to) Webhook nodes for automating various use cases, from GDPR compliance to RSS feeds.\n\n### Automated GDPR compliance\n\nGDPR compliance is as much an important topic, as it is a hassle to implement. Securing and deleting user can cost a team hours of tedious work. We've been there, and predictably we tried to automate at least part of this process.\n\nThis [workflow handles data deletion requests](https://n8n.io/workflows/1455) through Slack slash commands. The Webhook node triggers the workflow when the set slash command is called in Slack, and the Respond to Webhook node sends responses back to Slack.\n\n### Automated URL shortening\n\nURL shorteners are very useful for creating user-friendly links that can easily be shared on social platforms. However, the existing tools tend to have limited functionality.\n\nInstead, you can use this [workflow to create a self-hosted URL shortener](https://n8n.io/workflows/1093) that also displays the number of clicks on a dashboard. Webhook nodes are used to trigger actions based on events on a specified website.\n\nFollow [this tutorial](https://n8n.io/blog/how-to-build-a-low-code-self-hosted-url-shortener/) to learn how to set it up step-by-step.\n\n### Automated incident response\n\nIncidents like a server outage or data breach can cause significant damage to a business and its reputation, and it's crucial to mitigate the risks as soon as possible. In this kind of situations, workflow automation can save you precious time.\n\nThis series of three [workflows automatically follow an incident response playbook](https://n8n.io/workflows/353). Webhook nodes are used to trigger the workflows when an incident is created in PagerDuty, and when team members acknowledge and resolve the incident.\n\nLearn more about automated incident response and how to configure the workflows step-by-step in the blog post [*How to automate every step of an incident response workflow*](https://n8n.io/blog/automated-incident-response-workflow/).\n\n### Automated RSS feeds\n\nWhat to do when you want to stay up to date with a website's content, but it doesn't provide the classic RSS feed? You guessed it, there's a workflow for that as well.\n\nThis [workflow creates an RSS feed based on a website's content](https://n8n.io/workflows/1418), using the Webhook node to trigger actions when new content is added to a specified page. The other core nodes then extract the required data and generate the feed.\n\n### Automated reading list\n\nHonestly, how many tabs do you keep open with articles you want to read? You haven't yet found the time, and until you do, they're there clogging your browser, staring in the back of your mind, making you feel nervous about your to-do list.\n\nTo make things easier, this [workflow automatically adds articles to Notion](https://n8n.io/workflows/1110) based on a Discord slash command. You can simply send a link in a Discord chat, and the Webhook node triggers the workflow to add that page to a Notion list.\n\n### Automated document e-signatures\n\nGetting a signature on a document is often the green light that allows you to move on with a project, so it's important to manage these records in a timely manner.\n\nThis [workflow manages Acrobat Sign signatures](https://n8n.io/workflows/1588), using Webhook nodes to trigger the workflow on new sign intents on a document, and the Respond to Webhook node to set the response headers.\n\n> This post was originally published on the [n8n blog](https://blog.n8n.io/webhooks-for-workflow-automation/)."
    },
    {
      "id": "/2022/03/11/devsecops-tools",
      "metadata": {
        "permalink": "/blog/2022/03/11/devsecops-tools",
        "source": "@site/blog/2022-03-11-devsecops-tools.md",
        "title": "13 tools to use for DevSecOps automation",
        "description": "Discover the open-source and commercial tools that can help you embed security in your DevOps pipeline.",
        "date": "2022-03-11T00:00:00.000Z",
        "formattedDate": "March 11, 2022",
        "tags": [
          {
            "label": "n8n",
            "permalink": "/blog/tags/n-8-n"
          }
        ],
        "readingTime": 7.275,
        "truncated": true,
        "authors": [],
        "frontMatter": {
          "layout": "post",
          "title": "13 tools to use for DevSecOps automation",
          "share-description": "Discover the open-source and commercial tools that can help you embed security in your DevOps pipeline.",
          "tags": [
            "n8n"
          ],
          "canonical_url": "https://n8n.io/blog/devsecops-tools/"
        },
        "prevItem": {
          "title": "Webhooks explained",
          "permalink": "/blog/2022/06/14/webhooks-explained"
        },
        "nextItem": {
          "title": "My Visual Studio Code setup for technical writing",
          "permalink": "/blog/2022/02/11/vscode-setup-for-technical-writing"
        }
      },
      "content": "Discover the open-source and commercial tools that can help you embed security in your DevOps pipeline.\n\n<!--truncate-->\n\n![cover image](https://n8n.io/blog/content/images/size/w2000/2022/03/secopstools_cover.svg)\n\nIn 2021 there have been [25%](https://cdn.statcdn.com/Infographic/images/normal/26148.jpeg) more ransomware attacks than in 2020––and these are only the publicized attacks. No doubt, it’s becoming more necessary to secure your organization and software products against cyber threats and security vulnerabilities.\n\n**Software security issues** include bugs, authentication, unsecured authentication, data exposure, libraries with vulnerabilities, and misconfigured settings. All these can be exploited by malicious actors who want to gain access to your system. That's why it's crucial to have a solid security development plan to protect against (or at least warn about) **external threats**, such as ransomware, malware, phishing, and DDoS attacks.\n\nThe good news is, you most probably don't have to start from zero, but build on your existing development processes. In this post, we'll explain the DevSecOps methodology and its benefits, and introduce you to some of the most popular tools that you can use to automate your incident response and security operations.\n\n## DevOps vs SecOps vs DevSecOps\n\nWhile **DevOps** (Development Operations) aims to make software deployment and maintenance faster and more efficient, **SecOps** (Security Operations) aims to establish and strengthen the software and network security. \n\nTraditionally, development and security teams would work separately and come together only in the later stages of the SDLC (software development lifecycle). This separation of concerns means that security was often left as an afterthought, making it more difficult to integrate security recommendations in the end-product.\n\n**DevSecOps** emerged as a solution to unify these two methodologies in order to make the SDLC more efficient.\n\n## Advantages of DevSecOps\n\nBy making security a core part of the software delivery process and establishing best practices for both teams, DevSecOps:\n\n- reduces friction between development and security concerns\n- enables collaboration, coordination, and shared responsibility between development and security teams\n- ensures efficiency and security at every step of the development pipeline\n- allows for scaling and dynamic integration of new perspectives\n- provides a more cohesive overview of the end-product, compliance measures, and vulnerabilities\n\n## Tools for DevSecOps automation\n\nThe DevSecOps methodology is powered by automation at every stage of the CI/CD pipeline. There are many tools designed for DevSecOps professionals. We're here to help you choose the right ones for your needs, whether that's a general-purpose service or a task-specialized app.\n\nWe collected some of the most popular integrations for **threat modeling** and incident **monitoring**, **alerting**, and **visualization**. Some are open-source/free (🔓), some are paid (💰), and all have dedicated n8n nodes with which you can build workflow automations.\n\n\n### Threat modeling\n\nThreat modeling tools help you identify security threats to a system or patterns in vulnerabilities, evaluate their severity, and pinpoint remediation methods. Then, based on the insights that these tools can offer, you can make informed decisions about the course of action: what vulnerabilities to patch, what security threats to prioritize, how to diminish their impact.\n\n🔓 [**MISP (Malware Information Sharing Platform)**](https://www.misp-project.org/) is an open-source threat intelligence platform. It shares, stores, and correlates Indicators of Compromise (IoC) of targeted attacks, threat intelligence, financial fraud or vulnerability information. If you need to automatically manage points such as attributes, events, feed, and warning lists, use the [*MISP node*](https://docs.n8n.io/nodes/n8n-nodes-base.misp/) in your n8n workflows.\n\n🔓 [**TheHive**](https://thehive-project.org/) is a scalable open-source and free security incident response platform designed to help information security practitioners and bring security incident response to the masses. You can synchronize TheHive with one or multiple MISP instances to investigate MISP events, or export an investigation's results as a MISP event to help detect and react to attacks. The [*TheHive node*](https://docs.n8n.io/nodes/n8n-nodes-base.theHive/) allows you to manage alerts, cases, logs, observables, and tasks. For example, you can use this node to build [a workflow for zero dollar detection and response orchestration](https://wlambertts.medium.com/zero-dollar-detection-and-response-orchestration-with-n8n-security-onion-thehive-and-10b5e685e2a1).\n\n🔓 [**Cortex**](https://github.com/TheHive-Project/CortexDocs) offers a powerful observable (e.g. URL, file, IP) analysis mechanism, with which you can analyze collected observables, respond to threats, and interact with the constituency and other teams. Cortex can also be used in conjunction with TheHive to analyze tens to hundreds of observables. The [*Cortex node*](https://docs.n8n.io/nodes/n8n-nodes-base.cortex/) allows you to execute analyzers and responders, and get job details and reports. For example, you can use it to build [a workflow that analyzes a URL and gets job details](https://n8n.io/workflows/809).\n\n![](https://community.n8n.io/uploads/default/optimized/2X/4/4f73bbb0a2c1dbf169cf9c9029ab419617a227e6_2_690x369.jpeg)\n\n### Alerting\n\nSecurity incidents can occur at every stage of software development, due to internal mishaps or external threats. Whatever the case, it's critical to become aware of potential security vulnerabilities as soon as they arise. Tools that automatically issue alerts when a threat or vulnerability is detected can help your team investigate and fix it as soon as possible, thus minimizing the critical mean time to respond (MTTR).\n\n💰 [**SIGNL4**](https://www.signl4.com/) is a plug-and-play cloud solution produced by Derdack. It automatically notifies teams on their mobile devices in case of critical events. The [*SIGNL4 node*](https://docs.n8n.io/nodes/n8n-nodes-base.signl4/) allows you to send and resolve alerts. For example, you can build workflows that automatically [store database alerts in Notion](https://n8n.io/workflows/1122) or [monitor files changes and send alerts](https://n8n.io/workflows/967).\n\n![](https://n8n.io/blog/content/images/size/w1000/2022/03/secopstools_workflow_signl4.png)\n\n🔓 [**Rundeck**](https://www.rundeck.com/) is an open-source runbook automation tool for incident management, business continuity, and self-service operations. This tool is typically used in security and compliance, helping organizations maintain compliance controls, control access to sensitive data, and audit activity logs. Use the [*Rundeck node*](https://docs.n8n.io/nodes/n8n-nodes-base.rundeck/) to automatically execute jobs and get their metadata.\n\n🔓 Rundeck is actually created by [**PagerDuty**](https://www.pagerduty.com/), a cloud computing company that produces a SaaS incident response platform for IT departments. The [*PagerDuty node*](https://docs.n8n.io/nodes/n8n-nodes-base.pagerDuty/) allows you to manage incidents and incident notes, log entries, and users. For example, you can use it in a [workflow that automates every step of an incident response playbook](https://n8n.io/blog/automated-incident-response-workflow).\n\n![](https://n8n.io/blog/content/images/size/w1000/2022/03/irplan_all.png)\n\n\n### Monitoring\n\nSecurity monitoring is the automated process of collecting and analyzing indicators of potential security threats, then triaging these threats with appropriate action.\n\n💰 [**Sentry.io**](https://sentry.io/) is a service that helps you monitor and fix crashes in real-time, so that you can diagnose and optimize code performance. The [*Sentry.io node*](https://docs.n8n.io/nodes/n8n-nodes-base.sentryIo/) allows you to manage information about events, issues, projects, and releases.\n\n💰 [**ServiceNow**](https://www.servicenow.com/) is a cloud computing platform to help companies manage digital workflows for their operations. The [*ServiceNow node*](https://docs.n8n.io/nodes/n8n-nodes-base.serviceNow/) allows you to manage, among others, incidents, business services, and user roles.\n\n💰 [**SecurityScorecard**](https://securityscorecard.com/) has been named a 2021 Gartner Peer Insights Customers’ Choice for IT Vendor Risk Management (VRM) Tools. The tool enables organizations to prove and maintain compliance with leading regulations and standards mandates that include PCI, NIST, SOX, and GDPR. Industries, as varied as Government, Insurance, Tech, or Retail, can use SecurityScorecard. Common uses cases include scanning attack surfaces, managing third-party risks, staying in compliance. The [*SecurityScorecard node*](https://docs.n8n.io/nodes/n8n-nodes-base.securityScorecard/) allows you to manage data about the company, industry, portfolio, and reports, among others.\n\n💰 The [Microsoft Graph Security API](https://docs.microsoft.com/en-us/graph/security-concept-overview) allows connecting to Microsoft security products, services, and partners to streamline security operations and improve threat protection, detection, and response capabilities. With the [*Microsoft Graph Security node*](https://docs.n8n.io/nodes/n8n-nodes-base.microsoftGraphSecurity/) you can manage your secure score and control profile.\n\n\n### Dashboards\n\nAn image is more compelling than raw numbers. When it comes to security, this can mean the difference between being aware of all issues and ready to take informed actions, or having to dive into disparate data to figure out what's going on. DevSecOps visualization tools provide an overview of the key metrics related to security incidents via customizable dashboards, that can serve as visual CTAs for your team.\n\n💰 [**Elastic Security**](https://www.elastic.co/security) helps security teams to prevent, detect, and respond to threats quickly and at cloud scale. The [*Elastic Security node*](https://docs.n8n.io/nodes/n8n-nodes-base.elasticSecurity/) allows you to automatically manage cases and comments, add or remove tags, and create connectors. You can supplement the n8n workflows with Elastic Security dashboards that give you a visual breakdown of the alerts.\n\n🔓 [Grafana](https://grafana.com/) is a multi-platform open source analytics and interactive visualization web application that provides charts, graphs, and alerts for the web when connected to supported data sources. Use the [*Grafana node*](https://docs.n8n.io/nodes/n8n-nodes-base.grafana/) to manage your dashboards, teams, and users.\n\n💰 [**Splunk**](https://www.splunk.com/) is a service for searching, monitoring, and analyzing machine-generated data via a Web-style interface. It indexes and correlates information in a container that makes it searchable, and makes it possible to generate alerts, reports, and visualizations. The [*Splunk node*](https://docs.n8n.io/nodes/n8n-nodes-base.splunk/) allows you to manage fired alerts, users, as well as search configurations, jobs, and results. Similarly to Elastic Security, you can visualize critical incidents and activities in Splunk dashboards.\n\n## Start automating!\n\nNow that you have a list of open-source and commercial tools, you're ready to automate your DevSecOps practice. The best part is, you can [start automating for free with n8n](https://docs.n8n.io/getting-started/installation/).\n\nWhat's your DevSecOps process and what tools do you use? Join the discussion in our [community forum](https://community.n8n.io/).\n\n> This post was originally published on the [n8n blog](https://n8n.io/blog/devsecops-tools/)."
    },
    {
      "id": "/2022/02/11/vscode-setup-for-technical-writing",
      "metadata": {
        "permalink": "/blog/2022/02/11/vscode-setup-for-technical-writing",
        "source": "@site/blog/2022-02-11-vscode-setup-for-technical-writing.md",
        "title": "My Visual Studio Code setup for technical writing",
        "description": "The theme, extensions, and settings I use in my daily job.",
        "date": "2022-02-11T00:00:00.000Z",
        "formattedDate": "February 11, 2022",
        "tags": [
          {
            "label": "writing",
            "permalink": "/blog/tags/writing"
          }
        ],
        "readingTime": 1.95,
        "truncated": true,
        "authors": [],
        "frontMatter": {
          "layout": "post",
          "title": "My Visual Studio Code setup for technical writing",
          "share-description": "The theme, extensions, and settings I use in my daily job.",
          "tags": [
            "writing"
          ],
          "permalink": "vscode-setup-for-technical-writing"
        },
        "prevItem": {
          "title": "13 tools to use for DevSecOps automation",
          "permalink": "/blog/2022/03/11/devsecops-tools"
        },
        "nextItem": {
          "title": "How to automate every step of an incident response workflow",
          "permalink": "/blog/2022/02/10/automated-incident-response"
        }
      },
      "content": "The theme, extensions, and settings I use in my daily job.\n\n<!--truncate-->\n\nI write in Markdown pretty much every day, either personal blog posts or docs at my day job as a Technical Writer. In the past year, I've customized my writing workspace in Visual Studio Code to make writing more efficient and pleasant. In this post, I'll share with you my VSCode setup, including theme, extensions, and settings.\n\n## Theme\nI use [**i - A Minimal Theme**](https://marketplace.visualstudio.com/items?itemName=ctrlplusb.i-minimal-theme). As the name says, it's a minimalistic, distraction-free theme, with no bold colors or playful fonts. The theme comes in three palettes: Light, Solarised, and Night. I alternate between Solarised and Night, depending on my mood and time of day when I'm writing.\n\n  ![VSCode theme night](./blog_images/vscode_setup.png)\n\n## Extensions\n\nI tuned up my VSCode with the following writer-dedicated extensions:\n\n- [**Code Spell Checker**](https://marketplace.visualstudio.com/items?itemName=streetsidesoftware.code-spell-checker) to catch typos.\n- [**Vale**](https://marketplace.visualstudio.com/items?itemName=errata-ai.vale-server) to check that my writing follows the Microsoft Style Guide.\n- [**Word Count**](https://marketplace.visualstudio.com/items?itemName=ms-vscode.wordcount) to show me how many words I write. This is particularly useful for SEO-based blog posts, where I need to hit a certain amount of words. (To automate more SEO checks, I built an [n8n workflow](https://www.youtube.com/watch?v=CwivuZ8YP3s&t=87s) and am working on a Python script, but that's a topic for another blog post.)\n\n  ![VSCode extensions](./blog_images/vscode_setup_extensions.png)\n\n- [Tabnine AI Autocomplete](https://marketplace.visualstudio.com/items?itemName=TabNine.tabnine-vscode) to suggest me autocompletions. It saves me quite a lot of typing time and, though not world-dominatingly intelligent, it can make good suggestions.\n\n  ![VSCode Tabnine](./blog_images/vscode_copilot.gif)\n\n- [**Markdown All in One**](https://marketplace.visualstudio.com/items?itemName=yzhang.markdown-all-in-one) to easily format my text (e.g. checkboxes, lists, table of contents).\n- [**Markdown Preview Enhanced**](https://marketplace.visualstudio.com/items?itemName=shd101wyy.markdown-preview-enhanced) to preview the rendered text and export docs as PDF.\n\n  ![VSCode theme night](./blog_images/vscode_setup2.png)\n\n## Settings\n\nApart from the theme and extensions, I use these VSCode settings for a more clean, zen look:\n\n- **Centered layout**: *View > Appearance > Centered Layout :check:*\n- **Zen mode**: *View > Appearance > Zen Mode :check:*\n- **Word wrap**: *View > Word Wrap* or *Alt+Z*\n- **Hidden side bar**: *View > Appearance > Show Side Bar :uncheck:*\n- **Hidden minimap**: *View > Show Minimap :uncheck:*\n\n  ![VSCode theme solarised](./blog_images/vscode_setup_solarised.png)\n\nThat's it: my VSCode setup for technical writing. I'm pretty sure it will change and evolve in the future, but for now this works just fine. If you fellow tech writers have any ideas or suggestions of how I could improve it, please let me know."
    },
    {
      "id": "/2022/02/10/automated-incident-response",
      "metadata": {
        "permalink": "/blog/2022/02/10/automated-incident-response",
        "source": "@site/blog/2022-02-10-automated-incident-response.md",
        "title": "How to automate every step of an incident response workflow",
        "description": "Minimize the damage caused by IT incidents by following an incident response playbook with PagerDuty, Jira, and Mattermost.",
        "date": "2022-02-10T00:00:00.000Z",
        "formattedDate": "February 10, 2022",
        "tags": [
          {
            "label": "n8n",
            "permalink": "/blog/tags/n-8-n"
          },
          {
            "label": "tutorials",
            "permalink": "/blog/tags/tutorials"
          }
        ],
        "readingTime": 7.065,
        "truncated": true,
        "authors": [],
        "frontMatter": {
          "layout": "post",
          "title": "How to automate every step of an incident response workflow",
          "share-description": "Minimize the damage caused by IT incidents by following an incident response playbook with PagerDuty, Jira, and Mattermost.",
          "tags": [
            "n8n",
            "tutorials"
          ],
          "canonical_url": "https://n8n.io/blog/automated-incident-response-workflow/"
        },
        "prevItem": {
          "title": "My Visual Studio Code setup for technical writing",
          "permalink": "/blog/2022/02/11/vscode-setup-for-technical-writing"
        },
        "nextItem": {
          "title": "How hyperautomation will transform business operations",
          "permalink": "/blog/2022/01/27/hyperautomation-trends"
        }
      },
      "content": "Minimize the damage caused by IT incidents by following an incident response playbook with PagerDuty, Jira, and Mattermost.\n\n<!--truncate-->\n\n**Cyberincidents** (like cyber crime, IT failure/outage, and data breaches) are the [number one threat of large and small companies](https://www.statista.com/statistics/422207/leading-business-risks-by-company-size/). Now imagine your company is hit by a data breach. One moment you're minding your business as usual, the next you notice unusually high outbound traffic, some changes in important files, suspicious user activity, or you're straight-up locked out of your accounts. How do you react?\n\nIf simply thinking about this scenario triggered trepidation, you're unprepared for such a situation. But don't panic.\n\nIn this post, we'll explain what incidents are, why it's crucial to have an incident response playbook in place, how to create one and automate it with our workflow template.\n\n## What is incident response?\n\nLet's start with a definition of the **key terms** you'll encounter in this article: incident, incident response, and incident response plan.\n\nAn **incident** is an event that threatens the operations, services, or functionality of an organization. For example, a server outage or cybersecurity attacks disrupt the everyday operations of an organization. \n\nAn **incident response** is the reaction to an incident. This reaction represents the way the organization manages the situation: from identifying the issue, over analyzing and fixing the problem, to preventing future re-occurrences.\n\nAn **incident response plan** (or **playbook**) is an organized, pre-established sequence of measures that the IT team takes when faced with an incident.\n\n## Why do you need an incident response playbook?\n\nIt is a truth universally acknowledged that people don't make good decisions under stress. The stakes are even higher at large scale, when the data of millions of users or critical business operations are impacted.\n\nAnd yet, [more than 77%](https://www.cybintsolutions.com/cyber-security-facts-stats/) of organizations do not have an incident response plan. [A Kaspersky report](https://media.kasperskycontenthub.com/wp-content/uploads/sites/43/2021/09/13085018/Incident-Response-Analyst-Report-eng-2021.pdf) revealed that 51% of companies detected an incident after impact, and 74% needed weeks up to months for remediation.\n\nAn incident response playbook allows you to jump from (over-)thinking to acting in moments of crisis. The less decisions you need to make on the spot, the faster you can respond to the incident, minimize damage, and protect your company's reputation.\n\n## What are the phases of an incident response plan?\n\nAn efficient incident response typically consists of [six phases](https://www.sans.org/white-papers/33901/):\n\n1. **Preparation**: Get the IT team together, assign clear roles to each of them of what they should do in the case of an incident, and ensure that all necessary tools or services are available.\n2. **Identification**: This is the \"Houston, we have a problem\" moment. Once an incident is identified, establish what happened, where it originated, what parts of the business it affects and to what extent.\n3. **Containment**: Limit the incident threat from spreading to affect other processes, systems, or operations, and establish backups.\n4. **Eradication**: Make sure the issue is fixed and systems are patched, so thoroughly that no trace of the incident is left behind.\n5. **Recovery**: Once the incident is resolved, get all the services up and running again, restore connections and services to production.\n6. **Learnings**: Get the IT team together to discuss what they learned from managing the threat, what went well, and––most importantly perhaps––what could be improved. Add the incident and the taken measures to your internal documentation, so they can serve as reference for future issues.\n\nThese six phases should also be reflected in your incident response playbook.\n\n## Why should you automate the incident response?\n\nHaving an incident response playbook is a first step towards managing incidents more efficiently. However, it still involves manual actions (for example, notifying all team members in due time or creating tickets). The solution to this problem is **automated incident response**. \n\nThough you can’t automate the whole process (someone has to roll up their sleeves and fix the actual issue), you can automate some [low-level tasks](https://n8n.io/blog/features-of-tasks-that-can-be-automated/#4-rule-based-tasks) that add up to cause delays and errors. Automation can assist, supplement, or completely replace human intervention. As a result, the IT team has less trivial tasks to worry about, and instead more time to deal with the critical ones.\n\nBy automating pretty much every step of the playbook, you achieve **more efficient communication** between team members and **faster response times**, which in turn improve the main **incident response metrics**: mean time to detect (MTTD),mean time to acknowledge (MTTA), and mean time to resolution / respond (MTTR).\n\n## Build an automated incident response workflow\nLet's put the theory into practice. In this part, we'll build an automated workflow that follows the following incident response protocol:\n\n1. Triage issue in the project management platform\n2. Create a special channel in the team communication platform\n3. Add the on-call team members to the new channel\n4. Acknowledge the issue\n5. Fix the issue\n6. Resolve the issue ticket\n\nIn our workflow, we'll use the following services:\n- **Jira** for project management and issue tickets. (Alternatively, you can use [Trello](https://docs.n8n.io/nodes/n8n-nodes-base.trello/))\n- **Mattermost** for team communication. (Alternatively, you can use [Slack](https://docs.n8n.io/nodes/n8n-nodes-base.slack/))\n- **PagerDuty** for managing incidents.\n\nThis workflow consists of three parts, each tackling different playbook steps:\n- [Workflow 1](https://n8n.io/workflows/353) covers triaging the issue, creating a special communication channel, and tagging the on-call team (steps 1-3).\n- [Workflow 2](https://n8n.io/workflows/354) covers acknowledging the issue (step 4).\n- [workflow 3](https://n8n.io/workflows/355) covers fixing the issue and resolving the ticket automatically (steps 5-6).\n\nAll put together, the final workflow look like this:\n\n![image](./blog_images/irworkflow_numbered.png)\n\n### Workflow prerequisites\n\nTo follow along this tutorial and implement the workflow yourself, you'll need the following:\n\n- [n8n](https://n8n.io/#get-started): You can download the desktop app, sign up for n8n.cloud, or install n8n self-hosted. In this tutorial, we use n8n version 0.158.0.\n- [PagerDuty credentials](https://docs.n8n.io/credentials/pagerDuty/): You'll need these for configuring the *PagerDuty node* to manage incidents.\n- [Jira credentials](https://docs.n8n.io/credentials/jira/): You'll need these for configuring the *Jira node* to create tickets.\n- [Mattermost credentials](https://docs.n8n.io/credentials/mattermost/): You'll need these for configuring the *Mattermost node* to send notifications.\n\n### Workflow 1 - Inform the team about the incident\n\nThe first workflow automates the ChatOps practice when a new incident is detected.\n\n![workflow](./blog_images/irplan1.png)\n\n- *Webhook node* triggers the workflow when an incident is created in PagerDuty.\n- *Mattermost node* creates a new channel for the specific incident.\n- *Mattermost1 node* adds responsible users to the channel.\n- *Jira node* creates an issue about the incident in Jira.\n- *Mattermost2 node* posts a message in the channel with links to the PagerDuty incident and Jira issue.\n- *Mattermost3 node* posts a message in the channel with the options (buttons) `Acknowledge` and `Resolve`.\n\n### Workflow 2 - Ensure that the incident is acknowledged internally\n\nThe second workflow automates the acknowledgement of the incident by the on-call team member.\n\n![image](./blog_images/irplan2.png)\n\n- *Webhook1 node* triggers the workflow when the button `Acknowledge` is clicked in the Mattermost channel.\n- *PagerDuty node* updates the incident status as \"Acknowledged\".\n- *Mattermost4 node* posts a message in the channel that the incident has been acknowledged.\n\n### Workflow 3 - Mark the incident as resolved\n\nThe third workflow automates the resolution of the issue.\n\n![image](./blog_images/irplan3.png)\n\n- *Webhook2 node* triggers the workflow when the button `Resolve` is clicked in the Mattermost channel.\n- *PagerDuty1 node* updates the incident status as \"Resolved\".\n- *Jira1 node* \n- *Mattermost5 node* posts a message in the channel that the issue has been closed in PagerDuty and Jira.\n- *Mattermost6 node* posts a message in the channel that the incident has been resolved.\n\n### Activate the workflows in production\n\nWe're done with building the workflows! Here's how to see them all in action, from start to end:\n\n1. In the n8n Editor UI, click the `Execute Workflow` button.\n2. Go to your PagerDuty account and create a test incident. \n3. Back in the n8n Editor UI, you'll see information being passed through the Webhook nodes, and the nodes of the workflows being executed.\n\nWith this configuration, the workflows work only when you manually execute them. To make the workflows run automatically, every time an incident is created in PagerDuty, you need to use the `Production webhook` and activate the workflows. Here's how to do this:\n\n1. Get the Production webhook URL from the different Webhook nodes, \n2. Update the URLs on PagerDuty and the Mattermost node from Workflow 1, Step 6,\n3. Save the workflows\n4. Activate the workflows.\n\nThat's it: you now have three no-code workflows that automate every step of an incident response, so the on-call team can focus on solving the problem.\n\n> This post was originally published on the [n8n blog](https://n8n.io/blog/automated-incident-response-workflow/)."
    },
    {
      "id": "/2022/01/27/hyperautomation-trends",
      "metadata": {
        "permalink": "/blog/2022/01/27/hyperautomation-trends",
        "source": "@site/blog/2022-01-27-hyperautomation-trends.md",
        "title": "How hyperautomation will transform business operations",
        "description": "Discover the four trends in hyperautomation and how low-code tools help businesses increase productivity and scale operations.",
        "date": "2022-01-27T00:00:00.000Z",
        "formattedDate": "January 27, 2022",
        "tags": [
          {
            "label": "n8n",
            "permalink": "/blog/tags/n-8-n"
          }
        ],
        "readingTime": 6.05,
        "truncated": true,
        "authors": [],
        "frontMatter": {
          "layout": "post",
          "title": "How hyperautomation will transform business operations",
          "share-description": "Discover the four trends in hyperautomation and how low-code tools help businesses increase productivity and scale operations.",
          "tags": [
            "n8n"
          ],
          "canonical_url": "https://n8n.io/blog/hyperautomation-trends/"
        },
        "prevItem": {
          "title": "How to automate every step of an incident response workflow",
          "permalink": "/blog/2022/02/10/automated-incident-response"
        },
        "nextItem": {
          "title": "How to write in accessible language",
          "permalink": "/blog/2022/01/15/accessible-language"
        }
      },
      "content": "Discover the four trends in hyperautomation and how low-code tools help businesses increase productivity and scale operations.\n\n<!--truncate-->\n\nAre you trying to convince your manager to invest (more) in automation? Or are you the manager who went a bit over budget with automation tools and you're wondering if it was worth it? Or maybe you're just exploring the vast space of workflow automation tools and wondering: \"Is this the future of real work, or is it just fantasy?\"\n\nIn this post we’ll share with you the **key facts you need to know about hyperautomation**: what it is, why it is important, what are examples of hyperautomation tools, how businesses can use them, and how hyperautomation is predicted to evolve in the next few years.\n\n\n## What is hyperautomation?\n[Gartner defines](https://www.gartner.com/en/information-technology/glossary/hyperautomation) **hyperautomation** as \"a business-driven, disciplined approach that organizations use to rapidly identify, vet and automate as many business and IT processes as possible. Hyperautomation involves the orchestrated use of multiple technologies, tools or platforms.\"\n\nExamples of **hyperautomation tools** are:\n* no-code/low-code application platforms (N/LCAP)\n* workflow automation tools (WAT)\n* robotic process automation (RPA)\n* Artificial Intelligence (AI) and Machine Learning (ML) \n* chatbots and conversational agents\n\n[The hyperautomation-enabling software market has been rising in the past two years and is expected to reach $596bn in 2022.](https://www.statista.com/statistics/1234927/worldwide-hyperautomation-enabling-software-market/)\n\n## How is hyperautomation different from automation?\n**Automation** refers to the accomplishment of a specific task without manual or human intervention. For example, you can use a no-code [workflow that creates tickets from form submissions](https://n8n.io/workflows/791) automatically, instead of doing this manually. Automation is well-suited for [repetitive, boring, regular, rule-based, software-based, and time-consuming tasks](https://n8n.io/features-of-tasks-that-can-be-automated) at small scale.\n\n**Hyperautomation** refers to the combination and connection of several automated workflows, thus creating an orchestrated automation. This orchestration feature takes automation to the *hyper* level, allowing businesses to scale individual processes. Taking the example above a step further, you can add this workflow alongside an ML model or service that detects the sentiment of user reviews, a chatbot that assists customers, an application that processes text from invoices, and a database synchronization to keep the information up-to-date -- all these forming a hyperautomated business.\n\n## Why do we need hyperautomation?\nThe examples above highlight the two main benefits of hyperautomation: **increased productivity** and **seamless scaling of business operations**. Without automation orchestration, business departments risk working out of sync, thus impacting the overall progress and costs of the organization.\n\n[Gartner](https://www.gartner.com/en/webinars/4007544/the-gartner-2022-predictions-hyperautomation-inclusive-of-rpa-low-code-) points out that hyperautomation is driven by two forces: **operational excellence** and **digital acceleration**. Operational excellence is reflected in profits (businesses being able to deliver faster or cheaper), whereas digital acceleration is reflected in adoption (attracting more customers at a faster pace).\n\nTo get to that point, organizations can go two ways. Traditionally, they can increase productivity by hiring department-specific people or IT-skilled engineers who can set up automations. IT teams become fusion teams, where employees with different skills can directly contribute to the automation processes needed in their department.\n\nIn short, hyperautomation helps organizations to save costs, increase efficiency, and overall improve their business model. On an individual level, employees whose tedious tasks are automated have more time to focus on meaningful and creative work, which in turn increases their job satisfaction.\n\n## How are businesses leveraging hyperautomation?\nAccording to a [Gartner study](https://www.gartner.com/en/documents/4006716-gartner-s-2021-digital-business-acceleration-survey-the-speed-of-the-game-has-increased), **businesses have on average 4 automation processes.** This number seems low even for small businesses, considering how many individual tasks are on the to-do lists of employees in every department. However, 80% of senior business executives say they will spend more on digital initiatives in 2022, aiming to accelerate their business (65%) and go to market faster (71%).\n\nBusinesses in all industries can leverage the power of hyperautomation. For example:\n* **E-Commerce** can automate almost the entire journey, from announcing product launches, sending and analyzing emails, issuing invoices, tracking packages, running inventories, and notifying customers.\n* **IT** can automate DevOps and SecOps use cases like contributions to a repository, critical incident response, or vulnerability disclosure.\n\n## What is the future of hyperautomation?\nGartner foresees four trends in hyperautomation in the next few years.\n\n### 1. Orchestrated automation processes\n> \"By 2024 diffuse (siloed) approach to hyperautomation initiatives will drive up initiative specific total cost of ownership by 40-fold, making adaptive governance a differentiating factor in financial performance.\"\n\nThink of how many different apps and services you use in your daily job, and how many more your colleagues from other departments use as well. In fact, 78% of business professionals use three tools from different categories to accomplish their daily tasks, which is not really practical or efficient.\nIn the next few years, businesses will try to turn these disparate (disconnected) automations into orchestrated (connected) hyperautomation workflows. For example, you can have one workflow that synchronizes data between the Sales Pipedrive and Marketing Hubspot.\n\n### 2. Automation marketplaces\n> \"By 2024, growth of “automation marketplaces” will propel 80% of the large enterprises to pivot to principles of composability to minimize operational interdependencies and maximize value of hyperautomation initiatives.\"\n\nBusinesses will change the way they deliver their digital products, replacing packaged applications (like individual projects or products) with composed applications (in the style of catalogs or markets). Think of these \"automation marketplaces\" as curated, interactive exhibitions. \n\nFor example, in a workflow automation marketplace, you would not only see a list of integrations, but also sort them by industry or function, try out automation templates, and learn from supportive content––maybe even tailored to your personal role.\n\n### 3. Vendor-agnostic hyperautomation\n> \"By 2024, the lack of standardization and uniformity in vendor pricing structures will continue driving 40% of clients to increase hyperautomation vendor-agnostic business capabilities.\"\n\nIt's hard to find the one tool that ticks all the boxes: affordable price, powerful functionality, intuitive UI, blazing speed. Commonly, businesses are willing to compromise on some features for the sake of simplicity (keep their processes on one platform––at the risk of vendor lock-in.\n\nHyperautomation tools can diminish this risk, since they make it possible to interconnect apps services. As a consequence, in the future businesses will move away from a commitment-based model to a consumption-based model, preferring to combine features of different tools until it's a match for their use case. \n\n### 4. Infrastructure automation\n> \"By 2024, 40% of organizations will use managed service provider hyperautomation offerings to fill infrastructure operations gaps fortifying a foundation for TCO and scaled automation.\"\n\nThe COVID-19 pandemic has forced many companies to go into remote work mode. But with employees working around the world, it's challenging to ensure smooth connectivity, data security, incident response management, timely decision-making, and ongoing support and maintenance.\n\nTo create a solid infrastructure for these processes, businesses will rely on hyperautomation tools. For example, you can build no-code workflows for automatic [incident response](https://n8n.io/blog/learn-to-automate-your-factorys-incident-reporting-a-step-by-step-guide/) or [database monitoring](https://n8n.io/blog/database-monitoring-and-alerting-with-n8n/).\n\n## Start automating!\nGoing back to the questions in the introduction, we hope the information in this post helps you make a compelling case for automation in your workplace, rest assured that your investment in automation tools is worth every penny, and dare to explore the hyperautomation space.\n\nOne more thing: 3% of hyperautomation professionals characterize their organization as having a high impact on hyperautomation governance. Are you going to be among them?\n\n> This post was originally published on the [n8n blog](https://n8n.io/blog/hyperautomation-trends/)."
    },
    {
      "id": "/2022/01/15/accessible-language",
      "metadata": {
        "permalink": "/blog/2022/01/15/accessible-language",
        "source": "@site/blog/2022-01-15-accessible-language.md",
        "title": "How to write in accessible language",
        "description": "As a technical writer creating tutorials and courses for users of all levels, I'm constantly re-evaluating my content, looking for ways to make it more accessible and easy to understand. I had one such oportunity yesterday, when I attended the workshop Improve Your Writing Using Accessible Language, held by Amy Dickens and organized by You Got This. In this post, I'll share with you my learnings from the workshop, combining the speaker's slides with my personal notes.",
        "date": "2022-01-15T00:00:00.000Z",
        "formattedDate": "January 15, 2022",
        "tags": [
          {
            "label": "conferences",
            "permalink": "/blog/tags/conferences"
          },
          {
            "label": "writing",
            "permalink": "/blog/tags/writing"
          }
        ],
        "readingTime": 4.03,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "post",
          "title": "How to write in accessible language",
          "tags": [
            "conferences",
            "writing"
          ],
          "share-description": "Learnings from the YouGotThis workshop \"Improve Your Writing Using Accessible Language\""
        },
        "prevItem": {
          "title": "How hyperautomation will transform business operations",
          "permalink": "/blog/2022/01/27/hyperautomation-trends"
        },
        "nextItem": {
          "title": "How to synchronize data between two systems (one-way vs. two-way sync)",
          "permalink": "/blog/2021/11/18/data-sync-workflows"
        }
      },
      "content": "As a technical writer creating tutorials and courses for users of all levels, I'm constantly re-evaluating my content, looking for ways to make it more accessible and easy to understand. I had one such oportunity yesterday, when I attended the workshop [***Improve Your Writing Using Accessible Language***](https://yougotthis.io/talks/improving-writing-using-accessible-language), held by Amy Dickens and organized by [You Got This](https://yougotthis.io/events/cyberspace/). In this post, I'll share with you my learnings from the workshop, combining the speaker's slides with my personal notes.\n\n\n## What is accessible language?\nAccessible language is a way of writing that makes is easy for the reader to process the text and understand the message. Though this might sound like the expected or natural way to communicate, sometimes we even consciously choose to use inaccessible (complex) language, for different reasons:\n\n* it's what we're **taught in school**: Using complex language is a sign of higher education. Think of the term papers and dissertations you had to write as a student, the legislations written in legalese, and reasearch papers written in academese––it takes at least two passes to get through these texts.\n\n* we want to **sound intelligent**: A rich vocabulary sprinkled with Latin maxims is one (annoying) way to show off you're well-read, but in most cases this technique only complicates the intended message.\n\n* we're talking to a **technical audience**: Each profession has its own jargon, a set of industry-specific words and expressions that members use to signal their domain knowledge and belonging to the group.\n\nComplex language makes it hard for the reader to process the text or understand the message. Inaccessible language creates frustration for both the reader, who can't get the information she needs, and for the writer, who can't get his point across. Why should you care about this?\n\n## Why do we need accessible language?\nAs a writer, you must always keep in mind that you're **writing for people** (not for a search algorithm, not for a machine). Of these people, many possibly have an (in)visible disability that affects the way the consume your content. For example, people with visual impairments might not be able to read small fonts, those with dyslexia would find it hard to read serif fonts, and those with autism spectrum disorders might not get your irony or metaphors.\n\nAccessible language puts the readers' needs first, helping them understand the message the first time they read it. Content written in accessible language appeals to a wider audience and improves international readability.\n\n## How to write in accessible language\nAmy Dickens proposed five practical ways of making writing more accessible:\n\n1. **Use short and concise sentences:** Stick to one idea (per sentence) helps the reader focus on the main message. If you need to convey several pieces of information in one go, break down the sentences visibly, by using commas or lists. Also, remove any unnecessary words (usually adverbs) and keep your sentences at an average length of 15-20 words (for print) or 7-10 words (for web).\n\n2. **Say exactly what you mean:** To make your message clear, use a basic vocabulary. If you're writing for the general public, it's best to avoid jargon, but if that's unavoidable, explain a term the first time you use it. Moreover, avoid similes, metaphors, or idioms, as these can be difficult to understand for people who have a mainly literal comprehension of language or who have a different first language. \n\n3. **Use active voice:** Sentences in active voice are usually shorter and quicker to read than those in passive voice. Two ways to \"activate\" your sentences are avoiding nominalizations (e.g. *Submit an application for a loan.* &rarr; *Apply for a loan.*) and following the language-specific basic word order (subject-verb-object in English).\n\n4. **Arrange your writing clearly:** Create a visible structure of your text by using headings, subheadings, lists, and if necessary a table of contents. This way, the reader should be able to get the gist of the text by a quick scan. \n\n5. **Test your content:** Once you've finished writing your piece, run it through a readability test like the [Gunning fog index](https://en.wikipedia.org/wiki/Gunning_fog_index), [Flesch Reading Ease Scale](https://en.wikipedia.org/wiki/Flesch%E2%80%93Kincaid_readability_tests#Flesch_reading_ease), or [SMOG grade](https://en.wikipedia.org/wiki/SMOG) to check how understandable it is for your target audience. Some of these tests are integrated in online writing tools like [Grammarly](https://www.grammarly.com/) and [ghostwriter](https://wereturtle.github.io/ghostwriter/).\n\nNext, Amy Dickens suggested some concrete writing tips specific to four channels:\n\n* **Social media:** Use proper punctuation, avoid replacing words with emojis (not only can they be tricky to decifer, but they can have different meanings across languages/cultures), and use capitalizations (also in hasthags, e.g. *#yougotthis* &rarr; *#YouGotThis*)\n\n* **Email:** Keep the subject line concise, avoid excessive words, and use lists and text formatting to highlight information or action items.\n\n* **Web content:** Apart from the above-mentioned visible structure, use [contextual links](https://www.eebew.com/seo-tutorial/on-page-seo/basic-elements/internal-navigation/contextual-link-building/) and left-aligned text (justified creates gaps in text, which affects the reading experience)."
    },
    {
      "id": "/2021/11/18/data-sync-workflows",
      "metadata": {
        "permalink": "/blog/2021/11/18/data-sync-workflows",
        "source": "@site/blog/2021-11-18-data-sync-workflows.md",
        "title": "How to synchronize data between two systems (one-way vs. two-way sync)",
        "description": "Whether you work in marketing, sales, or data engineering, you most likely use several apps or services for collecting and storing data from different sources. To boost productivity and reduce errors, you can automate various tasks in each system -- from lead capturing in CRMs to e-commerce workflows and data processing pipelines.",
        "date": "2021-11-18T00:00:00.000Z",
        "formattedDate": "November 18, 2021",
        "tags": [
          {
            "label": "n8n",
            "permalink": "/blog/tags/n-8-n"
          },
          {
            "label": "tutorials",
            "permalink": "/blog/tags/tutorials"
          }
        ],
        "readingTime": 9.735,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "post",
          "title": "How to synchronize data between two systems (one-way vs. two-way sync)",
          "tags": [
            "n8n",
            "tutorials"
          ],
          "share-description": "Learn how to build no-code workflows that automatically sync data between your CRMs.",
          "canonical_url": "https://n8n.io/blog/how-to-sync-data-between-two-systems/"
        },
        "prevItem": {
          "title": "How to write in accessible language",
          "permalink": "/blog/2022/01/15/accessible-language"
        },
        "nextItem": {
          "title": "5 ways to keep your skills fresh after finishing a coding bootcamp",
          "permalink": "/blog/2021/11/14/keep-your-skills-fresh-after-coding-bootcamp"
        }
      },
      "content": "Whether you work in marketing, sales, or data engineering, you most likely use several apps or services for collecting and storing data from different sources. To boost productivity and reduce errors, you can automate various tasks in each system -- from [lead capturing in CRMs](/crm-workflows) to [e-commerce workflows](/ecommerce-workflows) and [data processing pipelines](/etl-pipeline-workflow).\n\nBut the challenge is consolidating data across different sources so that the information is consistent within all systems. **Data synchronization** is a process that ensures your data is up-to-date, relevant, and complete, thus helping you make informed decisions for your business. And this process, too, can be automated!\n\nData from two systems can be synced in one direction (one-way sync) or in two directions (two-way sync). In n8n, the [***Merge node***](https://docs.n8n.io/nodes/n8n-nodes-base.merge/#node-reference) can help in the process of syncing data. This node allows you to choose between eight different ways of merging data from two sources in order to then synchronize them.\n\n![Aggregation of data from different sources](./blog_images/mergedatasources.png)\n\nIn this tutorial, you'll learn step-by-step how to build **one-way sync and two-way sync workflows with no code** in n8n. The use case for this post is synchronizing data between two CRMs ([Pipedrive](https://www.pipedrive.com/) and [HubSpot](https://www.hubspot.com/)), but the underlying principle can be applied to any other apps.\n\n## Workflow prerequisites\n\nThe workflows presented in this post are available on our [workflows page](https://n8n.io/workflows). If you want to follow along with the tutorial, you need to have:\n\n-   n8n set up. You can either install the desktop app (for [Mac](https://downloads.n8n.io/file/n8n-downloads/n8n-mac.zip) or [Windows](https://downloads.n8n.io/file/n8n-downloads/n8n-win.zip)), sign up for a [cloud instance](https://www.n8n.cloud/), or set up n8n [locally](https://docs.n8n.io/getting-started/installation/)\n-   Pipedrive and HubSpot accounts and credentials\n\n## One-way synchronization\n\nIn **one-way sync (unidirectional sync)**, information is updated only in one direction. Practically, data are added/updated from a primary location (source) to a secondary location (target) in one direction, but no files data are ever copied back to the primary location.\n\nA one-way sync is useful when information like Contacts or Orders are only ever updated in your source system, or when you back up files. In this case, we say that the source is mirrored to target, as the secondary location is in sync with the primary location.\n\nNext, we'll put the theory into practice and implement a no-code, one-way sync workflow in n8n.\n\n### One-way sync workflow: Sync contacts from Pipedrive in HubSpot\n\nLet's consider a common business use case: storing information about customers (such as contact name and email) in two CRMs, for instance, Pipedrive and HubSpot.\n\n[This workflow](https://n8n.io/workflows/1334) syncs contact information from Pipedrive to HubSpot, so that when the information is updated in Pipedrive, then it's updated in HubSpot as well. The workflow for this use case looks like this:\n\n![Explanation of one-way sync workflow](./blog_images/onewaysync.png)\n\n\n1\\. The *Cron node* schedules the workflow to run every minute.\n\n💡 Schedule or trigger the workflow**: Note that for this use case, you can have [a workflow](https://docs.n8n.io/nodes/n8n-nodes-base.pipedriveTrigger/#example-usage) with the *Pipedrive Trigger node*, which triggers the workflow any time information is updated in Pipedrive. But if you need to sync data between apps or services that don't have an n8n Trigger node, you need to use the *Cron node* as a trigger.\n\n2\\.  The *Pipedrive* and *Hubspot1 nodes* pull in both lists of persons from Pipedrive and contacts from HubSpot.\n\n3\\.  The *Merge node* with the option *Remove Key Matches* identifies the items that uniquely exist in Pipedrive.\n\n4\\.  The *Hubspot2 node* takes those unique items and adds them to HubSpot.\n\nNow let's see how to set up each node.\n\n#### 1\\. Get data from the source CRM\n\nIn the *Pipedrive node*, add your [credentials](https://docs.n8n.io/credentials/pipedrive/#prerequisites) in the Pipedrive API section. Then, configure the following parameters:\n\n-   *Resource*: Person\n-   *Operation*: Get All\n-   *Return All*: Toggle to True\n\n![Configuration of Pipedrive node to get data](https://lh6.googleusercontent.com/q84g65S-bvZiMbrXdjH80l5VClW5tLSDUc8iQSLG6IIxdO6y-MUC15NRV-vvD4T0RW72Yx_MnfMiwv5bNhP5xkK_OmMJpT_CCz4wPOijS0NnaghczWMzsRY2EUl6m8bZCndfemX3)\n\n\n#### 2\\. Get data from the target CRM\n\nIn the *HubSpot node*, add your [credentials](https://docs.n8n.io/credentials/hubspot) in the HubSpot API section. Then, configure the following parameters:\n\n-   *Resource*: Contact\n-   *Operation*: Get All\n-   *Return All*: Toggle to True\n\n![Configuration of HubSpot node to get data](https://lh5.googleusercontent.com/ZIJvzR3xprkrs-ZV4yEiVeIF9GFhS-_8CdglzDDac2CEbHCSSI4KXzKnpTZpuKdQL0T-rBXpqG3ZV4b-uMFZxZag-aKIkF3ctd343iIvDecv5mGNJrZfcAbAJjSoZQc_rJWHLCWq)\n\n\nIn HubSpot, we have three contacts, which are also in Pipedrive. We want to add the fourth person from Pipedrive into HubSpot.\n\n#### 3\\. Merge data from source and target CRMs\n\nNow that the data is all set, it's ready to be merged. For our use case, we want to identify contacts that exist only in Pipedrive and add them to HubSpot.\n\nIn a one-way sync workflow, it's important to know clearly from the beginning which system serves as the source, as that would be **Input 1** (in our case, Pipedrive). The target system would then be **Input 2** (in our case, HubSpot).\n\nThe *Merge node* receives data from Input 1, goes back to Input 2 and executes the node(s) in order, then outputs the merged data from Input 1 and Input 2.\n\nTo match data between two systems, the *Merge node* needs a piece of information that is present in both systems, to serve as a reference. This reference is a key column -- a common column between the two tables that contains key values, which uniquely identify rows in the table.\n\nFor this reason, the key values must be different in each row. The key column can be, for example, the customer's email address, an SKU, or the order number.\n\n![Illustration of the one-way sync process](./blog_images/insertsystemB.png)\n\n\nIn our case, the key column between Pipedrive and HubSpot is *email*, so we'll merge data based on this column.\n\nBack to the workflow, in the *Merge node*, configure the following parameters:\n\n-   *Mode*: Remove Key Matches\\\n    This operation keeps data of Input 1 if it doesn't find a match with an [item](https://docs.n8n.io/getting-started/key-concepts/#item) of data from Input 2.\n-   *Property Input 1*: email[0].value\n-   *Property Input 2*: identity-profiles[0].identities[0].value\\\n    The source key column is passed in *Property Input 1* and the target key column is passed in *Property Input 2*.\n\n💡 For *Property Input 1* and *Property Input 2*, you need to enter the property key in **dot-notation format (as text, not as an expression).**\n\n#### What is dot-notation?\n\nIn n8n, all the data that is passed between nodes is an array of objects. When you execute a node, the returned data is displayed in Table view and JSON view.\n\n![Table view](https://lh3.googleusercontent.com/FY7FCNc8fGL9hAUnCPYjlssa4S0jXdk3scajPIzNP4jYAkGJcarNf0WTs14gzrua99HHlAawtnwym9tqjBzyX7U7uqkr-cAyIenRzMJRxksRp9aDHDk3wy1zLVsyAp8muNZgkKI4)\n\n\n![Table view](https://lh6.googleusercontent.com/y1GeYbW5PRF2twcI0YNU-l7nI979iSfZ8r7g8IXnZ4fRvSMAcbnB7fhKbKxILPZ4lWIaSS4qDlk9ZpjFaoXEz8olIhLuomO5eB3mTVEuqb8YV12nqG6M2ompz_xSuxCx7WXDo1kC)\n\n\n**Dot notation** is a way to access the properties of an object. The dot-notation syntax is `object.property`, so the reference is made from outermost to innermost.\n\nIn the example above, the object (column name) *email* contains three properties: *label*, *value*, and *primary*. To reference the *email* value (email address), we write the dot notation `email[0].value`. The `[0]` references the first item of the email array.\n\n![Configuration of Merge node](https://lh5.googleusercontent.com/DYr5aXAXQzqVqDuHBxMhBQcXxqBTnNFVqSGuJ5R_vynqzmR_ZDbYWR6c6HK9UdbPpiPMFZ-YAyGQ3OxkmDQZvnmpmIKNMk72ePg74JqSI494xIUAbPWE0HK4awfusSetGmDUd4C_)\n\nThe merge operation *Remove Key Matches* returns the data of three contacts.\n\n#### 4\\. Update data in the target CRM\n\nOnce the data is merged, you need to update the information in the target system (HubSpot).\n\nTo update contact information in HubSpot, configure the following parameters in the *HubSpot node*:\n\n-   *Resource*: Contact\n-   *Operation*: Create/Update\n-   *Email > Add Expression > Current Node > Output Data > JSON > email > [Item: 0] > value*: {% raw %}`{{$json[\"email\"][0][\"value\"]}}`{% endraw %}\n-   *Additional Fields > Add Field > First Name > Add Expression > Current Node > Output Data > JSON > first_name*: {% raw %}`{{$json[\"first_name\"]}}`{% endraw %}\n\n![Configuration of HubSpot node to update data](https://lh4.googleusercontent.com/nrBLg2_gGG1VMjmcdt79gjVxvNs1vMR774UAJiEDhpmgin759JZwd9uJRkuJml_Ev7PdX6fa1889yy68PnM8x7lktiaESIKzc7m6ApaNFxFdmhs3WRN2awKTMolqHnJXhOuTChsa)\n\n\n## Two-way synchronization\n\nIn a **two-way sync (or bidirectional sync)**, information is updated in both directions. This means that if the information is updated in the first location, then it will be automatically updated in the second location as well -- and vice-versa. This is assuming that both systems are actively used, so users can enter data in both systems.\n\nA two-way sync is useful when two different systems are actively used to store information, for example, if one system offers more features that are relevant for the sales team, but the relevant company-wide information is stored in another system. In this case, we say that the two systems are equivalent.\n\nLet's see what this looks like in action in a workflow that synchronizes data between Pipedrive and HubSpot, extending the one-sync workflow above.\n\n### Two-way sync workflow: Sync contacts between HubSpot and Pipedrive\n\nIn [this workflow](https://n8n.io/workflows/1333), the information about Pipedrive persons and HubSpot contacts (their name and email) is updated in both Pipedrive and HubSpot when you make changes in either source.\n\nIn a two-way sync workflow in n8n, you essentially have two parallel workflows, each including a *Merge node* that takes input from the opposite and inline workflow. For our use case, the workflow looks like this:\n\n![Two-way sync workflow](./blog_images/twowaysync.png)\n\n\nThe first part of the workflow starts with the *Pipedrive node*, which fetches the data when information about a person is updated in the CRM. The second part of the workflow starts with the *HubSpot node*, which fetches the data when information about a contact is updated in HubSpot.\n\n1.  The *Cron node* schedules the workflow to run every minute.\n2.  The *Pipedrive* and *Hubspot nodes* pull in both lists of persons from Pipedrive and contacts from HubSpot.\n3.  The *Merge1* and *Merge2 nodes* with the option *Remove Key Matches* identify the items that uniquely exist in HubSpot and Pipedrive, respectively.\n4.  The *Update Pipedrive* and *Update HubSpot nodes* take those unique items and add them in Pipedrive and HubSpot, respectively.\n\nNow let's see how to set up each node.\n\n#### 1\\. Get data from both CRMs\n\nThe setup of the *Pipedrive* and *HubSpot nodes* is the same as in the one-way sync workflow above.\n\nIn the *Pipedrive node*, configure the following parameters:\n\n-   *Resource*: Person\n-   *Operation*: Get All\n-   *Return All*: Toggle to True\n\nIn the *HubSpot node*, configure the following parameters:\n\n-   *Resource*: Contact\n-   *Operation*: Get All\n-   *Return All*: Toggle to True\n\n#### 2\\. Merge data from the two CRMs\n\nTo merge data from Pipedrive and HubSpot, you need to use two *Merge nodes*, one for each system.\n\nThe first *Merge1 node* is connected to Pipedrive, so it takes Input 1 from HubSpot and Input 2 from Pipedrive. In the *Merge1 node*, set the following parameters:\n\n-   *Mode*: Remove Key Matches\n-   *Property Input 1*: identity-profiles[0].identities[0].value\n-   *Property Input 2*: email[0].value\n\nThe second *Merge2 node* is connected to HubSpot, so it takes Input 1 from Pipedrive and Input 2 from HubSpot. In the *Merge2 node*, set the following parameters:\n\n-   *Mode*: Remove Key Matches\n-   *Property Input 1*: email[0].value\n-   *Property Input 2*: identity-profiles[0].identities[0].value\n\n![Output of Merge node](https://lh3.googleusercontent.com/11OxKlPm-ppZRl_JLW7411BgYxWtMk3TKIQcKT97r131qBvTj43GQp2afCCpr3j9jo3mHrtGjz7sO5emoyjrA5oBqBeJK2OwPGGkcOr88zdDbJlj9SZasZHVaVfuwNGW-JKF-IpF)\n\n\n#### 3\\. Update data in both CRMs\n\nOnce the data is merged, you need to update the information in both systems.\n\nTo update person information in Pipedrive, configure the following parameters in the *Pipedrive node*:\n\n-   *Resource*: Person\n-   *Operation*: Create\n-   *Name > Add Expression*: {% raw %}`{{$json[\"properties\"][\"firstname\"][\"value\"]}}`{% endraw %}\n-   *Additional Fields > Email > Add item > Add Expression*: {% raw %}`{{$json[\"identity-profiles\"][0][\"identities\"][0][\"value\"]}}`{% endraw %}\n\nTo update contact information in HubSpot, configure the following parameters in the *HubSpot node*:\n\n-   *Resource*: Contact\n-   *Operation*: Create/Update\n-   *Email > Add Expression*: {% raw %}`{{$json[\"email\"][0][\"value\"]}}`{% endraw %}\n-   *Additional Fields > Add Field > First Name > Add Expression*: {% raw %}`{{$json[\"properties\"][\"firstname\"][\"value\"]}}`{% endraw %}\n\n![Output of updated HubSpot node](https://lh5.googleusercontent.com/x1qJVl98Xge8U5CvoiiTCtilRr2HdHb67JBTy0W0Tpze0DtwUblelTAchKFetNUUzXmS2KYgwakyxmaqQixQ3uGTcck5I3_fB9BvOjC8UoTHZP7dx_n9h_OKCwfVHn_u2Es1EvTB)\n\n\nNow both Pipedrive and HubSpot contain the latest, up-to-date information about contacts.\n\n## What's next?\n\nIn this tutorial, you've learned how to build one-way and two-sync workflows to synchronize data between two CRMs. You can further adapt these workflows for different business needs.\n\nFor example, you can sync data between different databases like [Postgres](https://docs.n8n.io/nodes/n8n-nodes-base.postgres/) or [MongoDB](https://docs.n8n.io/nodes/n8n-nodes-base.mongoDb/), sales data between [Salesforce](https://docs.n8n.io/nodes/n8n-nodes-base.salesforce/) and [Shopify](https://docs.n8n.io/nodes/n8n-nodes-base.shopify/), or marketing data between [Google BigQuery](https://docs.n8n.io/nodes/n8n-nodes-base.googleBigQuery/) and [MailChimp](https://docs.n8n.io/nodes/n8n-nodes-base.mailchimp/).\n\n> This post was originally published on the [n8n blog](https://n8n.io/blog/how-to-sync-data-between-two-systems/)."
    },
    {
      "id": "/2021/11/14/keep-your-skills-fresh-after-coding-bootcamp",
      "metadata": {
        "permalink": "/blog/2021/11/14/keep-your-skills-fresh-after-coding-bootcamp",
        "source": "@site/blog/2021-11-14-keep-your-skills-fresh-after-coding-bootcamp.md",
        "title": "5 ways to keep your skills fresh after finishing a coding bootcamp",
        "description": "One year ago at this time, I was nervously making last-minute changes to slides for a presentation. Not just any presentation, but one of my final project(https://github.com/lorenanda/speech-emotion-recognition)) for the Data Science Bootcamp at SPICED Academy, where I spent three full months learning about different machine learning algorithms and Python best practices.",
        "date": "2021-11-14T00:00:00.000Z",
        "formattedDate": "November 14, 2021",
        "tags": [
          {
            "label": "data science",
            "permalink": "/blog/tags/data-science"
          },
          {
            "label": "thoughts",
            "permalink": "/blog/tags/thoughts"
          }
        ],
        "readingTime": 2.855,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "post",
          "title": "5 ways to keep your skills fresh after finishing a coding bootcamp",
          "tags": [
            "data science",
            "thoughts"
          ],
          "share-description": "Tips for maintaining and developing your coding skills after you're done with formal education."
        },
        "prevItem": {
          "title": "How to synchronize data between two systems (one-way vs. two-way sync)",
          "permalink": "/blog/2021/11/18/data-sync-workflows"
        },
        "nextItem": {
          "title": "Market basket analysis with the Apriori algorithm in Python",
          "permalink": "/blog/2021/10/17/market-basket-analysis-with-apriori-algorithm"
        }
      },
      "content": "One year ago at this time, I was nervously making last-minute changes to slides for a presentation. Not just any presentation, but one of my [final project]([https://github.com/lorenanda/speech-emotion-recognition](https://github.com/lorenanda/speech-emotion-recognition)) for the Data Science Bootcamp at SPICED Academy, where I spent three full months learning about different machine learning algorithms and Python best practices.\n\nToday, I work as a technical writer at a startup that is developing a low-code workflow automation tool. Though in this role I don't use my data science and Python skills on a daily basis, I still apply them occasionally in data analyses and personal projects.\n\nIn this post, I'll share with you five tips for maintaining and even developing your coding skills after you're done with formal education.\n\n## 1. Improve your school projects\n\nBootcamps are fast-paced. So much so that you might barely complete some projects before the deadline and if you somehow do, they still won't be perfect. There will always be things left to improve, and you should take the time to do them.\n\nOne way to improve your projects and coding skills is to try new models and libraries. For example, if you did classification with logistic regression, try also with random forest; if you used [Tensorflow](https://www.tensorflow.org/), now try [Keras](https://keras.io/); if you scraped a website with [BeautifulSoup](https://beautiful-soup-4.readthedocs.io/en/latest/), now do it with [Scrapy](https://scrapy.org/). You get the point. \n\n## 2. Work on new projects\n\nEven with all the mandatory projects you'll need to complete in the bootcamp, you'll probably get a  lot of ideas for others. After the bootcamp is the time to explore them!\n\nIdeally, work on real-life projects or some that have business value for the field you're targeting. There are many data sets available for marketing, finance, medicine, and other fields. Find a relevant data set and apply different models to derive insights from raw numbers. For more ideas, check out the [Kaggle](https://www.kaggle.com/) data sets and competitions.\n\n## 3. Code regularly\n\nWhat you do every day matters. Small things add up: actions turn into habits turn into skills. That's why it's important to code regularly. It doesn't have to be a complex project, even a 15-minute coding session or a short exercise counts. \n\nFor example, you can block one hour every Saturday to practice algorithms and data structures on [LeetCode](https://leetcode.com/), [Codewars](https://www.codewars.com/), or [HackerRank](https://www.hackerrank.com/). You'll not only sharpen your coding skills, but also get a confidence boost as you progress through levels and get badges.\n\n## 4. Keep on learning\n\nIn data science, machine learning, and AI, research and applications are advancing fast! New papers, models, libraries, and business applications are coming out almost every day. That's why it's important to keep up with the news and advances in the field.\n\nThere are many resources for this. You can read blogs (like [Towards Data Science](https://towardsdatascience.com/?gi=73ee6fa159ba) and [Data Science Central](https://towardsdatascience.com/?gi=73ee6fa159ba)) and [papers](https://arxiv.org/list/stat.ML/recent), watch videos (for NLP enthusiasts I recommend the YouTube channel [AI Coffee Break with Letitia](https://www.youtube.com/c/AICoffeeBreak)), listen to podcasts, and take online courses.\n\n## 5. Document your learnings\n\nI find the best way to learn something is by teaching it. Explaining something to others helps you structure the learned information and identify problems or issues that are unclear. \n\nTo document your learnings, you can create a blog on [Medium](https://lorenaciutacu.medium.com/) or [dev](http://dev.to/lorena), where you write about your projects. If you're into web development, you can even build your own blog (I made mine with [Jekyll](https://jekyllrb.com/)).\n\nWith these five tips in mind, keep on coding and learning!"
    },
    {
      "id": "/2021/10/17/market-basket-analysis-with-apriori-algorithm",
      "metadata": {
        "permalink": "/blog/2021/10/17/market-basket-analysis-with-apriori-algorithm",
        "source": "@site/blog/2021-10-17-market-basket-analysis-with-apriori-algorithm.mdx",
        "title": "Market basket analysis with the Apriori algorithm in Python",
        "description": "In my previous post, I wrote about using spectral biclustering for making product recommendations. In this post, I'll build on that marketing example and use the Apriori algorithm for analyzing product purchases This example is, again, taken from Giuseppe Bonaccorso's book Mastering Machine Learning Algorithms.",
        "date": "2021-10-17T00:00:00.000Z",
        "formattedDate": "October 17, 2021",
        "tags": [
          {
            "label": "algorithms",
            "permalink": "/blog/tags/algorithms"
          },
          {
            "label": "data science",
            "permalink": "/blog/tags/data-science"
          },
          {
            "label": "Python",
            "permalink": "/blog/tags/python"
          },
          {
            "label": "tutorials",
            "permalink": "/blog/tags/tutorials"
          }
        ],
        "readingTime": 6.16,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "post",
          "title": "Market basket analysis with the Apriori algorithm in Python",
          "tags": [
            "algorithms",
            "data science",
            "Python",
            "tutorials"
          ],
          "comments": true
        },
        "prevItem": {
          "title": "5 ways to keep your skills fresh after finishing a coding bootcamp",
          "permalink": "/blog/2021/11/14/keep-your-skills-fresh-after-coding-bootcamp"
        },
        "nextItem": {
          "title": "Making product recommendations using Spectral Biclustering in Python",
          "permalink": "/blog/2021/10/03/spectral-biclustering-for-marketing-in-python"
        }
      },
      "content": "In my [previous post](/blog/2021-10-03-spectral-biclustering-for-marketing-in-python.mdx), I wrote about using spectral biclustering for making product recommendations. In this post, I'll build on that marketing example and use the Apriori algorithm for analyzing product purchases This example is, again, taken from Giuseppe Bonaccorso's book [*Mastering Machine Learning Algorithms*](#references).\n\n## Market Basket Analysis\nMarket basket analysis is a data mining technique used by retailers to understand purchasing patterns of their customers. This technique is usually applied on large data sets, such as purchase history in a supermarket, and can reveal how products are grouped and what products are likely to be purchased together.\n\n![market basket analysis](https://upload.wikimedia.org/wikipedia/commons/4/4a/AffinityAnalysis.png)\n\nIn formal terms, given a set of products $P = \\{p_1, p_2, ..., p_n\\}$, a transaction $T_i$ is a subset of $P$, expressed as $P \\supseteq T_i = \\{p_i, p_j, ..., p_k\\}$. A collection of transactions is a set of subsets, $T_i$, expressed as $C = \\{T_1, T_2, ..., T_p\\}$.\n\nNow, given a transaction containing a set of items, what is the probability of another item $p_t$ being bought? In other words, if a customer purchased lemons and salt, what product are they likely to purchase next (or how likely is it that they will purchase a bottle of tequila for the mix)?\n\nThis is the question at the core of market basket analysis, which aims to mine all existing association rules expressed as $if (p_i, p_j, ..., p_k) \\in T_g \\Rightarrow P(p_t) \\gt \\pi$. This expressions means that for a transaction of products, the probability of finding the item $P(p_t)$ is greater than a discriminant threshold $\\pi$.\n\n## Understanding the Apriori algorithm\nThe Apriori algorithm was proposed by [Agrawal and Srikant (1994)](#references) as a solution for performing market basket analysis on large transaction data sets. This algorithm can reveal the most important association rules in the dataset, thus guding the planning of marketing strategies, such as product segmentation or promotion.\n\nGiven the maximum number of items in a transaction, the Apriori algorithm proceeds in 4 steps:\n1. **Computing** the support of all products $S(p_i)$\n2. **Removing** all items with $S(p_i) \\lt \\pi$ (with $\\pi \\gt 0$, since we are not interested in products that are rarely sold).\n3. **Analyzing** all couples, triplets, and so on, and applying the same filter.\n4. **Splitting** the results of each pass (item sets) into disjointed subsets to make up the association rules.  \n   The association rules have the form of logical implications in modus ponens: $if A \\Rightarrow B$ and $A$ is true, $B$ is also true. Practically, an association rule is, for example: `{bread, hummus} -> {avocado}`.  \n   Given the association rules, we also need to know the confidence of a rule, i.e. how frequently the consequent is true when the whole rule is true. The confidence of a rule is represented as an item set: $C(I) = \\frac{S(I)}{S(A)}$, where $S(I)$ is the support of the item set before the split and $I$ has been split into $A$ and $B$.\n\nThe Apriori algorithm has proven to be simple and effective in practice. However, it also has the drawback that it needs larget thresholds for very large datasets. Therefore, it's recommended to segment the customers and apply Apriori to to each subset of transactions.\n\n## Implementing the Apriori algorithm in Python\nLet's put the theory into practice! For this example, we have a data set that contains information about 100 users and 100 transactions (purchases) of 100 different products. Every purchase has a number of items associated with it. For example, Purchase 1 consists of two products (Product 2 and Product 5), Purchase 2 consists of five products, and so on. We want to know what other product could be associated with each transaction.\n\nFirst of all, we need to install the Python library [`efficient-apriori`](https://pypi.org/project/efficient-apriori/):\n\n```python\npip install efficient-apriori\n```\n\nNow we can mine the shopping transaction data set introduced in the previous post. We want to detect all rules that have a maximum length of 2. \n\n```python\nfrom efficient_apriori import apriori\n\n_, rules = apriori(transactions,\n                   min_support = 0.15,\n                   min_confidence = 0.75,\n                   max_length = 3,\n                   verbosity = 1) #show messages throughout the learning process\n```\n\nThe code above outputs the following result:\n\n```\nGenerating itemsets.\n Counting itemsets of length 1.\n  Found 100 candidate itemsets of length 1.\n  Found 100 large itemsets of length 1.\n Counting itemsets of length 2.\n  Found 4950 candidate itemsets of length 2.\n  Found 1672 large itemsets of length 2.\n Counting itemsets of length 3.\n  Found 14634 candidate itemsets of length 3.\n  Found 170 large itemsets of length 3.\nItemset generation terminated.\n\nGenerating rules from itemsets.\n Generating rules of size 2.\n Generating rules of size 3.\nRule generation terminated.\n```\n\nThis code output describes the steps enumerated in the [theoretical part](#theory-apriori-algorithm). The Apriori algorithm defines the item sets, starting with 1 and ending with 3 (the set limit), filters out the elements with support below 0.15 (the set threshold), and generates rules from the item sets. Now we want to see the generated rules:\n\n```python\nprint(\"Number of rules: {}\".format(len(rules)))\nfor r in rules:\n    print(r)\n```\n\nThe code above returns the following result:\n\n```\nNumber of rules: 233\n{P92} -> {P36} (conf: 0.750, supp: 0.240, lift: 1.923, conv: 2.440)\n{P23} -> {P20} (conf: 0.769, supp: 0.200, lift: 2.137, conv: 2.773)\n{P93} -> {P33} (conf: 0.765, supp: 0.260, lift: 2.012, conv: 2.635)\n{P19, P64} -> {P10} (conf: 0.750, supp: 0.150, lift: 1.875, conv: 2.400)\n{P10, P20} -> {P64} (conf: 0.789, supp: 0.150, lift: 2.134, conv: 2.992)\n{P10, P64} -> {P31} (conf: 0.773, supp: 0.170, lift: 1.981, conv: 2.684)\n{P10, P31} -> {P64} (conf: 0.810, supp: 0.170, lift: 2.188, conv: 3.307)\n{P33, P39} -> {P10} (conf: 0.750, supp: 0.150, lift: 1.875, conv: 2.400)\n...\n```\n\nThis means that the Apriori algorithm found 233 rules with confidence between 0.75 and 1.00, and support between 0.15 and 0.26 (close to the set value 0.15). The results can be read like this: \n- If a customer bought product 92, there is a 75% chance that they will buy product 36.\n- If a customer bought the products 10 and 31, there is an 81% chance that they will buy product 64.\n\nThe high confidence is helpful for excluding all the rules with a low probability. However, we should also consider the **lift** value, expressed as: \n\n$L(I) = \\frac{C(I)}{S(B)} = \\frac{S(I)}{S(A)S(B)}$\n\nThe lift value $L(I)$ represents the ratio between the joint probability of the rule and the product of the probabilities of the antecedent and consequent. \n\nThe lift will always be greater than or equal to the confidence. As $C(I) = L(I)S(B)$, the smaller $S(B)$ is, the larger $L(I)$ has to be. The lift value of 1 indicates that all transactions contain the element(s) in $B$–the ideal case. In real-world cases, this is almost impossible, so the lift is generally greater than 1 and is reasonable in the range (1.5, 2.5).\n\nOn the other hand, the support value $S(B)$ of 1 indicates a trivial rule, that most transactions contain the specific product (for example, shopping bags).\n\nKeep this in mind when setting the threshold for lift and, better yet, define an interval.\n\n-----\n## References\n1. Bonaccorso, G. (2020). Clustering and Unsupervised Models for Marketing. In: *Mastering Machine Learning Algorithms*, 233–246. UK: Packt Publishing.\n2. Agrawal, R.  & Srikant, R. (1994). Fast Algorithms for Mining Association Rules in Large Databases. In: *Proceedings of the 20th International Conference on Very Large Data Bases (VLDB '94)*, 487–499. Morgan Kaufmann Publishers Inc."
    },
    {
      "id": "/2021/10/03/spectral-biclustering-for-marketing-in-python",
      "metadata": {
        "permalink": "/blog/2021/10/03/spectral-biclustering-for-marketing-in-python",
        "source": "@site/blog/2021-10-03-spectral-biclustering-for-marketing-in-python.mdx",
        "title": "Making product recommendations using Spectral Biclustering in Python",
        "description": "My roles in data analytics/science so far have always been focused on online marketing applications, such as analyzing the impact of ad campaigns, the engagement of users with a website, or the performance of blog posts. Machine Learning algorithms and Python libraries like scikit-learn can help marketers derive insights from user data and establish connections between their features or actions that otherwise would go unnoticed.",
        "date": "2021-10-03T00:00:00.000Z",
        "formattedDate": "October 3, 2021",
        "tags": [
          {
            "label": "algorithms",
            "permalink": "/blog/tags/algorithms"
          },
          {
            "label": "data science",
            "permalink": "/blog/tags/data-science"
          },
          {
            "label": "Python",
            "permalink": "/blog/tags/python"
          },
          {
            "label": "tutorials",
            "permalink": "/blog/tags/tutorials"
          }
        ],
        "readingTime": 4.01,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "post",
          "title": "Making product recommendations using Spectral Biclustering in Python",
          "tags": [
            "algorithms",
            "data science",
            "Python",
            "tutorials"
          ],
          "comments": true
        },
        "prevItem": {
          "title": "Market basket analysis with the Apriori algorithm in Python",
          "permalink": "/blog/2021/10/17/market-basket-analysis-with-apriori-algorithm"
        },
        "nextItem": {
          "title": "6 features of tasks that can be automated",
          "permalink": "/blog/2021/09/28/automatable-tasks"
        }
      },
      "content": "My roles in data analytics/science so far have always been focused on online marketing applications, such as analyzing the impact of ad campaigns, the engagement of users with a website, or the performance of blog posts. Machine Learning algorithms and Python libraries like scikit-learn can help marketers derive insights from user data and establish connections between their features or actions that otherwise would go unnoticed.  \n\nIn my search for resources that bridge data science and marketing, I found the chapter *Clustering and Unsupervised Models for Marketing*[$^1$](#references), which explains how to use spectral biclustering for making product recommendations. In this post, I share with you my learnings about this algorithm and its implementation in Python, as taken from the book.\n\n## Biclustering methods\nBiclustering is a clustering method that concomitently operates on two levels that are correlated by the presence of a medium (e.g., customers and products by rating). Biclustering aims to find the regions where the medium is cohesive (e.g., the rating is high or low) by rearranging the structure of both levels.\n\nBiclustering operates on a matrice \\\\( A ∈ R^{n \\times m} \\\\). In a marketing example, the rows and columns can represent customers and products. Each element $a_{ij} ∈ A$ indicates a rating (or zero) for the product $p_j$ given by the customer $c_i$. $A$ has an underlying checkerboard structure, where the compact regions (biclusters) represent sub-matrices.\n\n## Understanding the Spectral Biclustering algorithm\nSpectral Biclustering is an algorithm developed by [Kluger et al. (2003)](https://pubmed.ncbi.nlm.nih.gov/12671006/) for classifying genes and conditions. Spectral Biclustering relies on Singular Value Decomposition (SVD) and was initially applied to bioinformatics tasks, but has found applications in other fields as well.\n\nThe algorithm consists of five steps:\n\n1. **Bistochaization**, i.e. a preprocessing iterative phase where the values of $a_{ij}$ are adjusted so that all row and column sums add up to a constant common value (usually 1).\n2. **Decomposition** of the bistochastic matrix $A_b$ using SVD. $A_b$ is decomposed into left and right singular vectors ($A_bA_b^T$ and $A_b^TA_b$). The singular values are sorted in descending order and the singular vectors are rearranged accordingly.\n3. **Ranking** of the singular vectors by analyzing their similarity with $\\bar{v}_p$, the checkerboard structure to be highlighted by biclustering. \n4. **Projection** of the data set onto the sub-space spanned by the columns of $P_k$.\n5. **Application of K-Means** to find the labelling for the $k$ clusters. This operation yields two label vectors, $\\bar{r}$ and $\\bar{c}$. \n\n## Implementing Spectral Biclustering in Python\nTo implement spectral biclustering, we first need a data set. For practical reasons, we can generate an artificial data set that consists of:\n- 100 users\n- 100 transactions (purchases)\n- 100 products\n- ratings in the range 1-10 (10 different possible biclusters)\n\n```python\nimport numpy as np\n\nnb_users = 100\nnb_products = 100\n\nitems = [i for i in range(nb_products)]\ntransactions = []\nratings = np.zeros(shape=(nb_users, nb_products), dtype=np.int)\n\nfor i in range(nb_users):\n    n_items = np.random.randint(2, 60)\n    transaction = tuple(np.random.choice(items, replace=False, size=n_items))\n    transactions.append(list(map(lambda x: \"P{}\".format(x+1), transaction)))\n\n    for t in transaction:\n        rating = np.random.randint(1, 11)\n        ratings[i, t] = rating\n```\nWe can visualize the generated ratings on a heatmap:\n\n```python\nimport seaborn as sns\nsns.heatmap(ratings, center=0)\n```\n\n![heatmap of ratings](data:image/png;base64,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)\n\nNow we can implement the spectral biclustering algorithm with scikit-learn:\n\n```python\nfrom sklearn.cluster.bicluster import SpectralBiclustering\n\nsbc = SpectralBiclustering(n_clusters = 10,\n                           n_best = 5, #project the dataset on the top 5 singular vectors\n                           svd_method = \"arpack\", #good for small/medium matrices\n                           n_jobs = -1,\n                           random_state = 1000)\n\nsbc.fit(ratings)\n```\n\nNext, we need to compute the final matrix using the outer product of the sorted row and column indices:\n\n```python\nrc = np.outer(np.sort(sbc.row_labels_) + 1,\n              np.sort(sbc.column_labels_) + 1)\n```\n\nWe can visualize the rearranged matrix on a heatmap as well:\n\n```python\nsns.heatmap(rc)\n```\n\n![heatmap of rearranged matrix](data:image/png;base64,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)\n\nFinally, we can use the matrix for a practical marketing use case: determining the group of users that rated a group of five products in order to send those users a newsletter with product recommendations. To do this, we need to select all the rows and columns associatd with the biclusters with an index of 5 (because 0 means no rating).\n\n```python\nprint(\"Users: {}\".format(np.where(sbc.rows_[8, :] == True)))\nprint(\"Products: {}\".format(np.where(sbc.columns_[8, :] == True)))\n```\n\nThe code above outputs the following result: \n```python\nUsers: (array([ 7, 26, 30, 44, 54, 66, 95]),)\nProducts: (array([61, 64]),\n```\n\nThis means that we need to check the products in the `Products` array, select other products that are similar to those, and send them to the users in the `Users` array.\n\nThere you have it–Spectral Biclustering implemented in Python with scikit-learn for marketing recommendations.\n\n-------\n## References\n1. Bonaccorso, G. (2020). Clustering and Unsupervised Models for Marketing. In: *Mastering Machine Learning Algorithms*, 233–246. UK: Packt Publishing.\n2. Kluger, Y. et al. (2003). Spectral biclustering of microarray data: coclustering genes and conditions. *Genome research, 13*(4), 703–716."
    },
    {
      "id": "/2021/09/28/automatable-tasks",
      "metadata": {
        "permalink": "/blog/2021/09/28/automatable-tasks",
        "source": "@site/blog/2021-09-28-automatable-tasks.md",
        "title": "6 features of tasks that can be automated",
        "description": "You're working long hours now, six in the morning to six in the afternoon. Sometimes even eight in the afternoon, six days a week. Sometimes seven days a week. It's a long hustle but it keeps you busy.",
        "date": "2021-09-28T00:00:00.000Z",
        "formattedDate": "September 28, 2021",
        "tags": [
          {
            "label": "n8n",
            "permalink": "/blog/tags/n-8-n"
          }
        ],
        "readingTime": 4.415,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "post",
          "title": "6 features of tasks that can be automated",
          "tags": [
            "n8n"
          ],
          "share-description": "Stop doing that! If your work doesn’t spark joy, you should automate it (or do something else entirely)–but first, check if it meets these six criteria.",
          "canonical_url": "https://n8n.io/blog/features-of-tasks-that-can-be-automated/"
        },
        "prevItem": {
          "title": "Making product recommendations using Spectral Biclustering in Python",
          "permalink": "/blog/2021/10/03/spectral-biclustering-for-marketing-in-python"
        },
        "nextItem": {
          "title": "Create a toxic language detector for Telegram in 4 steps",
          "permalink": "/blog/2021/09/21/toxic-language-workflow"
        }
      },
      "content": "You're working long hours now, six in the morning to six in the afternoon. Sometimes even eight in the afternoon, six days a week. Sometimes seven days a week. It's a long hustle but it keeps you busy.\n\nBusy but unfulfilled, because many of the things you do are plain boring, repetitive, unengaging, and could probably be done (better) by a machine. If you've found your way to this article, it means you've had it with manual work and you're ready to start automating at least part of it. Congratulations, welcome to the future of work!\n\nThere are plenty of things in our daily lives that surely would rather be automated than half-heartedly accomplished by a bored human who, mind you, might even make mistakes. Workflow automation platforms like n8n enable you to automate even complex tasks with no code (or a bit of JavaScript, if you insist).\n\nBut how do you decide where to begin? In this post, we present to you **six features of tasks that can (and should) be automated**.\n\n\n## 1\\. Repetitive tasks\n\nDownload this from here, upload it there, write this, click that. Repeat 10 times a day, 50 times a year, until the end of time. Or until you discover automation, because tasks like this shouldn't be accomplished manually anymore.\n\nFor example, [getting your daily news](https://docs.n8n.io/courses/level-one/chapter-2.html), [checking the weather,](https://docs.n8n.io/getting-started/create-your-first-workflow/daily-weather-notifications/) [creating backups of your work](https://n8n.io/workflows/1222) (this is also a reminder to do it!), or cross-posting [articles on different channels](https://n8n.io/blog/learn-how-to-automatically-cross-post-your-content-with-n8n/) (like we do with our blog posts on [Medium](https://medium.com/n8n-io) and [dev.to](https://dev.to/n8n)) are all activities that can be automated with no code.\n\n## 2\\. Boring tasks\n\n\"My favorite part of my job is copy-pasting data from one file into a spreadsheet!\", said no one ever. More likely, you wish you could be doing anything but that. If a task is unengaging, it doesn't involve decision-making, higher-order thinking, creativity, or \"the human touch\", then it's a good candidate for automation.\n\nFor example, a common activity in sales is collecting information about companies (like the number of employees, industry, and location) from the website of a business event, in order to create contacts or leads in a CRM. Instead of manually copy-pasting data, you can create a workflow that does [web-scraping](https://n8n.io/blog/how-uproc-scraped-a-multi-page-website-with-a-low-code-workflow/), data transfer, and even [email validation](https://n8n.io/workflows/1055) for you.\n\n## 3\\. Frequent, regular tasks\n\nIt's the end of yet another month and you need to calculate yet another budget for your business, run an inventory on your products and orders. You need to set a reminder and block a full day to get this job done--a day when you could be doing more exciting work or even take a holiday.\n\nIf the task has to be done at the same time or interval and it involves the same sequence of steps, then it could be automated. In fact, we've already built a [workflow](https://n8n.io/workflows/1207) for the use case mentioned above.\n\n[![Workflow for creating backups on GitHub](https://f000.backblazeb2.com/file/n8n-website-images/5c7c9905bf8d406b9f222a375c1b8dda.png)](https://n8n.io/workflows/1222 \"Workflow for creating backups on GitHub\")\n\n\n## 4\\. Rule-based tasks\n\nMost automation-friendly tasks are rule-based, meaning they follow a logical sequence of steps in the form of \"if A, then B, else C\". This is the kind of low-level decision-making that can be established by a human and, if you know that the process won't change, delegated to a machine.\n\nFor example, filtering sales orders based on their value was a boring task that our friend [Nathan had to do for his team](https://docs.n8n.io/courses/level-one/chapter-3.html), before we taught him how to [automate it](https://docs.n8n.io/courses/level-one/chapter-4.html). In the same way, you could [automate your e-commerce business](https://n8n.io/blog/no-code-ecommerce-workflow-automations/), for example by [filtering positive and negative reviews](https://n8n.io/workflows/1075) or [issuing invoices](https://n8n.io/workflows/1206).\n\n## 5\\. Software-based tasks\n\nDid you know that U.S. users had [on average 20 apps](https://www.statista.com/statistics/267309/number-of-apps-on-mobile-phones/) installed on their mobile? And that organizations worldwide were using [on average 80](https://www.statista.com/statistics/1233538/average-number-saas-apps-yearly/) software as a service (SaaS) applications? Now think of all the tasks that you're doing daily and how many of those involve transferring or synchronizing data between different apps, without human input.\n\nFor example, if you need to sync data between your CRM and a database, you can create a no-code workflow for that and let the two systems communicate with each other. Don't act as an intermediary machine, you're just making it awkward.\n\n## 6\\. Time-consuming tasks\n\nI don't know about you, but for me, the most annoying thing about boring tasks is that they are time-consuming. Fine, life and work are not always rainbows and butterflies, sparkling with creativity and meaningful activities. I can do some brainless tasks for a while if needed, but when they start taking up hours of my precious time--that's where I draw the line and reach for n8n.\n\nFor example, in one of my previous roles, I was responsible for creating reports, which involved aggregating data from different sources (Google Analytics, BigQuery, Salesforce, Postgres), calculating some custom metrics, and sending the results to management or clients. This kind of reporting could take up to two hours, every month/quarter/year--or only a few minutes once to set up a [workflow](https://n8n.io/workflows/892) in n8n.\n\n[![Workflow for running inventories on Shopify orders](https://f000.backblazeb2.com/file/n8n-website-images/338ebf4b187a49f2a376b91394ea62a6.png)](https://n8n.io/workflows/1207 \"Workflow for running inventories on Shopify orders\")\n\n\nIn this post, you've learned how to identify tasks that can be automated. To sum up, automatable tasks are **repetitive, boring, regular, rule-based, software-based, and time-consuming**.\n\n> This post was originally published on the [n8n blog](https://n8n.io/blog/features-of-tasks-that-can-be-automated/)."
    },
    {
      "id": "/2021/09/21/toxic-language-workflow",
      "metadata": {
        "permalink": "/blog/2021/09/21/toxic-language-workflow",
        "source": "@site/blog/2021-09-21-toxic-language-workflow.md",
        "title": "Create a toxic language detector for Telegram in 4 steps",
        "description": "When was the last time you talked to someone online, be it friends, coworkers, or even strangers? Nowadays, you most likely do it every day.",
        "date": "2021-09-21T00:00:00.000Z",
        "formattedDate": "September 21, 2021",
        "tags": [
          {
            "label": "AI",
            "permalink": "/blog/tags/ai"
          },
          {
            "label": "n8n",
            "permalink": "/blog/tags/n-8-n"
          },
          {
            "label": "tutorials",
            "permalink": "/blog/tags/tutorials"
          },
          {
            "label": "NLP",
            "permalink": "/blog/tags/nlp"
          }
        ],
        "readingTime": 6.77,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "post",
          "title": "Create a toxic language detector for Telegram in 4 steps",
          "tags": [
            "AI",
            "n8n",
            "tutorials",
            "NLP"
          ],
          "share-description": "Leverage the power of automation and machine learning to enable kinder online discussions.",
          "canonical_url": "https://n8n.io/blog/create-a-toxic-language-detector-for-telegram/"
        },
        "prevItem": {
          "title": "6 features of tasks that can be automated",
          "permalink": "/blog/2021/09/28/automatable-tasks"
        },
        "nextItem": {
          "title": "How to get started with CRM automation (with 3 no-code workflow ideas)",
          "permalink": "/blog/2021/09/14/crm-workflows"
        }
      },
      "content": "When was the last time you talked to someone online, be it friends, coworkers, or even strangers? Nowadays, you most likely do it every day.\n\n## Toxic language online\n\nOnline communication platforms like Telegram, Reddit, or Discord have made it possible for people from all over the world to connect and share their thoughts on pretty much any topic, instantly.\n\nThis can be an enriching experience for users, but these platforms can also foster toxicity like cyberbullying, threats, and insults, forcing some users offline and silencing their voices.\n\n## Managing toxic language with Perspective API\n\nOne solution to this problem comes from [Jigsaw](https://jigsaw.google.com/) and Google's Counter Abuse Technology team, who developed [*Perspective API*](https://www.perspectiveapi.com/): a free API that uses machine learning to identify toxic language in English, Spanish, French, German, Portuguese, Italian, and Russian. Toxic language is defined here as\"a rude, disrespectful, or unreasonable comment that is likely to make someone leave a discussion\".\n\n![perspective gif](./blog_images/n8n_toxlan_perspective.gif)\n\n\nIn practice, Perspective scores a phrase based on the perceived impact the text may have in a conversation. The phrase can be analyzed on different attributes: flirtation, identity attack, insult, profanity, sexually explicit, threat, and (severe) toxicity. Keep in mind though that machine learning models can only be as good as the data they're trained on. This means that they may misclassify as toxic some innocent comments (and vice versa), so the flagged comments should be reviewed by a human eye.\n\nPerspective API has been implemented by [several major publishers and platforms](https://www.perspectiveapi.com/case-studies/) like Reddit, The New York Times, and DISQUS, helping them moderate online comments. At n8n, we communicate with our 16,000+ community members in the [Discourse forum](https://community.n8n.io/), on [Discord](https://discord.gg/vWwMVThRta), [Twitter](https://twitter.com/n8n_io), and even via [Telegram for Spanish speakers](https://t.me/comunidadn8n). We value open, [inclusive](https://n8n.io/workflows/982), and respectful communication and want to ensure that everyone has a positive experience in the n8n community -- and beyond.\n\nTo this end, we used the Perspective API to build the [***Google Perspective node***](https://docs.n8n.io/nodes/n8n-nodes-base.googlePerspective/)*,* which allows you to integrate toxic language detection in your workflows.\n\n## Workflow for detecting toxic language in Telegram messages\nTo give you an idea of how you can use the *Google Perspective node*, we created [a workflow](https://n8n.io/workflows/1216) that detects toxic language in messages sent in a Telegram chat and replies with a warning message.\n\n![workflow](./blog_images/n8n_toxlan_workflow.png)\n\n-   ***Telegram Trigger node*** starts the workflow when a new message is sent in a Telegram chat.\n-   ***Google Perspective node*** analyzes the text of the message and returns a probability value between 0 and 1 of how likely it is that the content is toxic.\n-   ***IF node*** filters messages with a toxic probability value above 0.7.\n-   ***Telegram node*** sends a message in the chat with the text \"I don't tolerate toxic language\" if the probability value is above 0.7.\n-   ***NoOp node*** takes no action if the probability value is below 0.7. This node is optional and serves only to show that the workflow can be extended in this direction.\n\nNow let's see how to configure each node step by step. If you haven't built an n8n workflow yet, you might want to take a look at our [quickstart guide](https://docs.n8n.io/quickstart/) or take the [beginner's course](https://docs.n8n.io/courses/level-one/). This will help you understand the configuration of the nodes used in this workflow.\n\n### 1\\. Get new messages from Telegram\n\nFirst of all, you need to create a Telegram bot and get credentials. Start a chat with [Botfather](https://telegram.me/BotFather) in your Telegram account and follow the instructions to create your bot and get credentials. Make sure you add your newly created bot to the channel you want to monitor.\n\nThen, open the *Telegram Trigger node* and add your *Credentials Name* and *Access Token* in *Telegram API*.\n\nIn the *Updates* field select: *message, edited_message, channel_post,* and *edited_channel_post*. These update options will trigger the workflow when a text message is posted.\n\nTo test if the bot works well so far, execute the *Trigger node* and send a message to the Telegram channel. We tested this workflow with the message \"You're a stupid bot! I hate you!\" (we swear it's just for testing purposes, we actually think bots are pretty cool and smart). The *Telegram Trigger node* should output the following result:\n\n![Configuration of the Telegram Trigger node](https://lh5.googleusercontent.com/biB_15LoriRD_BK0T2yJqtYDCadzvBOy9WZsgNWbafxnbQEsKXLuHbyFiva1Kz_umJn8Uo3tjc4xBaLFIEkFohLEUxPg__rrW0YQrprJbBKkPEDj-3qvJTYo_U_KbuYihUccdoRY=s0)\n\n\n### 2\\. Analyze the toxicity of the message\n\nIn the second step, the incoming message from Telegram has to be analyzed with Perspective. In the *Google Perspective node* configure the following parameters:\n\n-   *Operation*: Analyze Content\\\n    This operation analyzes the incoming text message.\n-   *Text*: {% raw %} `{{$json[\"message\"][\"text\"]}}` {% endraw %}\n    This expression selects the incoming Telegram message to be analyzed.\n\nIn the section *Attributes to Analyze* you can add one or more attributes supported by Perspective that you want to be detected in the incoming message. If you don't add any attribute, all will be returned by default. For this example, the node is configured to detect profanities and identity attacks in the text, so two attributes are added with the properties:\n\n-   *Attribute Name:* Profanity\n-   *Score Threshold*: 0.00\\\n    This value sets the score above which to return results. The score is a value between 0 and 1 representing the probability that the text is toxic; it doesn't reflect the intensity (how toxic the text is). For example, if you set the *Score Threshold* at 0.5, then only messages that are 50% likely to be toxic are returned. If no value is set, at zero all scores are returned. You can read more [in this article](https://medium.com/jigsaw/what-do-perspectives-scores-mean-113b37788a5d) about what the scores mean.\n\nIn the section *Options*, you can select the *Language* of the text input. This option is useful if you want to monitor only a specific language. If unspecified, the node will auto-detect the language. In our example, we select the *Language* English.\n\nNow if you execute the *Google Perspective node*, the output should look like this:\n\n![Configuration of the Google Perspective node](https://lh4.googleusercontent.com/XI2H1OnmArFhHG6PADlYp4ENKJZXe2S0RpWEtf2kLwXte19R1STMMSY8xAn0mwSv7PAcy89_ElOp_zaye6FywKjLAV4shx10m1sqWuuIFi5UK270vo6kk4iheAjXhHkGNu5tFY0j=s0)\n\n\n### 3\\. Filter toxic messages\n\nIn the third step, the toxic messages with a probability higher that 0.7 have to be filtered out. For this, you need to set up an *IF node* with the following parameters:\n\n-   *Value 1:* {% raw %} `{{$json[\"attributeScores\"][\"PROFANITY\"][\"summaryScore\"][\"value\"]}}` {% endraw %} \\\n    This expression selects the score value of the respective attribute.\n-   ***Operation:* Larger**\n-   *Value 2*: 0.7\\\n    This is the value we want to compare the score with.\n\nIf you execute the IF node now, it outputs the following results:\n\n![Configuration of the IF node](https://lh3.googleusercontent.com/ulTV-NrI_foRhtJdXSsee98X2iiamH5YXSuyMhJOXcNCnj2EDV6NF_vdH-HkobnmyMECQcxVTCZtBz4ZJ1J6Ivzslmkg9VJxEt9JfaSB2SRsCFIuizGfrR2e2bhHwXpbd80mCG6w=s0)\n\nThe message \"You're a stupid bot! I hate you!\" scored 0.92 for profanity and 0.62 for identity attack, which means it has downright strong toxic language on these attributes.\n\n### 4\\. Send a warning message to Telegram\n\nThe final step is taking action against the toxic message. A mild action would be to just reply to the message in the Telegram channel warning the user that \"We don't tolerate toxic language here!\". To do this, configure the *Telegram node* with the following parameters:\n\n-   *Resource*: Message\n-   *Operation*: Send Message\n-   *Chat ID*: {% raw %} `{{$node[\"Telegram Trigger\"].json[\"message\"][\"chat\"][\"id\"]}}` {% endraw %}\n-   *Text*: I don't tolerate toxic language!\n-   *Add Field > Reply to Message ID*: {% raw %} `{{$node[\"Telegram Trigger\"].json[\"message\"][\"message_id\"]}}` {% endraw %}\n\n![Configuration of the Telegram node](https://lh4.googleusercontent.com/wA7GLd-yBfCEzNKH4hYxGC1Y7oV46KLpObgeDiPo7lBZjTnqyc02B01Ja_gNwbFZLeh_CTPtjVqUz_VlkPHvg2PO6SW2-5qzevSlYc0F6SeDve8bUp_NYJ9pddmKrKdgLcd26_57=s0)\n\n\nNow the bully will be publicly admonished in Telegram (once again, sorry, bot, you're really cool):\n\n![Bot response to a toxic message in Telegram](./blog_images/n8n_toxlan_telegrammsg.jpeg)\n\n\n## What's next?\n\nIn this post, you've learned about the challenge and importance of monitoring toxic language in online communities and how you can build a no-code Telegram bot for this purpose. The use case in this tutorial is fairly simplistic, but this kind of toxic language detector can be implemented in various platforms at scale.\n\nFor example, you could tweak this workflow and connect the *Google Perspective node* to Discord, Discourse, or DISQUS to detect toxic language in online communities and forums, or even to Gmail to filter out toxic emails. You can take different actions to toxic messages, for example forwarding them to a moderator, storing them in a database, flagging or banning the user depending on their message scores.\n\n> This post was originally published on the [n8n blog](https://n8n.io/blog/create-a-toxic-language-detector-for-telegram/)."
    },
    {
      "id": "/2021/09/14/crm-workflows",
      "metadata": {
        "permalink": "/blog/2021/09/14/crm-workflows",
        "source": "@site/blog/2021-09-14-crm-workflows.md",
        "title": "How to get started with CRM automation (with 3 no-code workflow ideas)",
        "description": "If you run a business, sell a product, or offer services, you know how important it is to nurture the relationship with your customers and gain new ones. Whether you operate alone or with a sales team, you probably also know how difficult it can be to keep track of your leads, customers, and orders.",
        "date": "2021-09-14T00:00:00.000Z",
        "formattedDate": "September 14, 2021",
        "tags": [
          {
            "label": "n8n",
            "permalink": "/blog/tags/n-8-n"
          }
        ],
        "readingTime": 10.01,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "post",
          "title": "How to get started with CRM automation (with 3 no-code workflow ideas)",
          "tags": [
            "n8n"
          ],
          "share-description": "Discover how you can automatically capture insightful information from your sales funnel and save your sales team hours of work every week.",
          "canonical_url": "https://n8n.io/blog/how-to-get-started-with-crm-automation-and-no-code-workflow-ideas/"
        },
        "prevItem": {
          "title": "Create a toxic language detector for Telegram in 4 steps",
          "permalink": "/blog/2021/09/21/toxic-language-workflow"
        },
        "nextItem": {
          "title": "6 e-commerce workflows to power up your Shopify store",
          "permalink": "/blog/2021/09/02/ecommerce-workflows"
        }
      },
      "content": "If you run a business, sell a product, or offer services, you know how important it is to nurture the relationship with your customers and gain new ones. Whether you operate alone or with a sales team, you probably also know how difficult it can be to keep track of your leads, customers, and orders.\n\nWe're here to show you that you can optimize your sales and customer workflows with two keywords: **CRM** and **automation**. Read on to learn what exactly a CRM is, why and when you should use a CRM for your business, and how to automate three common CRM sales workflows in only a few clicks.\n\nA **customer relationship management (CRM)** tool is pretty self-explanatory: it helps you manage the relationships with your customers. This means that it stores information on your customers (such as name, title, company, role), the relation they have with your company (e.g., lead, opportunity, deal), and the status of the relationship (e.g., closes, open, waiting), and the monetary value of closed deals (like quote and price).\n\n[The CRM software market is forecast to grow to $43.5 bn in 2024](https://www.statista.com/statistics/605933/worldwide-customer-relationship-management-market-forecast/), proving the increasing popularity and necessity of CRM tools for companies.\n\n![](https://lh3.googleusercontent.com/VLLc8EAiYXhhfRHvUiNpOGW_kPGMmCQldNOf0hYQf8Ah_hCpOTxTuzmnZySPTioCopf8jTMsc57CEs0Mwf1qk37qnMqGg6djctaChzx1pxGM-9Wz5Ob_CStaNUrlRrrw9t9U8N7T=s0)\n\n[*Global CRM software market growth*](https://www.statista.com/statistics/605933/worldwide-customer-relationship-management-market-forecast/)‌‌\n\nMost commonly, CRMs are used by sales teams to track their sales activities and milestones in the sales funnel, from lead to deal.\n\n## Why and when you should use a CRM\n\nTo better understand the value of a CRM, let's take a business example.\n\nSay you have a creative business specializing in professional corporate photography. Your **visitors** can book shootings on your website [(which offers plenty of opportunities for automation)](https://n8n.io/blog/no-code-ecommerce-workflow-automations/), but you also actively network to find new **opportunities**. At a business event you've attended, you've met Marketing and Branding employees from different companies (**contacts**). Of these, a couple have expressed interest in your professional photo shootings for their team, qualifying them as **leads** for your business. This means they could become your **customers**, which is awesome! Now you need to follow up with them, make them an **offer** they can't refuse, and eventually close the **deal**.\n\n![Sales funnel](https://lh4.googleusercontent.com/LZfwmishdMdwdXF4JJl_30FZmTFEZeLdZlQB9Wb1Aeb1kZ2WFEbqjbKrGEVfCXnxz_Edn7g6EcbJtIKQ8yG2Red91qek0wc2O7R4GiH1mv5RfbK36QCClVLmhM-1A44urIjNU7RC=s0)\n\nHow can you and your sales team keep track of all these steps, for each lead, while also making sure that you nurture the relationship with your (potential) customers and organize the logistics with your team of photographers? If you think a digital calendar, a paper agenda, and a spreadsheet could do the job, you wouldn't be completely wrong. Sure they can help you organize your time and contacts, but provide limited features, are error-prone and even unmanageable in the long run.\n\nHere are 3 common challenges that sales-oriented teams face in their organization:\n\n1.  **Information is spread in different sources.**\\\n    You contact a customer via email, your salesperson talks to them on the phone, then you write down key information on a post-it and your colleague inserts the order details in a spreadsheet. If anyone asks something about that specific customer, you'd need to sift through emails and notes and ask several people who've come in contact with the customer--a highly inefficient process.\n2.  **Information is duplicated or missing completely.**\\\n    If your salespeople have back-to-back calls and meetings with leads and customers, inevitably they might forget to pass on some information, or even assume that a colleague has taken care of that. At the other end, two salespeople might contact the same lead because they don't have an overview of their assignments. This is how meetings get overlooked, clients get annoyed, and you don't get orders.\n3.  **Salespeople are unmotivated and exhausted by repetitive tasks.**\\\n    It's no secret that sales is a fast-paced and high-pressure field. But the role of a salesperson can become particularly challenging if they are often tasked with repetitive work (like sending the same email to different leads) or their work is inefficiently organized. These problems lead to a decrease in performance and invariably in sales.\n\nA CRM solves the five problems listed above, by providing you with a feature-rich integrated system that can save your business up to 10 hours of work every week. In short, you should use a CRM if you sell a product, provide a service, or deal with customers or clients in any way.\n\nIf you are convinced by the advantages of a CRM, but reticent about the costs, note that [the cost of not automating](https://n8n.io/blog/3-reasons-why-startups-should-invest-in-automation/) is higher than the investment in a CRM, which ultimately increases the productivity and value of your sales team.\n\n## 3 steps to CRM automation for the sales funnel\n\nNow that you've seen the advantages of using a CRM, it's time to start implementing it.\n\n### 1\\. Choose the right CRM for your use case\n\nOn a first look, you might get overwhelmed by all the CRM providers available on the market. There are options for different team sizes, departments, and budgets. To help you get an overview of their features, we compiled a list of 10 of the most popular CRMs, which also come with n8n integrations that allow you to perform common CRUD (create, read, update, delete) operations on your saved contacts, companies, deals, and more.\n\n![](https://lh3.googleusercontent.com/QTroBlryabnuUQQNrLAJ7be2dc7ZVdsSNxNNR_QPxOtDaiSQqo9oZMAM2P0jMESJoUcXuIzFi3euSdmVSzCI_6DsJv7GzvfjpzWlho_EqOLasts3GdqPVkUXQo-OvsZxJc-q0xtj=s0)\n\n\n1.  [**Agile CRM**](https://www.agilecrm.com/) is an all-in-one CRM software for marketing, sales, and service. With the [*Agile CRM node*](https://docs.n8n.io/nodes/n8n-nodes-base.agileCrm/) you can manage company, contact, and deal details in your workflows.\n2.  [**Copper**](https://www.copper.com/) is a CRM integration for Google Workspace and is best suited for small and medium-sized businesses. The n8n nodes [*Copper*](https://docs.n8n.io/nodes/n8n-nodes-base.copper/) and [*Copper Trigger*](https://docs.n8n.io/nodes/n8n-nodes-base.copperTrigger/) provide the basic CRUD operations for companies, customer sources, leads, opportunities, persons, projects, tasks, and users.\n3.  [**HubSpot**](https://www.hubspot.com/)'s CRM platform provides tools for social media marketing, sales, content management, and customer service. With the [*HubSpot node*](https://docs.n8n.io/nodes/n8n-nodes-base.hubspot/) and [*HubSpot Trigger node*](https://docs.n8n.io/nodes/n8n-nodes-base.hubspotTrigger/) you can manage contacts, contact lists, companies, deals, forms, and tickets.\n4.  [**Intercom**](https://www.intercom.com/) is a conversational relationship platform which allows businesses to communicate with prospective and existing customers within their app, on their website, through social media, or via email. The [*Intercom node*](https://docs.n8n.io/nodes/n8n-nodes-base.intercom/) lets you manage companies, leads, and users from the CRM.\n5.  [**Keap**](https://keap.com/) offers an e-mail marketing and sales platform for small businesses, including products to manage and optimize the customer lifecycle, customer relationship management, marketing automation, lead capture, and e-commerce. The [*Keap node*](https://docs.n8n.io/nodes/n8n-nodes-base.keap/) and [*Keap Trigger node*](about:blank) allow you to manage companies, contacts, contact notes and tags, ecommerce orders and products, emails, and files.\n6.  [**Pipedrive**](https://www.pipedrive.com/) is a cloud-based sales software company that aims to improve the productivity of businesses through the use of their software. You can use the [*Pipedrive node*](https://docs.n8n.io/nodes/n8n-nodes-base.pipedrive/) and [*Pipedrive Trigger node*](about:blank) to manage activities, deals, deal products, files, leads, notes, organizations, persons, and products.\n7.  [**Salesforce**](https://www.salesforce.com/) is the [leading vendor in the CRM market worldwide](https://www.statista.com/statistics/972598/crm-applications-vendors-market-share-worldwide/). Salesforce provides customer relationship management service and also sells a complementary suite of enterprise applications focused on customer service, marketing automation, analytics, and application development. The [*Salesforce node*](https://docs.n8n.io/nodes/n8n-nodes-base.salesforce/) allows you to manage over 10 different fields, such as contacts, leads, opportunities, flows, and tasks.\n8.  [**Salesmate**](https://www.salesmate.io/) is a cloud-based CRM solution that caters to small and midsize businesses across various industries. Key features include contact management, sales pipeline management, email marketing and internal chat and phone integration. The [*Salesmate node*](https://docs.n8n.io/nodes/n8n-nodes-base.salesmate/) lets you manage information about activities, companies, and deals.\n9.  [**Zoho CRM**](https://www.zoho.com/crm/) is an online Sales CRM software that manages sales, marketing and support. The *Zoho CRM* node allows you to manage accounts, contacts, deals, invoices, leads, products, purchase orders, quotes, sales orders, and vendors. With the [*Zoho CRM node*](https://docs.n8n.io/nodes/n8n-nodes-base.zohoCrm/) you can perform CRUD operations on deals, invoices, leads, quotes, and many more.\n10. [**Freshworks**](https://www.freshworks.com/) is a cloud-based CRM that helps businesses manage their interactions with their customers and leads. The [*Freshworks CRM node*](https://docs.n8n.io/nodes/n8n-nodes-base.freshworksCrm/)provides basic operations for managing sales activities, tasks, deals, and more.\n\n### 2\\. Decide what you will automate\n\nAfter you've picked a CRM and explored its functionalities, you should define what you want to automate. Think of the tasks involved in every step of the sales funnel and ask yourself these questions:\n\n-   Is the task repetitive?\n-   Is the task time-consuming?\n-   Do you need to perform the task often and regularly?\n-   Does the task have a high value?\n\nIf you answered yes to these questions, then your task is most probably a case for automation. Once you've identified the pain points in your current manual workflows, you can start defining and designing automated workflows.\n\n### 3\\. Build workflows\n\nBefore starting to create workflows for the tasks identified in the previous step, ask yourself one more question: do you need to automate tasks that take place only within the CRM or also between the CRM and other apps or services?\n\nFor the first case, note that some of the CRMs listed above offer built-in automation functionality for simple workflows. For the second case, you can take advantage of the n8n nodes, which allow you to connect your CRM to 200+ apps or services.\n\nTo help you get started, we've created **3 workflows with HubSpot and Pipedrive** for automation at every step of the sales journey. Of course, you can replace the *HubSpot* and *Pipedrive nodes* with another CRM of your choice.\n\n### Capture leads from Typeform submissions\n\nTypeforms are a presentable and efficient way of capturing leads and feedback from your customers. For example, you can embed a typeform on your website where visitors can request a quote for your services, or one which asks them to submit their contact details in order to download gated content.\n\n[This workflow](https://n8n.io/workflows/1223) is triggered when a typeform is submitted, then it saves the sender's information into HubSpot as a new contact.\n\n\n### Send reminders after meetings with prospects\n\nWe mentioned that a common problem within sales teams is synchronization and information transfer since salespeople might forget to note down details from their conversations with leads, jumping from one call to another.\n\n[This workflow](https://n8n.io/workflows/1221) is triggered when a client meeting is scheduled via Calendly. Then, an activity is automatically created in Pipedrive, to keep track of the lead cycle. Fifteen minutes after the end of the meeting, a message is sent to the responsible salesperson in Slack, reminding them to write down their notes and insights from the meeting with the lead.\n\n### Process newly created deals based on their stage, value, and priority\n\nYou're reaching the bottom of the sales funnel and getting deals--good for you! From here, there are several tasks you can automate to speed up the sales process.\n\n[This workflow](https://n8n.io/workflows/1225) is triggered when a new deal is created in HubSpot. Then, it processes the deal based on its type and stage.\n\nThe first branching follows three cases:\n\n-   If the deal is closed and won, a message is sent in a Slack channel, so that the whole team can celebrate the success.\n-   If a presentation has been scheduled for the deal, then a Google Slides presentation template is created.\n-   If the deal is closed and lost, the deal's details are added to an Airtable table. From here, you can analyze the data to get insights into what and why certain deals don't get closed.\n\nThe second branching follows two cases:\n\n-   If the deal is for a new business and has a value above 500, a high-priority ticket assigned to an experienced team member is created in HubSpot\n-   If the deal is for an existing business and has a value below 500, a low-priority ticket is created.\n\nApart from Typeform, you can also use the [*Eventbrite Trigger node*](https://docs.n8n.io/nodes/n8n-nodes-base.eventbriteTrigger/)to capture the contact information of people who registered for an event, or the [*SurveyMonkey Trigger node*](https://docs.n8n.io/nodes/n8n-nodes-base.surveyMonkeyTrigger/) to save the responses of a survey.\n\n## What's next?\n\nIn this post, you've learned about the advantages of CRM tools, when and why you should use a CRM, and what workflows you can automate with different CRMs. You've seen how automating different processes in the sales funnel can increase your productivity and minimize the time between the first contact and a closed deal.\n\n> This post was originally published on the [n8n blog](https://n8n.io/blog/how-to-get-started-with-crm-automation-and-no-code-workflow-ideas/)."
    },
    {
      "id": "/2021/09/02/ecommerce-workflows",
      "metadata": {
        "permalink": "/blog/2021/09/02/ecommerce-workflows",
        "source": "@site/blog/2021-09-02-ecommerce-workflows.md",
        "title": "6 e-commerce workflows to power up your Shopify store",
        "description": "The online shopping trend has been driven by increasing digitalization in the past years, and the COVID-19 outbreak has only fueled e-commerce growth. Consumers around the world are turning to online stores for pretty much all product categories, due to in-person restrictions or contamination concerns.",
        "date": "2021-09-02T00:00:00.000Z",
        "formattedDate": "September 2, 2021",
        "tags": [
          {
            "label": "n8n",
            "permalink": "/blog/tags/n-8-n"
          }
        ],
        "readingTime": 6.845,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "post",
          "title": "6 e-commerce workflows to power up your Shopify store",
          "tags": [
            "n8n"
          ],
          "share-description": "Want to power up your online business and win back time? Discover how no-code workflow automation can help!",
          "canonical_url": "https://n8n.io/blog/no-code-ecommerce-workflow-automations/"
        },
        "prevItem": {
          "title": "How to get started with CRM automation (with 3 no-code workflow ideas)",
          "permalink": "/blog/2021/09/14/crm-workflows"
        },
        "nextItem": {
          "title": "Clustering and classification of emotion verbs",
          "permalink": "/blog/2021/08/06/romanian-psych-verbs-study"
        }
      },
      "content": "The online shopping trend has been driven by increasing digitalization in the past years, and the COVID-19 outbreak has only fueled e-commerce growth. Consumers around the world are turning to online stores for pretty much all product categories, due to in-person restrictions or contamination concerns.\n\nIn 2020, [retail e-commerce sales worldwide](https://www.statista.com/statistics/379046/worldwide-retail-e-commerce-sales/) amounted to $4.28 trillion and i[n 2021](https://www.statista.com/statistics/251666/number-of-digital-buyers-worldwide/), over 2.14 billion people worldwide are expected to buy goods and services online.\n\nBut as online shoppers have grown in numbers, so have digital shop owners. Thanks to e-commerce platforms, almost anyone can set up an online shop with only a few clicks. However, running even a small digital business also involves some manual, repetitive tasks that might add up and steal too much of your precious time. Luckily, these kinds of tasks can be automated.\n\nIn this post, we'll have a look at the most popular e-commerce platforms and how to automate common e-commerce workflows in n8n.\n\n\n## The e-commerce platforms leading the global market\n\nMany e-commerce software platforms have emerged in the last years, but the [most popular ones](https://www.statista.com/statistics/710207/worldwide-ecommerce-platforms-market-share/), leading the market, are **[WooCommerce](https://woocommerce.com/)** and [**Shopify**](https://www.shopify.com/).\n\n![](https://n8n.io/blog/content/images/2021/09/statista_ecommerceplatforms.png)\n\n[*Market share of e-commerce software platforms worldwide in 2021*](https://www.statista.com/statistics/950550/worldwide-ecommerce-platforms-market-share/)\n\n**Shopify** is a paid e-commerce platform that offers templates for quickly designing your online shop. With Shopify, you don't have to worry too much about technicalities and instead focus on selling your products.\n\n**WooCommerce** is the second most popular e-commerce software platform as of [April 2021](https://www.statista.com/statistics/710207/worldwide-ecommerce-platforms-market-share/), owning over 23% of the market share. WooCommerce is actually an open-source WordPress plugin, making it the go-to choice for smaller and cost-conscious shop owners.\n\n## Why and when you should automate your online shop\n\nRegardless of the platform you choose or the products you sell, there might come a time when managing your orders becomes too time-consuming and you'll probably find yourself in one of these two common situations:\n\n-   **Your business is growing** and you don't have the bandwidth anymore to manage all the orders. You might consider hiring one or two people to help you out, but be aware that additional employees could mean additional responsibilities for you.\n-   **Your online shop is a side hustle** and you don't have much time to take care of it besides your main job. You neither want to compromise your career nor give up your hobby business.\n\nIf you identify yourself with one of these cases, you should consider automating at least part of your work. Workflow automation might sound intimidating or a skill reserved for the tech-savvy ones -- but don't be intimidated.\n\nNo-code tools with a visual user interface like [n8n](https://n8n.io/) make automation feel like childsplay. You can easily combine WooCommerce and Shopify integrations with other apps or services to automate common workflows in your digital store.\n\n## Workflow automation ideas for Shopify\n\nn8n offers four nodes for Shopify and WooCommerce: [***Shopify node***](https://docs.n8n.io/nodes/n8n-nodes-base.shopify/), [***Shopify Trigger node***](https://docs.n8n.io/nodes/n8n-nodes-base.shopifyTrigger/), [***WooCommerce node***](https://docs.n8n.io/nodes/n8n-nodes-base.wooCommerce/), and [***WooCommerce Trigger node***](https://docs.n8n.io/nodes/n8n-nodes-base.wooCommerceTrigger/). They provide various operations for managing orders, products, customers, and carts. Read our docs to learn how to configure the nodes, then you can start configuring various parameters to build workflows for your online store.\n\nThe *Shopify* and *WooCommerce* nodes open up many possibilities for automation, helping you win back time and focus on things that matter. Here are six ideas of workflows you can automate for your Shopify store (and adapt for WooCommerce):\n\n### Promote your new products on social media\n\nA [survey on social commerce](https://www.statista.com/statistics/1031962/global-social-commerce-activities-age/) revealed that 43% of users research products online via social networks and 28% discover brands via ads on social media. This goes to show how important it is to have a presence on social media and regularly share and promote your products.\n\nTo help you automate your social media activity, we created [a workflow](https://n8n.io/workflows/1205) that is triggered when you create a product on your shop and automatically shares the news on your Twitter account and a Telegram channel with the message \"Hey there, my design is now on a new product ✨ Visit my [shop_name] shop to get this cool [product_name] (and check out more [product_category]) 🛍️ [shop_link]\".\n\n![](https://lh4.googleusercontent.com/PfT3breNP11_HKVZtsbWEbvaQeAx6Lw9DndVq-cxhtkJd7omEgOVxzmaSp3lXU4vWbFLBXzo0McRpv3o0mUZrZQaDuKJoBcL1PqyoJ6aV3BC2Jr89Oly36Mvv9r-Dq-rFaHjiMTU=s0)\n\nYou can make the message even more appealing to your audience by adding a *Bannerbear node* that automatically creates template images for your new product announcements.\n\n### Update customer and order details in Zoho CRM\n\nOnce the first orders start to come in, it's time to neatly track the orders and nurture the relationship with your customers, ideally in a customer relationship management (CRM) system.\n\nThe first branch of [this workflow](https://n8n.io/workflows/1206) saves customer orders from Shopify to the Zoho CRM and Trello. In the *Zoho node*, you can select the option *Create or Update*, which creates a new contact if a contact with a matching last name and email address exists. This way, you don't have to worry about duplicate contacts.\n\n![](https://lh5.googleusercontent.com/VZPu93FFd12B4IH-MmCNYumKg_yt-SxoIPvF6uKI9KjM2unE4kUTdwE0t2R3DXUwc5CUA7Wy2iwVeoB37AnQ3pKrTv1lvr7BTPfPVG-VI7yV_m1ucxoKDf6xrHulPm1oY7JOkRxy=s0)\n\nOf course, you can replace the *Zoho node* with another CRM, for example, *Salesforce* or *Agile CRM*.\n\n### Create invoices for new orders\n\nA not-so-fun part of being a shop owner is paperwork like invoices. Manually writing the details of each order for each customer is not only tedious but also error-prone. So why not automate this task?\n\nThe second branch of [this workflow](https://n8n.io/workflows/1206) automatically generates invoices with Harvest when an order is created in Shopify. Then, it creates a Trello card with the order information and the invoice attached.\n\n![](https://lh3.googleusercontent.com/gQKFDQxQjHaujl4qDGx3HA7fxZzrEk9BYx7xph5d-Di69h8gWmVdZH53l_k6433ECcb61VMAzJRtFECbVnUfI1enrz-NGxKqbLLbbTtoDypbwr92lV30fpu_CmfROXWH-wkns_BB=s0)\n\n\n### Offer coupons and discounts to high-order customers\n\n[In 2020](https://www.statista.com/statistics/1231069/leading-reasons-for-buying-products-when-shopping-online/), the leading reasons why internet users around the world added a product to their online basket and purchased the item were free delivery, coupons or discounts, and reviews from other customers.\n\nThough free delivery is up to your pricing model, you can (and should) invest in the latter two incentives. For example, you can offer discounts to high-order customers.\n\nThe third branch of [this workflow](https://n8n.io/workflows/1206) is triggered when a new order is created and checks if the order value is above 100 (or any value you set) -- if it is, it sends an email to the customer with a 10% discount coupon for their next order, otherwise, it sends them an email thanking them for their order.\n\n![](https://lh6.googleusercontent.com/_hXeGJ0Zy91SxF_t_mXa3GYiyuJT2RyjmfCJeN92vtND7lsuJLo7oGbvDTPCg7TJQSOEaV1E0I3xgtJ_Z9O-2qu0MyX5hz9oUVsMwp4gp8Uajn2EI9f2J3j-1G4buWZm3b0kmB4-=s0)\n\n\nFor extra productivity, you can combine this and the previous two workflows into [one](https://n8n.io/workflows/1206) super-workflow:\n\n![](https://lh6.googleusercontent.com/rrUF6JaAVO0z9RQmXa5BMHVgsFN9q7H4IbAI7r2WOvRCOwjjtaKoAQzcyb78HNEqSbwFd4hqNHYBH9mMqKd-xgVRPE1BwT3Jq0JjP-g-OWaowQArRJoUecsFM2gdzlbKrn1l6geb=s0)\n\n\n### Request customers to write a review after they have received their order\n\nIn the previous workflow, we've mentioned product reviews and the third reason why customers buy products online. In recent years, it has become increasingly important to the consumer to read up on a product, business, or service before spending any money. In 2020, reviews were the third top reason that convinced shoppers to purchase a product online. [This year](https://www.statista.com/statistics/1020836/share-of-shoppers-reading-reviews-before-purchase/), nearly 70% of online shoppers typically read between one and six customer reviews before making a purchasing decision.\n\nThis proves the value of investing in customer experience and incentivizing shoppers to write reviews about your products. To incentivize them, you can tweak [this workflow](https://n8n.io/workflows/1206) to be triggered when an order is marked as fulfilled by selecting the *Topic: Order Fulfilled* in the *Shopify Trigger node*. Then, an email should be sent to the customer, asking them to write a review about their product.\n\n### Run sales inventories and reports in Google Sheets\n\nWhen your online shop generates a steady flow of orders, it's necessary to keep an inventory and track your growth regularly. For small businesses, even a Google Sheet can be enough for keeping records.\n\n[This workflow](https://n8n.io/workflows/1207) is scheduled to run every week, when it gets all your Shopify orders, calculates their sales value, and stores the data in Google Sheets for you to evaluate. Additionally, it can send a message to a Slack channel (if you work together with a small team) or Telegram to inform you about your weekly sales.\n\n## What's next?\n\nIn this post, we talked about the growing popularity of e-commerce and possibilities for automation. Whether as a full-time business or a fun side hustle, if you want to sell goods online -- from art prints to clothing and cosmetic products -- e-commerce platforms like Shopify and WooCommerce enable you to set up an online shop with only a few clicks.\n\n> This post was originally published on the [n8n blog](https://n8n.io/blog/no-code-ecommerce-workflow-automations/)."
    },
    {
      "id": "/2021/08/06/romanian-psych-verbs-study",
      "metadata": {
        "permalink": "/blog/2021/08/06/romanian-psych-verbs-study",
        "source": "@site/blog/2021-08-06-romanian-psych-verbs-study.md",
        "title": "Clustering and classification of emotion verbs",
        "description": "I've been interested in emotions ever since I've become aware of them. Maybe because I was unsure how to process and interpret them, in both myself and others. How do people express emotions and why do they do it so differently? Is being scared the same as fearing? Can I really understand or experience sirva vigad if the word doesn't exist in my native language?",
        "date": "2021-08-06T00:00:00.000Z",
        "formattedDate": "August 6, 2021",
        "tags": [
          {
            "label": "data science",
            "permalink": "/blog/tags/data-science"
          },
          {
            "label": "linguistics",
            "permalink": "/blog/tags/linguistics"
          },
          {
            "label": "NLP",
            "permalink": "/blog/tags/nlp"
          },
          {
            "label": "Python",
            "permalink": "/blog/tags/python"
          }
        ],
        "readingTime": 8.775,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "post",
          "title": "Clustering and classification of emotion verbs",
          "gh-repo": "lorenanda/psych-verbs",
          "gh-badge": [
            "star",
            "fork",
            "follow"
          ],
          "tags": [
            "data science",
            "linguistics",
            "NLP",
            "Python"
          ],
          "comments": true
        },
        "prevItem": {
          "title": "6 e-commerce workflows to power up your Shopify store",
          "permalink": "/blog/2021/09/02/ecommerce-workflows"
        },
        "nextItem": {
          "title": "5 tasks you can automate with the new Notion API",
          "permalink": "/blog/2021/06/08/notion-workflows"
        }
      },
      "content": "I've been interested in emotions ever since I've become aware of them. Maybe because I was unsure how to process and interpret them, in both myself and others. How do people express emotions and why do they do it so differently? Is *being scared* the same as *fearing*? Can I really understand or experience *sirva vigad* if the word doesn't exist in my native language?\n\nMy solution to deal with this personal struggle was to read and research about emotions. I know I know, you can't learn everything from books, especially not human feelings and interactions, but nevertheless I've discovered some fascinated learnings about emotions.\n\nThis is why in university I chose to research verbs of emotion, more specifically to investigate the semantic characteristics of Romanian verbs of emotion.\n\nThe project was conducted during my Master's degree in Linguistics at Humboldt-Universität zu Berlin and was also inspired by the research of [Prof. Elisabeth Verhoeven](https://www2.hu-berlin.de/experiencer/alternation/en/index.html) and [Hartshorne et al.](https://www.researchgate.net/publication/308761372_Psych_verbs_the_linking_problem_and_the_acquisition_of_language) on psych-verbs in different languages.\n\nSince 2018, when I started working on this project, I've had the chance to present some preliminary findings at the [5th Linguistik Meetup in Potsdam]({% post_url 2018-08-04-poster-linguistik-meetup-potsdam %}) and apply newly-learned [machine learning models](https://github.com/lorenanda/psych-verbs) for analyzing the data. \n\nIn this blog post, I want to share with you the fascinating topic of psych-verbs, how I designed my research experiment, how I analyzed the collected data, and most importantly what I found out about Romanian psych-verbs.\n\n\n## A brief theory of psych-verbs \n\n**Psych-verbs** (also psychological verbs or experiencer verbs/predicates) express the mental state or emotion of an experiencer. The experiencer role refers to a participant that undergoes an event affecting consciousness. The experiencer can appear in one of two positions: as subject (**subject experiencer - SE**) or as object (**object experiencer - OE**). Take for example two illustrative sentences:\n\n|*Jack scares Wendy.*|*Wendy fears Jack.*|\n|--------------------|-------------------|\n|Wendy is the OE of an action (emotion) inflicted by Jack upon her.|Wendy is the SE of an emotion that arises within her in response to Jack being perceived as a threat.|\n\nEven though the two verbs express the same emotion of fear, they have different syntactic structures, depending on the experiencer form. \n\nThis structure appears to be unsystematic across languages, so there are no clear rules for which verbs are either OE, SE, or both. For this reason, psych-verbs pose a linguistic challenge in explaining the link between syntax and semantics. \n\nAt the point of writing (early 2019), theoretical and experimental research has limited to popular languages (e.g., English, German, Japanese) and there are no experimental studies on Romanian psych-verbs. This is unfortunate, because native speakers' intuition can provide insights into a language. \n\nTherefore, my aim was to fill this research gap by conducting an experiment that reveals the semantic properties of psych-verbs in my native language, Romanian.\n\n## Experiment design\nTo research how people perceive psych-verbs, I designed an experiment, carried it on native Romanian speakers, and analyzed the resulted data.\n\nI composed a list of 54 Romanian verbs myself. I chose verbs that have been included in research studies in other languages, but also new verbs that haven't been investigated before. Importantly, I assigned the verbs to six emotion categories (based on [Paul Ekman's theory of basic emotions](https://www.tandfonline.com/doi/abs/10.1080/02699939208411068)): anger, disgust, fear, happiness, sadness, and surprise.\n\nEach verb had to be rated from 1 (least) to 5 (most) on four criteria: \n- **valence**: whether the verb expresses a negative or a positive emotion\n- **arousal**: how intense is the emotion expressed by the verb)\n- **duration**: how long is the emotion expressed by the verb likely to last\n- **cause**: whether the emotion expressed by the verb is perceived to be caused by external or internal factors\n\nFor this experiment, I selected participants through word of mouth. I gave them the verb list in paper-form or sent email it to them, and asked them to complete it in one sitting. The verb list was randomized for each participant, so that the order of the verbs could not influence the ratings. \n\nIn total, I had 30 participants:\n- all native speakers of Romanian\n- 17 females, 6 males (self-identified)\n- 32 years old on average\n\n## Data analysis\nAfter collecting responses from each participant, I went into the deep work: analyzing the data. I did all data analysis in Python with libraries such as pandas, seaborn, and scikit-learn. If you're interested in seeing the detailed data analysis, check out this [Jupyther Notebook](https://github.com/lorenanda/psych-verbs/blob/master/psych-verbs.ipynb).\n\n### Descriptive analysis\nFirst, I ran a **descriptive analysis**, where I looked at the distribution of values by experiencer, emotion domain, feature rating, and verb. At this stage, I was interested in answering questions like:\n- How long do happiness emotions last on average?\n- What is the most intense emotion domain?\n- Are SE verbs more positive than OE verbs?\n- Is there a correlation betwee the duration and valence of an emotion verb?\n\n![](blog_images/psychverbs_cumplot.svg)\n\nThe pair plot above depicts three histograms of Duration, Arousal, and Value, as well as six scatterplots with regression lines of the combinations of the three features.\n\n### Clustering\nSecond, I used **K-Means clustering** to identify clusters of verbs based on their duration, arousal, and valence ratings. The question here was whether some emotion verbs are closer (i.e. more similar) based on their values, rather than on the emotion category they belong to. The K-means\n\n![](blog_images/psychverbs_kmeans.svg)\n\nThe scatterplot represents the three verb clusters formed based on the three features Duration, Arousal, and Valence.\n\n- The blue-marked cluster represents verbs with low values, concentrated in the lower left corner of the graph.\n- The red-marked cluster represents verbs with average values, concentrated at the middle of the y-axis and along the x-axis.\n- The green-marked cluster represents verbs with positive ratings, concentrated in the center and upper right corner of the graph.\n\nK-Means clustering has some limitations: it builds clusters of the same size and we need to specify the number of clusters in advance.\n\nAs an alternative to K-Means clustering, I also generated a dendogram to illustrate **hierarchical clustering**, or what is the distance between verbs based on their duration, arousal, and valence values.\n\n![](blog_images/psychverbs_dendrogram.svg)\n\n\nThe x-axis represents the indices of the verbs and the y-axis represents the distance between the verbs. The vertical blue line build the maximum distance. I choose a threshold of 8 to separate the maximum distance and thus form two main clusters (orange and green), or three considering the two green subclusters separate.\n\nThe dendrogram shows how close verbs are to each other, based on the three features.\n\nIn the first orange cluster, there are 5 pairs of directly related verbs:\n\n    to panic (24) - to surprise (50)\n    to get angry (32) - to amaze (51)\n    to fascinate (8) - to fear (44)\n    to impress (10) - to get annoyed (43)\n    to satisfy (30) - to suffer (48)\n\nIn the second green cluster, there are 7 pairs of directly related verbs:\n\n    to like (25) - to reject (29)\n    to wish (6) - to love (20)\n    to frustrate (9) - to provoke (28)\n    to sadden (17) - to stress (47)\n    to annoy (7) - to frighten (13)\n    to bring joy (1) - to please (11)\n    to depress (2) - to rejoice (31)\n\nIn the third green cluster, there are 8 pairs of directly related verbs:\n\n    to disgust (4) - to irritate (19)\n    to amuse (0) - to be ashamed (41)\n    to get scared (42) - to shock (45)\n    to shadder (12) - to humiliate (52)\n    to brighten up (18) - to shadder (33)\n    to wonder (22) - to offend (23)\n    to get sad (36) - to brighten up (37)\n    to bore (26) - to get bored (40)\n\n\n### Classification / Regression\nThird, I attempted to classify psych-vers by using K-Nearest Neighbor (KNN), logistic regression (LR), decision trees (DT), and random forest classifiers. I say \"attempted\" because I was aware from the start that my data set was way too small to be fit for ML models, so I was expecting low results.\n\n|model|accuracy|\n|-----|--------|\n|KNN|41.18%|\n|LR|70.37%|\n|DT|72.73%|\n\n![](blog_images/psychverbs_decisiontree.svg)\n\n\n## Findings\n### Means\n- The average and median **duration** of the verbs is 2.2, and most commonly 1.8, so the verbs in the dataset express emotions that are perceived to last only a short time.\n- The average, median, and mode of **arousal** is around 3.3, which means that the verbs express emotions that are not particularly intense.\n- The average **valence** of the verbs is 2.6, but most ratings are 1.9 and the most common rating is 1.55. This means that the selected verbs express mainly negative emotions.\n\n### Correlations\n- There is a **positive correlation** between Arousal and Duration, i.e. verbs with high levels of arousal tend to last longer.\n- There is a **negative correlation** between Valence and Duration, i.e. verbs with negative meaning tend to last a short time.\n- There is **no correlation** between Arousal and Valence.\n\n### Experiencer\n- While OE and SE verbs have similar Duration and Arousal values, they differ significantly in terms of Valence. OE verbs have low Valence (mean 2.18, median 1.75), whereas SE verbs have positive Valence (mean 3.19, median 3.55). This means that emotions that are caused by external factors are perceived as negative, whereas emotions that arise within the experiencer are rather positive.\n\n![](blog_images/psychverbs_experiencer.svg)\n\n### Emotions\n- The emotions that last longest express Happiness in SE form, and the shortes ones express Fear in SE form.\n- The verbs with extreme Arousal levels express Happiness but also Sadness, in both OE and SE form. The most intense emotions, both positive and negative, belong to the Happiness and Sadness domains.\n\n![](blog_images/psychverbs_domain.svg)\n\n- The most positive emotions express Surprise in OE form, while the most negative ones express Fear and Anger also in OE form.\n- Panic is to Surprise as Satisfaction is to Suffering. \n- Boring someone and being bored feel the same.\n\n## What's next?\n\n### Limitations\nThough this experiment revealed new findings, this subject can be explored further. The main limitation of this study is the small sample size of participants. Moreover, it would be interesting to explore how verb ratings differ by gender or age of the speakers.\n\n### Applications\nInitially, my main interest was to explore the linguistic structure of psych-verbs, but while working on the data analysis and reading research on emotions, I realized the findings of my study could be used for natural language processing (NLP) tasks, such as sentiment analysis and emotion recognition. For example, detecting signs of depression or anxiety in texts (e.g., forums, clinical notes)."
    },
    {
      "id": "/2021/06/08/notion-workflows",
      "metadata": {
        "permalink": "/blog/2021/06/08/notion-workflows",
        "source": "@site/blog/2021-06-08-notion-workflows.md",
        "title": "5 tasks you can automate with the new Notion API",
        "description": "If you're into productivity and organisation hacks, you've probably heard of (and even got to love) Notion, the all-in-one workspace app that allows you to take notes, create databases, manage projects, and schedule tasks--all with highly customisable designs.",
        "date": "2021-06-08T00:00:00.000Z",
        "formattedDate": "June 8, 2021",
        "tags": [
          {
            "label": "n8n",
            "permalink": "/blog/tags/n-8-n"
          }
        ],
        "readingTime": 4.865,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "post",
          "title": "5 tasks you can automate with the new Notion API",
          "tags": [
            "n8n"
          ],
          "share-description": "Notion's API opens new possibilities. Discover what workflows you can automate now!",
          "canonical_url": "https://n8n.io/blog/5-tasks-you-can-automate-with-notion-api/"
        },
        "prevItem": {
          "title": "Clustering and classification of emotion verbs",
          "permalink": "/blog/2021/08/06/romanian-psych-verbs-study"
        },
        "nextItem": {
          "title": "15 Google apps you can combine and automate to increase productivity",
          "permalink": "/blog/2021/05/12/google-apps-workflows"
        }
      },
      "content": "If you're into productivity and organisation hacks, you've probably heard of (and even got to love) [Notion](https://www.notion.so/), the all-in-one workspace app that allows you to take notes, create databases, manage projects, and schedule tasks--all with highly customisable designs.\n\nAt n8n, we've been using Notion since day one for the internal organisation: from meeting notes and onboarding checklists to content calendars and product research. Imagine our excitement when we heard that [Notion launched their API (beta)](https://developers.notion.com/), thus opening new possibilities of using the app in a more personalised way!\n\n\nOur developers got to work right away and [we launched](https://www.producthunt.com/posts/notion-n8n-integration) two of the most awaited nodes: [**Notion node**](https://docs.n8n.io/nodes/n8n-nodes-base.notion/#basic-operations) and [**Notion Trigger node**](https://docs.n8n.io/nodes/n8n-nodes-base.notionTrigger/).\n\nThe **Notion node** has five basic operations that allow you to manage blocks, get and query databases, create and update records in a database, create and search for pages, and get users who are part of your workspace. The **Notion Trigger node** allows you to check at regular intervals when a page is added to the database, then trigger a workflow.\n\nNow you can easily connect your tools to Notion to sync data and boost your productivity. We're super excited to automate some of our workflows, and even more so to see what other creative ideas the [n8n community](http://community.n8n.io/) comes up with. In this article, we'll present to you **5 Notion workflows** you can automate in n8n:\n\n\n## Add candidates' profile assessment to Notion before an interview\n\nScheduling interviews has become easier thanks to apps like Calendly, but evaluating candidates' skills and personality still requires a human touch and good psychological understanding. [Humantic AI](https://humantic.ai) can complement recruiters' evaluation, by generating psychometric assessments (including [DISC](https://en.wikipedia.org/wiki/DISC_assessment) and the [Big Five personality traits](https://en.wikipedia.org/wiki/Big_Five_personality_traits)) from candidates' résumés.\n\nFor this use case, we created [a workflow](https://n8n.io/workflows/1107) that is triggered when an interview is scheduled via Calendly. The Humantic AI node retrieves the LinkedIn profile of the candidate from Calendly, creates their psychometric assessment, then the Notion node inserts this information into a dedicated page.\n\nBy automating this process, you don't have to worry about your applicant database being up to date, and instead, you have more time to prepare for the meeting.\n\n![](https://lh3.googleusercontent.com/eam2Ikgu4aGqo6mcnDKyq3viUjLGH5zs9XA-p2_5gIroCFtZ4GCTFYmHK79PtTjEplv4GyaZwB8WlfZn-vL64uNLO1h9JbL9ImDb8Y_vprrNJdFyv3vbLaOno_5hhJ3jHURhO_aY)\n\n\n## Check to-do's in Notion and notify the assignee in Slack\n\nOne of the most useful applications of Notion for teams is keeping to-do lists where you can assign tasks to specific people. A limitation is that the in-app or email notifications can easily be overseen and are out of context. To overcome this issue, you can use [this workflow](https://n8n.io/workflows/1105) that regularly checks your to-do list in Notion and notifies the person in Slack when a new task is assigned to them.\n\n![Workflow for to-do list notification](https://lh5.googleusercontent.com/cYTvsnlN0p8PbiuPd7DTC2Ravaw8WB9U4m9NIKNA2dRKlQZtHH9X77DBFasfLLhIDJXtHbY9yHVHB52MMnVmVY9IDNgWJRGunpAcX-qwRTbx3DxJMHOir6IFxe3cSnR2A1tPI6n5)\n\n\nOf course, you can replace the Slack node with another messaging app of your choice. We prefer Mattermost and have built several [workflows around it](https://n8n.io/blog/5-workflow-automations-for-mattermost-that-we-love-at-n8n/), which substantially improve our communication and productivity.\n\n## Send notifications about new Notion notes to Mattermost\n\nAnother practical use of Notion pages is for taking notes in meetings and keeping them well-organised by topic or meeting type. After the meeting, you might first want to go get a coffee before taking care of the action items just discussed. But as it often happens, even a short break can interrupt your working flow, so you may easily forget what you needed to do or who you had to follow up with once you're back at your desk.\n\nLuckily, you can automate your chores away! [This workflow](https://n8n.io/workflows/1089) is triggered whenever new meeting notes are added in Notion. If the property field in the notes mentions the Marketing team, a message about the new notes will be sent to the Marketing channel in Mattermost, so all team members are up to date. Now you can enjoy your long-awaited coffee break knowing that the main action items will be taken care of.\n\n![Workflow for meeting notes updates](https://lh6.googleusercontent.com/tlVM2zUetq3H-9LuD930xgz42RQqWGQvd-JxKCQIf6P1s4oAEYLopnfZASlmk0ySFhxMgcp6FoVLfS4uZPEqrcbAerLXvYKnNT2kVSMSDNQQ8nHbYmNt6pR1y23Mk-BZq4m71Zwm)\n\n\n## Add positive feedback messages to a compliments table in Notion\n\nWe, humans, are prone to [negativity bias](https://en.wikipedia.org/wiki/Negativity_bias), meaning that we remember negative experiences more (strongly) than positive ones. For example, in a business context, when getting feedback from customers on your product or service, you are more likely to keep in mind that one negative review over the other tens of praise messages. This bias can impact your perception of your work and the team spirit, so it's important to highlight the good news.\n\nFor this use case, we built [a workflow](https://n8n.io/workflows/1109) that analyses the sentiment of feedback messages submitted via Typeform: messages with negative sentiment are added to a Trello board, while the ones with positive sentiment are added to a compliments table in Notion, then shared in a Slack channel. Looking once in a while at the impact you have on your customers is a wonderful way to keep your team motivated and inspired!\n\n![Workflow for feedback message analysis](https://lh6.googleusercontent.com/-z6QPNwDy4xAVe8u865TD9-Ba03wL4LMN4uj6BcRzNqZR92bwkK960oBEhVUcv6j3HEpjhH3Fo8v4L4QwLGvOQxulSRZeQ6FyX6EsnUSq1V9vKtBne8lwapynnH7mMI_vYYJHdwd)\n\n## Add memorable articles to your Notion reading list\n\nThe modern day reading list includes more than just books. [Millions of blog posts](https://wordpress.com/activity/posting/), news articles, and research papers are published every day, serving you information on how to solve problems, improve various skills, make informed decisions--or are just food for thought. You'll most probably stumble on some articles worth sharing or saving for a later (re-)read.\n\nTo help you manage your reading list, we designed [a workflow](https://n8n.io/workflows/1110) that automatically adds important articles to a Notion page from Discord. When you type in [Discord the slash command](https://discord.com/developers/docs/interactions/slash-commands) `/[URL]`, with the URL of the article you want to save, the workflow extracts the article title and adds the linked title to the reading list in your Notion page. When all is done, you get a confirmation message on Discord: \"The link was added to Notion.\" Now you can focus only on reading and taking notes.\n\n![Workflow for reading list management](https://lh5.googleusercontent.com/zy2CR3lyQClhHD8itnQjc1pTOApsKRH7LSqo-N63M8Z0xVE0yw6wHfSYs192NQr_GSwccSHykfqS9UTphbh8m145THKxvvq9nRr6GKxBX0w9JyyPsXU6ndmerBhy31db_VDlj0Mw)\n\n> This post was originally published on the [n8n blog](https://n8n.io/blog/5-tasks-you-can-automate-with-notion-api/)."
    },
    {
      "id": "/2021/05/12/google-apps-workflows",
      "metadata": {
        "permalink": "/blog/2021/05/12/google-apps-workflows",
        "source": "@site/blog/2021-05-12-google-apps-workflows.md",
        "title": "15 Google apps you can combine and automate to increase productivity",
        "description": "Google creates some of the best services on the web for professional use cases, with G Suite being the go-to choice for many businesses. One of the  main advantages is having an ecosystem of apps for various specific uses: from organisation and productivity tools to databases and language processing services.",
        "date": "2021-05-12T00:00:00.000Z",
        "formattedDate": "May 12, 2021",
        "tags": [
          {
            "label": "n8n",
            "permalink": "/blog/tags/n-8-n"
          }
        ],
        "readingTime": 4.835,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "post",
          "title": "15 Google apps you can combine and automate to increase productivity",
          "tags": [
            "n8n"
          ],
          "share-description": "Learn how to combine and automate popular Google apps for more productivity in the workplace.",
          "canonical_url": "https://n8n.io/blog/automate-google-apps-for-productivity/"
        },
        "prevItem": {
          "title": "5 tasks you can automate with the new Notion API",
          "permalink": "/blog/2021/06/08/notion-workflows"
        },
        "nextItem": {
          "title": "Automate your data processing pipeline in 9 steps",
          "permalink": "/blog/2021/05/04/etl-pipeline-workflow"
        }
      },
      "content": "Google creates some of the best services on the web for professional use cases, with G Suite being the go-to choice for many businesses. One of the  main advantages is having an ecosystem of apps for various specific uses: from organisation and productivity tools to databases and language processing services.\n\n[n8n](https://n8n.io) offers **15** [**nodes**](https://n8n.io/integrations/)for the most popular Google apps that you can combine to build powerful automated workflows. In this article, we'll give you some ideas of processes you can automate, so that you can boost productivity in the workplace, save time from repetitive tasks, and focus on what matters.\n\nSpecifically, we'll show you **six workflows** that use various Google nodes to automate analytics reports, database monitoring, onboarding processes, event registrations, feedback analysis, and file management--there's something for everyone!\n\n## Connect Google Analytics and BigQuery to Sheets and Gmail to automate your reports\n\nTo meet your business goals and check whether you're on the right track, it's important to keep an eye on different metrics (like visitors to your website, the performance of online campaigns) and create regular reports for stakeholders or clients. These tasks can be repetitive, so why not let [this ready-made workflow](https://n8n.io/workflows/892) do the work for you? The [**Google Analytics** node](https://docs.n8n.io/nodes/n8n-nodes-base.googleAnalytics/) will get the number of sessions on your website, grouped by country, and store the data in Airtable (or in [**Google Sheets**](https://docs.n8n.io/nodes/n8n-nodes-base.googleSheets/), to stay in the Google ecosystem).\n\n![](https://lh6.googleusercontent.com/OyQDv3Zwqsmg8CdLILFnJbdDNa_xYdNK5p9MYkSZro0aOhQn4qDxzDNhQ5C_sidZLtffA1hDJoi5tNcbcgeox5bluRufCu5_zOJs7qY2N81Wc509qgTDoAkIZj4SCNTrI3vCtq5g)\n\n\nYou can further extend this workflow by adding a [**Gmail** node](https://docs.n8n.io/nodes/n8n-nodes-base.gmail/) that emails the report to the stakeholders, so that you save up another 5 precious minutes of your work day! In addition, you can add a [**Google BigQuery** node](https://docs.n8n.io/nodes/n8n-nodes-base.googleBigQuery/)to retrieve records from or insert new records into your data warehouse. And if you're feeling particularly creative, you can even create a dashboard for KPIs from different sources, as explained in [this tutorial](https://n8n.io/blog/automatically-pulling-and-visualizing-data-with-n8n/).\n\n## Connect to Google Cloud Firestore to automatically monitor your databases\n\nAnother common use case related to data management is database monitoring, where you need to track the performance of your database and monitor key metrics in real-time, to quickly identify and fix potential issues. Google offers two cloud-based databases for real-time data syncing: Cloud Firestore and Realtime Database; n8n offers you the nodes to automate them: [**Google Cloud Firestore** node](https://docs.n8n.io/nodes/n8n-nodes-base.googleFirebaseCloudFirestore/#basic-operations) and [**Google Cloud Realtime Database** node](https://docs.n8n.io/nodes/n8n-nodes-base.googleFirebaseRealtimeDatabase/). You can schedule workflows like [this one](https://n8n.io/workflows/787) that regularly inserts data into a database or updates it.\n\n![](https://lh5.googleusercontent.com/FmAPPVq8Il79qdql-rymOmvkWNNc8H3Ag9__iBEC_gAvxI-XmrbULX-Fh4aB2opeoBbIwjw_AO75JhCJYD3be9JDaLOyadVocbWZAPaA6Ek8dhTizAfFH1XmmMGMLCLEVjqGHbyw)\n\n## Connect G Suite apps to automate the onboarding of new team members\n\nHiring new members for your team is an exciting moment! Not so exciting is the series of small boring tasks you need to do in advance, like creating an email account and setting up meetings. You can turn this process into an automated workflow that uses the [**Gmail** node](https://docs.n8n.io/nodes/n8n-nodes-base.gmail/) to detect when you get an email with the subject \"Welcome our new team member!\", then [creates a new user account](https://n8n.io/workflows/710) with the [**G Suite Admin** node](https://docs.n8n.io/nodes/n8n-nodes-base.gSuiteAdmin/), [adds the user to your contacts](https://n8n.io/workflows/637) using the [**Google Contacts** node](https://docs.n8n.io/nodes/n8n-nodes-base.googleContacts/), and finally schedules a welcome meeting for you using the [**Google Calendar** node](https://docs.n8n.io/nodes/n8n-nodes-base.googleCalendar/). Now your new hire is all set up for their first day and you have more time to prepare a nice onboarding experience (you could even let the [**Google Tasks** node](https://docs.n8n.io/nodes/n8n-nodes-base.googleTasks/) add that to your to-do list automatically).\n\n## Connect Typeform to Google Sheets and Calendar to automate event registrations\n\nOrganising an event involves a lot of work, even in these times when most of them are hosted online. One of the main challenges is keeping track of attendees and notifying them on time about the latest updates in the program. Doing these things manually would be not only time-consuming, but also prone to error. Luckily, this too can be automated and we even have an [in-depth tutorial](https://n8n.io/blog/supercharging-your-conference-registration-process-with-n8n/) dedicated to this use case. The workflow makes use of the [**Google Sheets** node](https://docs.n8n.io/nodes/n8n-nodes-base.googleSheets/) to store information from a registration form like [Typeform](https://www.typeform.com/), the [**Google Calendar** node](https://docs.n8n.io/nodes/n8n-nodes-base.googleCalendar/) to create events for the conference sessions, and the [**Gmail** node](https://docs.n8n.io/nodes/n8n-nodes-base.gmail/) to notify the attendees. For more engagement, you can use the [**Orbit** node](https://docs.n8n.io/nodes/n8n-nodes-base.orbit/) to pull in community metrics or post notifications.\n\n![](https://lh3.googleusercontent.com/NnGUrPUi8kYmXY3S4ccoPlNPpzRnHhQ7FwuF0EymJy6jfpU7Dc5ilwiyOJyN1U4jz_crrc0E7jHC8rbkVvS5JHr-3KV6Tfe7aZEMsIeCim6_yIC1eVoJVlzt_st1XA48747snOMq)\n\n\n## Connect Typeform with Google Natural Language and Sheets to analyse and store customer feedback\n\nGetting feedback on your product or service is an incredible opportunity for improvement. Particularly insightful are written reviews from customers, but simply reading them is not feasible, especially as your customer base grows. A [workflow for this case](https://n8n.io/workflows/1075) uses [**Google Cloud Natural Language** node](https://docs.n8n.io/nodes/n8n-nodes-base.googleCloudNaturalLanguage/) to analyse the sentiment of reviews submitted in Typeform, filter positive and negative reviews, and store them in [**Google Sheets**](https://docs.n8n.io/nodes/n8n-nodes-base.googleSheets/). Depending on your needs, you can also translate the reviews with the [**Google Translate** node](https://docs.n8n.io/nodes/n8n-nodes-base.googleTranslate/) before analysing them.\n\n![](https://lh3.googleusercontent.com/dpa4aF88iDaKpH3hKZbwsZ3JGgCteqe4ewFKrW47NUevxOy36CeNgJnrrf_cdby-G326Ew_piSuHOprxe5rcaRCct76oIGMp3F9R1K0SZIroNmrtHPo_iQhbJ95ABUbgvubVSzdN)\n\n## Connect Google Drive to Slides and Gmail to automatically manage your files\n\nLet's face it: we could all be a bit tidier in our digital workspace. But it seems like no matter how rigorously you organise your folders, a document always gets saved mistakenly in the wrong folder and before you know, digital clutter reigns.\n\nA solution to this problem is automating the regular file flow, so you don't have to worry about it anymore. For example, you can start with [this workflow](https://n8n.io/workflows/1035) to get the [**Google Slides**](https://docs.n8n.io/nodes/n8n-nodes-base.googleSlides/) of your weekly presentation, add the [**Google Drive** node](https://docs.n8n.io/nodes/n8n-nodes-base.googleDrive/) to store the slides in a shared team folder and the [**Gmail** node](https://docs.n8n.io/nodes/n8n-nodes-base.gmail/) to email them to the team members.\n\nYou can also connect the [**Google Drive** node](https://docs.n8n.io/nodes/n8n-nodes-base.googleDrive/) with other apps to automatically save files sent to you in a specific folder. Even your personal budget can be automated, as you can learn from [this tutorial](https://n8n.io/blog/automatically-adding-expense-receipts-to-google-sheets-with-telegram-mindee-twilio-and-n8n/) that shows you step-by-step how to add your expense receipts to [**Google Sheets**](https://docs.n8n.io/nodes/n8n-nodes-base.googleSheets/).\n\n> This post was originally published on the [n8n blog](https://n8n.io/blog/automate-google-apps-for-productivity/)."
    },
    {
      "id": "/2021/05/04/etl-pipeline-workflow",
      "metadata": {
        "permalink": "/blog/2021/05/04/etl-pipeline-workflow",
        "source": "@site/blog/2021-05-04-etl-pipeline-workflow.md",
        "title": "Automate your data processing pipeline in 9 steps",
        "description": "If you've ever struggled with setting up pipelines for extracting, transforming, and loading data (so-called ETL jobs), managing different databases, and scheduling workflows -- know that there's an easier way to automate these data engineering tasks.",
        "date": "2021-05-04T00:00:00.000Z",
        "formattedDate": "May 4, 2021",
        "tags": [
          {
            "label": "n8n",
            "permalink": "/blog/tags/n-8-n"
          },
          {
            "label": "tutorials",
            "permalink": "/blog/tags/tutorials"
          }
        ],
        "readingTime": 10.555,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "post",
          "title": "Automate your data processing pipeline in 9 steps",
          "tags": [
            "n8n",
            "tutorials"
          ],
          "share-description": "Learn how to build an n8n workflow that processes text, stores data in two databases, and sends messages to Slack.",
          "canonical_url": "https://n8n.io/blog/automate-your-data-processing-pipeline-in-9-steps-with-n8n/"
        },
        "prevItem": {
          "title": "15 Google apps you can combine and automate to increase productivity",
          "permalink": "/blog/2021/05/12/google-apps-workflows"
        },
        "nextItem": {
          "title": "6 findings from analysing the Oscars speeches of the best directors",
          "permalink": "/blog/2021/04/25/oscars-speech-analysis-with-python"
        }
      },
      "content": "If you've ever struggled with setting up pipelines for extracting, transforming, and loading data (so-called ETL jobs), managing different databases, and scheduling workflows -- know that there's an easier way to automate these data engineering tasks.\n\nIn this tutorial, you'll learn how to build a no-code n8n workflow that processes text, stores data in two databases, and sends messages to Slack.\n\n## The data pipeline\n\nA few months ago, I completed a Data Science bootcamp, where one week was all about data engineering, ETL pipelines, and workflow automation. The project for that week was to create a database of tweets that use the hashtag #OnThisDay, along with their sentiment score, and post tweets in a Slack channel to inform members about historical events that happened on that day. This pipeline had to be done with [Docker Compose](https://docs.docker.com/compose/) and included six steps:\n\n1.  Collect tweets with the hashtag #OnThisDay\n2.  Store the collected tweets in a [MongoDB](https://www.mongodb.com/) database\n3.  Extract tweets from the database\n4.  Process the tweets (clean the text, analyse sentiment)\n5.  Load the cleaned tweets and their sentiment score in a [Postgres](https://www.postgresql.org/) database\n6.  Extract and post tweets with positive sentiment in a [Slack](https://slack.com/intl/en-de/) channel\n\n![](https://n8n.io/blog/content/images/2021/04/ETL_pipeline_simple2-1.png)\n\n\n## The workflow with Python\n\nThis is a fun project that offers lots of learning opportunities about different topics: APIs, text processing with Natural Language Processing libraries, both relational and non-relational databases, social media and communication apps, as well as workflow orchestration.\n\nIf you're wondering, like I did, why we had to use two different databases, the answer is simple: for the sake of learning more. Postgres and MongoDB represent not only different database providers, but different kinds of database structures -- [relational (SQL) vs non-relational (NoSQL)](https://www.mongodb.com/nosql-explained/nosql-vs-sql) -- and it's useful to be familiar with both.\n\nThough our use case is just for fun, this pipeline can support most common data engineering tasks (e.g. aggregating data from multiple sources, setting up and managing the data flow across databases, developing and maintaining data pipelines).\n\nI was really excited, though also a bit overwhelmed by all the things I had to set up for this project. In total, I spent five days learning the tools, debugging, and building this pipeline with Python (including libraries like [Tweepy](https://www.tweepy.org/), [TextBlob](https://textblob.readthedocs.io/en/dev/), [VADER](https://github.com/cjhutto/vaderSentiment), and [SQLAlchemy](https://www.sqlalchemy.org/)), Postgres, MongoDB, [Docker](https://www.docker.com/), and [Airflow](https://airflow.apache.org/) (most frustrating part...). If you're interested to see how I did this, you can check out the project on [GitHub](https://github.com/lorenanda/tweets-docker-pipeline) and read [this blog post](/bootcamp7).\n\nBut in this article, I'll show you an easier way to achieve the same result in as much as an hour -- with a **no-code workflow** in n8n!\n\n## The workflow with no code\n\nSince I started using n8n, I've been looking for use cases for various data science tasks, starting with my existing projects. When I realised that all the apps and services that I used in my tweets pipeline are available as n8n nodes, I decided to replicate the project as an n8n workflow with nine nodes:\n\n1.  [Cron node](https://docs.n8n.io/nodes/n8n-nodes-base.cron/) to schedule the workflow\n2.  [Twitter node](https://docs.n8n.io/nodes/n8n-nodes-base.twitter/) to collect the tweets\n3.  [MongoDB](https://docs.n8n.io/nodes/n8n-nodes-base.mongoDb/) to store the tweets\n4.  [Google Cloud Natural Language](https://docs.n8n.io/nodes/n8n-nodes-base.googleCloudNaturalLanguage/) to analyse the sentiment of the tweets\n5.  [Set](https://docs.n8n.io/nodes/n8n-nodes-base.set/) to extract the sentiment values\n6.  [Postgres](https://docs.n8n.io/nodes/n8n-nodes-base.postgres/) to store the tweets and their sentiment\n7.  [IF](https://docs.n8n.io/nodes/n8n-nodes-base.if/) to filter positive and negative tweets\n8.  [Slack](https://docs.n8n.io/nodes/n8n-nodes-base.slack/) to post tweets into a channel\n9.  [NoOp](https://docs.n8n.io/nodes/n8n-nodes-base.noOp/) to ignore negative tweets\n\n![](https://lh6.googleusercontent.com/F42x14NNPvLbk_b68rOkkpXQDLRegGXTOjkK3PONzLJxPdoaAquCW32eYMzM0aLO2svLcCvv7txB8km2DWg7H7i55AmR67u2b624CXf_hXqfogfpEbCjS6poAxIu235bQ6UtJUVC)\n\nIn this article, I'll show you how to set up this workflow node by node. If this is your first n8n workflow, have a look at our [quickstart guide](https://docs.n8n.io/getting-started/quickstart.html) to learn how to set up n8n and how to navigate the Editor UI. It's also helpful to have at least basic knowledge of databases and SQL.\n\nOnce you have your n8n Editor UI open, there are two ways to follow this tutorial: either copy the workflow from [here](https://n8n.io/workflows/1045) into your Editor UI and deactivate the nodes, so that you can execute and test each node separately, or add the nodes one at a time.\n\n### 1\\. Starting the workflow\n\nWe will begin with the end in mind: We know that we want this whole workflow to run every day, so first we need to set up the **Cron node** to trigger our workflow every day at 06:00.\n\n![](https://lh4.googleusercontent.com/fY39vU3j9UgE-kZa7oCbuGLckzIYtlhGxNIYXpraCR7SljiX1t9zpwOp0rd6-agoXRY6RS4c3N4P7gpwG9lQE5D55WMvRaaYrEYfHOqjWf0oUMIuxW1gPOC8RxolI3JbB46NUqJg)\n\n\nThe Cron node makes it very easy to schedule and trigger workflows, compared to setting up [scheduling and triggers in Airflow](https://airflow.apache.org/docs/apache-airflow/1.10.1/scheduler.html), and this saved me so much time and nerves!\n\n### 2\\. Collecting tweets\n\nNext, we are going to collect tweets with the hashtag #OnThisDay. To do this, first you need to create a [Twitter Developer](https://developer.twitter.com/) account and register an app. Follow the instructions [in our reference docs](https://docs.n8n.io/credentials/twitter/) to learn how to set up your Twitter app and get the necessary credentials (Consumer Key and Consumer Secret). Once you have your credentials, copy and paste them in the ***Credentials*** field of the **Twitter node**. Next, set the parameters:\n\n-   *Operation*: Search\n-   *Search Text*: #OnThisDay\n-   *Limit*: 3. This last step is not mandatory, but I recommend limiting the number of collected tweets at least for testing the workflow, to ensure that you don't reach the query rate limit of the Twitter API and Google Cloud Natural Language.\n\n![](https://lh4.googleusercontent.com/PeJmypmy2yeLPl6LUmqTdVOlYI3tU-gc4LNfO8WSWKxor8wotBiaNYVbYoJbVMKHDfXlDRH1EHQW2OXnsqYoIQE-YrVjlAcyMIxfUToopAaERnM9UJKHszGPMpCC_jedmI04lu8K)\n\n### 3\\. Inserting tweets into MongoDB\n\nNow that we collected some tweets, we need to store them into a database. MongoDB is a non-relational database (NoSQL) that stores data in JSON-like documents. Since our tweets are returned in JSON format, MongoDB is the ideal database to store them in and the **MongoDB node** allows us to connect to the database. Before configuring the node, you need to create a MongoDB instance, set up a cluster, create a database and a collection within it.\n\n1.  [Create a MongoDB account](https://account.mongodb.com/account/register)\n2.  Set up a cluster: *cloud.mongodb.com  Clusters  Create New Cluster*\n3.  Create a database: *Cluster  Collections  Create Database*\n4.  Create a collection: *Cluster  Collections  Database  Create Collection*\n5.  Create a field: *Collection  Insert document  Type the field \"text\" below \"_id\"*\n6.  Allow access to the database: *Project  Security  Network Access  IP Access List  Add your IP address.*\n7.  Connect to the database from your terminal:\\\n    *mongo \"mongodb+srv://YourClusterName.mongodb.net/YourDatabaseName\" --username YourUsername*\n\nIf you need more detailed information or other set up options, refer to the [MongoDB documentation](https://docs.atlas.mongodb.com/connect-to-cluster/). Now that we have a MongoDB collection up and running, we can set up the MongoDB nodefor our workflow. Set up:\n\n-   *Connection String*: mongodb+srv://YourClusterName.mongodb.net/YourDatabaseName\n-   *Database*: YourDatabaseName\n\n![](https://lh4.googleusercontent.com/bZKvOmb77F_53sYOrMb9vhozRKP9ONC7_GXTx3yRwGpA3QZIx-mK7JY-El3W_n7kzrNo4GTB6_yOfUgTyJ39ki3wrrbEv1b4z_UeWIq7qB_sVLhUWo1sIWOQmkhbkMn-Kz0vw0VI)\n\n\nNext, configure the node parameters to insert the collected tweets into the collection:\n\n-   *Operation*: Insert\n-   *Collection*: YourCollectionName\n-   *Fields*: text\n\n![](https://lh3.googleusercontent.com/F4KpCoY7W8hor4N7si5b_U8QBtNiR-cKfBYdMeZIl7AJ00YlsVhWSZmI61yIfU5qFuPi2x0D7jxQeYIJqi1u-75m67AhIFeMSdeNOPkh7aX21ia6Oomz3csSUP-VFoPrX7E1DJiS)\n\n### 4\\. Analysing the sentiment of tweets\n\nHere comes my personal favourite part of this workflow: analysing the sentiment of tweets, i.e. the feeling associated with the entire text or entities in the text. For this, we use the **Google Cloud Natural Language node**, which analyses a text and returns two numerical values:\n\n-   **score**: Sentiment score between -1.0 (negative sentiment) and 1.0 (positive sentiment).\n-   **magnitude**: A non-negative number in the [0, +inf) range, which represents the absolute magnitude of sentiment regardless of score (positive or negative).\n\nBoth results are returned as documentSentiment in JSON format:\n\n{% raw %}\n```json\n{\n\"magnitude\": number,\n\"score\": number\n}\n```\n{% endraw %}\n\nBefore configuring the node, you have to sign up on the [Google Cloud Platform](https://cloud.google.com/) to enable the API and get the necessary credentials (Client ID and Client Secret). Follow the instructions in [our reference docs](https://docs.n8n.io/credentials/google/#prerequisites) to set up your account and the node credentials.\n\nOnce that's done, add an expression to the parameter *Content* by clicking on the gear icon and selecting *Current Node  Input Data  text*.\n\n![](https://lh6.googleusercontent.com/utW8tHN_ZgvOkw91HB2S63vwMvi4ujHuR2RAQGYw_Q0D2vaTKDFbROda34tQZ2P5zc_pJyt32ZeNE8cML4h4X-CoaGHlkzgI-fG1nWEVN1zknH7KK01ElxP8aPrE02prcX7SUrhj)\n\nAs a side note, here it was interesting to see how differently Google Cloud Natural Language and the VADER and TextBlob libraries evaluated the sentiment of text:\n\n|Tweet|GCNL|VADER|TextBlob|\n|---|---|---|---|\n|#OnThisDay in 2018!Champions League,quarter-final,1st legLiverpool-Manchester City 3-01-0 Salah(12),2-0 Oxlade-Chamber...|0.3|0|0|\n|Champions 🏆🏆#OnThisDay in 2016 🎉|0.1|0.7269|0|\n|#OnThisDay - 4th Apr 2018 former player, assistant manager &amp; caretaker manager passed away aged 61.3 years #GBNFRa...|0|0|-0.05|\n\n### 5\\. Processing sentiment analysis\n\nNow that we have sentiment scores for each tweet, we want to insert the text, sentiment score, and magnitude of the tweets into a new Postgres database. Since the magnitude sentiment score and the magnitude are included in the documentSentiment, we need to extract them in order to insert the values in two separate columns in Postgres.\n\nFor this, we use the **Set node**, which allows us to set new values based on the data we already have. In the node parameters, set three values:\n\n-   **Score** (number): *Current > Node > Input Data > JSON  documentSentiment > score*\n-   **Magnitude** (number): *Current Node > Input Data > JSON  >documentSentiment > score*\n-   **Text** (string): *Current Node > Input Data > JSON > sentences > [Item: 0] > text > content*\n\n![](https://lh5.googleusercontent.com/TGiCiuzI7T0I1vzsCKqI9K6HKL040hSPqXMnq-bYgSB3Hp-t6iKXbxc_W8jbg2njiV5BMl8ztpgZbpEAvg1ulpVror7ln-mxIgbYejaDZC8BJW5EafnZILkkxijuHoSr7aO-e4ax)\n\n### 6\\. Inserting tweets values into Postgres\n\nNext, we want to insert the newly set data values into a Postgres database. First, you need to [install Postgres](https://www.postgresql.org/download/), then create a database and a table for tweets. The process is quite similar to the MongoDB setup and you can do this from your terminal:\n\n1.  Connect to Postgres: `psql`\n2.  Create a database: `createdb twitter`\n3. Go into the created database: `psql twitter`\n4. Create columns in the database. The columns have to be named like the values defined in the Set node, in order to be matched. `CREATE TABLE tweets (text varchar(280), score numeric(4,3), magnitude numeric(4,3));`\n\nNow we can go ahead and configure the **Postgres node**. Fill in the name of your database, username, and password in the *Credential Data* fields, then configure the node parameters:\n\n-   *Operation*: Insert\n-   *Table*: tweets\n-   *Columns*: text, score, magnitude\n-   *Return Fields*: *\n\n![](https://lh4.googleusercontent.com/5A9pkc4DcQO5YE5IOJSPopYst9ELHwcISIDHxnfQI3T2CwZpio5ATw31y-AFCqLVT77EDSALP0Q43cbeBQ9D7G6pXGNTObA9KDexwmoIMNGDv038x0mHqdWEI2jxywKGVJgQscRV)\n\nAfter executing the node, you can check if the tweets have been inserted in the table by running *SELECT * FROM tweets;* in the terminal.\n\n### 7\\. Filtering positive and negative tweets\n\nHere comes another fun part related to sentiment analysis: filtering negative tweets. For this, we use the **IF node**, which allows us to split the workflow conditionally based on comparison operations. We define positive tweets as those with a sentiment score above 0. To configure the IF node with this condition, configure the parameters:\n\n-   *Value 1*: Current Node > Input Data > JSON > score\n-   *Operation:* Larger\n-   *Value 2:* 0\n\n![](https://lh4.googleusercontent.com/BvZZPOZzhvwmfD9T7Ogs0bnHb5b8dasX2xnAuSD_cppyZ2fWiMFVPV3uo4oUKsAfBabCCvHg4vlbm-RfDrceHdDTse2D0uJ2ncuEOFSwnndptKsa2yqmLb9Av28TKD1O01SAm_XV)\n\nThis condition determines the data flow to the following connection: if the sentiment score is greater than 0, the tweet will be sent to Slack, otherwise it will just be kept stored in the database.\n\n### 8\\. Sending positive tweets to Slack\n\nThe way to send tweets from a database to a Slack channel is via a Slackbot, which you have to create from your Slack account. Follow the instructions [on Slack](https://api.slack.com/bot-users) and in [our reference docs](https://docs.n8n.io/credentials/slack/) to learn how to create your Slackbot and get the necessary credentials (Access Token).\n\nOnce you have the Slack node credentials set up, configure the parameters:\n\n-   *Resource*: Message\n-   *Operation*: Post\n-   *Channel*: YourSlackChannelName\n-   *Text*: 🐦 NEW TWEET with sentiment score {% raw %}`{{$json[\"score\"]}}`{% endraw %} and magnitude {% raw %}`{{$json[\"magnitude\"]}}`{% endraw %}  {% raw %}`{{$json[\"text\"]}}`{% endraw %}\n\n![](https://lh3.googleusercontent.com/CU1W4OcN0xfGqQyxntlVVDEUzEndcftzYF-utzWb29pt3tGqibEyWMj7JTypmGhOJBWo3K0FVSnkd-O2-OyYhpj33705_MPo9S0sZ_NXmkDYa65Og13xf8yqbHYmjmjKC0JGghcU)\n\n\nAfter executing the node, check your Slack channel for a new tweet:\n\n![](https://n8n.io/blog/content/images/2021/04/Screenshot-from-2021-04-08-09-23-39.png)\n\n\n### 9\\. Ignoring negative tweets\n\nThe last node in this workflow is the **NoOp node**, which is used when we don't want to perform any operations. The purpose of this node is to make the workflow easier to read and understand where the data flow stops. Though this node is not necessary for our workflow, I included it to mark visually the false condition and make it clear that the workflow can be extended in this direction.\n\nFinally, execute the whole workflow and activate it, so that it runs as scheduled. Also, check your MongoDB collection and Postgres database to make sure that the tweets have been inserted properly.\n\n## What's next?\n\nCongrats -- you now have an automated workflow that informs you every day about positive historical events that happened on that day! As usual, you can tweak and extend this workflow, for example by keeping track of whether a tweet has been processed already, adding an action for the condition when the IF node is false, or cleaning the text of the collected tweets to check whether it influences the sentiment score.\n\nOf course, you can also build other ETL pipelines for various business use cases, such as product feedback at scale, Jira ticket automation based on customer sentiment, or regular database querying for reporting.\n\n> This post was originally published on the [n8n blog](https://n8n.io/blog/automate-your-data-processing-pipeline-in-9-steps-with-n8n/)."
    },
    {
      "id": "/2021/04/25/oscars-speech-analysis-with-python",
      "metadata": {
        "permalink": "/blog/2021/04/25/oscars-speech-analysis-with-python",
        "source": "@site/blog/2021-04-25-oscars-speech-analysis-with-python.md",
        "title": "6 findings from analysing the Oscars speeches of the best directors",
        "description": "Cinephiles and lovers of glam rejoice this Sunday – it's Oscars Award Ceremony time! Due to Corona, this year's 93rd edition was postponed from February to April 25th – the latest in the history of the award. I was personally happy about the delay, because it gave me more time to analyse some data and come up with this blog post.",
        "date": "2021-04-25T00:00:00.000Z",
        "formattedDate": "April 25, 2021",
        "tags": [
          {
            "label": "data science",
            "permalink": "/blog/tags/data-science"
          },
          {
            "label": "Python",
            "permalink": "/blog/tags/python"
          }
        ],
        "readingTime": 3.315,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "post",
          "title": "6 findings from analysing the Oscars speeches of the best directors",
          "tags": [
            "data science",
            "Python"
          ],
          "comments": true
        },
        "prevItem": {
          "title": "Automate your data processing pipeline in 9 steps",
          "permalink": "/blog/2021/05/04/etl-pipeline-workflow"
        },
        "nextItem": {
          "title": "5 workflow automations for Mattermost that we love at n8n",
          "permalink": "/blog/2021/04/09/mattermost-workflows"
        }
      },
      "content": "**Cinephiles and lovers of glam rejoice this Sunday – it's Oscars Award Ceremony time!** Due to Corona, this year's 93rd edition was postponed from February to April 25th – the latest in the history of the award. I was personally happy about the delay, because it gave me more time to analyse some data and come up with this blog post.\n\nOn this ocassion, I was curious to do some text mining on the acceptance speeches. Specifically, I analysed the speeches of the Best Directors between 1941 and 2019. I used a dataset from [Kaggle](https://www.kaggle.com/unanimad/the-oscar-award) and added missing data for 2017, 2018, and 2019 directly from the [Academy Awards Acceptance Speech database](http://aaspeechesdb.oscars.org/).\n\nIn total, 74 Best Directors have been awarded and almost all of them gave acceptance speeches, which I analysed with Python and the NLTK library. You can find the Jupyter notebook [here](https://colab.research.google.com/drive/18lgeB3LHdXg2Ly6cjMi49W5VNlmkqZNc?usp=sharing). Let's see what the words reveal about the Best Directors and the Oscars!\n\n### 1. Average speech length\nThe speech of a Best Director has 104 words on average, but speeches range widely from 8 to 267 words.\n\nHere's how to calculate the number of words in a text:\n\n```python\ndirecting[\"words\"] = directing['Speech_clean'].str.split().str.len()\n```\n\n### 2. Longest & shortest speeches\nThe longest speech runs at 267 words and was given by Mel Gibson at the 68th Academy Awards in 1995 for his film *Braveheart*. This guy had a looot of people to thank to and seems to have used up all his words for saying pretty much nothing.\n\nThe shortest speech was summed up in 8 words by Delbert Mann at the 28th Academy Awards in 1955 for his film *Marty*. I really like his efficient \"I came. I won. I thanked.\" structured speech:\n> Thank you. Thank you very much. Appreciate it.\n\nHere's how to find the longest and shortest text in a dataframe with pandas:\n```python\ndirecting.sort_values(by=\"words\")\n```\n\n### 3. Lexical richness\nLexical richness is a measure of how many unique words are used in the text. Lexical richness is calculated as the total number of unique words divided by the total number of words. The higher the score, the richer the vocabulary–and viceversa. Here's to calculate lexical richness for each speech in the dataframe with Python:\n\n```python\ndef lexical_richness(text):\n    return round(len(set(str(text))) / len(str(text)), 3)\ndirecting[\"lex_rich\"] = [lexical_richness(directing[\"Speech_clean\"][i]) for i in range(len(directing))]\n```\n\nThe speech with the highest lexical richness (0.408) is Delbert Mann's, the director of *Marty*, awarded in 1955. This means that 40.8% of the words he used are distinct.\n\nAt the other end, the speech with the lowest lexical richness (0.034) is Mel Gibson's, the director of *Braveheart*, awarded in 1995. This means that 3.4% of the words he used are distinct.  \n\n### 4. Longest words\nThe longest words used in directors' speeches have 15 words: *administrations*, *cinematographer*, and *czechoslovakian*.\n\nHere's how to select the longest words in a text:\n```python\nlong_words = [w for w in all_speeches_tokenized if len(w) > 14]\nsorted(long_words)\n```\n\n### 5. Most common words\nThe top 10 most common words in all acceptance speeches are: *thank* (201 occurences), *much* (56), *like* (50), *people* (48), *want* (42), *would* (30), *movie* (26), *film* (26), *say* (24), and *many* (22).\n\nInterestingly, out of these 10 words, 3 are nouns (referring to *people* and *film*/*movie*), 2 express large quantities (*much* and *many*), and 5 are verbs that express personal feelings (*want*, *like*) or actions (*say*, *thank*). It's also worth noting that the word *thank* has a significantly higher frequency than the following common words, which is however understandable.\n\nHere's how to find the frequency distribution of words in a text with NLTK:\n```python\nFreqDist(all_speeches_tokenized).most_common(10)\n```\n\n### 6. \"Thank\" to...\nOk, winners thank a lot, but who do they thank to? It turns out... to *you*, but also to *the Pacific Command of the United States*, *Mr. harry Cohn*, *Marlon*, *the producers*, and *each one of them*.\n\nHere's see the location of a word in context with NLTK:\n\n```python\nText(word_tokenize(all_speeches)).concordance('thank')\n```\n\nThat's all, folks!"
    },
    {
      "id": "/2021/04/09/mattermost-workflows",
      "metadata": {
        "permalink": "/blog/2021/04/09/mattermost-workflows",
        "source": "@site/blog/2021-04-09-mattermost-workflows.md",
        "title": "5 workflow automations for Mattermost that we love at n8n",
        "description": "n8n is a fair-code licensed tool that helps you automate tasks, sync data between various sources, and react to events --- all via a visual workflow editor. Our team has been using Mattermost for internal communication since the very beginning, and in time we have developed a ChatOps practice by integrating Mattermost with our workflows.",
        "date": "2021-04-09T00:00:00.000Z",
        "formattedDate": "April 9, 2021",
        "tags": [
          {
            "label": "n8n",
            "permalink": "/blog/tags/n-8-n"
          }
        ],
        "readingTime": 5.015,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "post",
          "title": "5 workflow automations for Mattermost that we love at n8n",
          "tags": [
            "n8n"
          ],
          "share-description": "Discover five of our favorite use cases of n8n with Mattermost, for both work productivity and team engagement.",
          "canonical_url": "https://n8n.io/blog/5-workflow-automations-for-mattermost-that-we-love-at-n8n/"
        },
        "prevItem": {
          "title": "6 findings from analysing the Oscars speeches of the best directors",
          "permalink": "/blog/2021/04/25/oscars-speech-analysis-with-python"
        },
        "nextItem": {
          "title": "How to automate your reading habit just in time for World Poetry Day",
          "permalink": "/blog/2021/03/21/poetry-workflow"
        }
      },
      "content": "[n8n](https://n8n.io/) is a fair-code licensed tool that helps you automate tasks, sync data between various sources, and react to events --- all via a visual workflow editor. Our team has been using [Mattermost](https://mattermost.com/) for internal communication since the very beginning, and in time we have developed a [ChatOps practice](https://mattermost.com/guides/chatops/) by integrating Mattermost with our workflows.\n\nIn this article, we present five of our favorite use cases of n8n with Mattermost, for both work productivity and team engagement.\n\n### Ensure gender-inclusive language\n\nWe value diversity and want to have a workplace where each of our team members feels heard and included. To ensure this, we created a gender inclusive bot for our Mattermost channels.\n\nIf someone addresses the group as \"guys\" or \"gals\", the bot promptly replies with: \"May I suggest \"folks\" or \"y'all\"? We use gender inclusive language here. 😄\"\n\nYou can find this [workflow here](https://n8n.io/workflows/982) and tweak it for your team.\n\n![gender-neutral n8n](https://mattermost.com/wp-content/uploads/2021/04/Gender-neutral-n8n.webp \"Gender inclusive\")\n\n## Schedule coffee chats\n\nSince we started working from home, we miss having spontaneous chats over a cup of coffee in the office kitchen. But we found a way to keep this habit and catch up with the team even remotely.\n\nWe created a workflow that runs every Monday morning, randomly divides the members of a Mattermost channel into groups, announces the pairs in the chat, and creates an appointment for a coffee chat in each person's calendar.\n\nRead our [blog post](https://n8n.io/blog/how-to-host-virtual-coffee-breaks-with-n8n/) to learn how to set up this workflow for your team.\n\n![A Mattermost chat window with a message posted by the n8n bot announcing the groups for the weekly coffee chats.](https://lh3.googleusercontent.com/rWSStaBE_XIZRga1oLejTKwjxxyY1gJFgIvbNY2lTGRFo6284-UmqlzQ4T49SdgDQnrd0uZMyzs39B33GPtrTZyaq65YNamjLuSjHLWxD1vKAo_fWCrObdYbfIt9fnSX1stNAqVA \"Coffee chats\")\n\n## Track Twitter mentions\n\nWe love to see how people all over the world use n8n for creative workflows. Many users share their workflows on Twitter and we want to keep track of these mentions, so that our team gets inspired and knows how to improve the product.\n\nTo this end, we created a workflow that searches for tweets that mention @n8n_io and posts them into a dedicated Mattermost channel every minute.\n\nRead our [blog post](https://n8n.io/blog/creating-triggers-for-n8n-workflows-using-polling/) to learn how to set up this workflow for yourself.\n\n![A Mattermost chat window with snippets of tweets posted by the n8n bot.](https://lh4.googleusercontent.com/QgGGOrdAh8T3bE0nKsLoXtrATieTdm6IcrCdNRk26_rtdKGyGhdt4aC4Y8qWIMEag5emfcKU99juS7fcgYqShyTKfIUtzpvhkkThYcgs8TeiIv4GroLKlUG_fNJFdSncVuVTFN-G \"Twitter notifications\")\n\n## Announce version releases\n\nWe are continuously improving our product and regularly release new versions.\n\nIn order to keep everyone up to date with the latest changes, we automated the announcement of n8n version releases in Mattermost. When a new version is created, it triggers a workflow, which posts the version update in the channel. From here, one of our developers can push the new version to staging or production on n8n.cloud with a click directly in the chat window.\n\nAnnouncements like these put a smile on our face!\n\n![A Mattermost chat window with messages about n8n version releases posted by the n8n bot.](https://lh4.googleusercontent.com/7T3eD-6aJpPbAk7f98oJ__dzNFEKx4sqPc5Wso5cmjF2xzRCnqFL5rgfxbhXnDH8ZY6YrX_52Oi7CxmPhdBiaqlpVD_K-QOxzr7c4S-zjA-4_0_kBDp3hw9C8Tgs99JtFFw8ycq5 \"Release announcements\")\n\n## Custom slash commands\n\nMattermost provides [slash commands](https://docs.mattermost.com/developer/slash-commands.html), which are useful for performing operations quickly by simply typing a word. On top of the basic ones, we created several custom slash commands integrated with n8n workflows for different purposes.\n\n### /brand command\n\nThe `/brand` command returns n8n's brand assets --- like our brand color, icon and logo, marketing copy, and useful administrative links. This slash command is especially useful to the Design and Marketing teams, when in need of a quick reference.\n\n![Messages showing n8n's brand color, logo, and description, posted by the n8n bot in a Mattermost channel.](https://lh5.googleusercontent.com/LONNZmN16A_3bFKNPdc3oVWR-joyUfM7L5J9D5A4y_ZqSDio7de1vRperrj6jDYRmNGzEDmQEBwNtxCLdoaZsDV_DQ8dL2o1kDPRex3oF_eDGrbeuTDFf8ipISWmj3wmU-7cIzpJ \"/brand command\")\n\n### /call command\n\nSometimes, we need to hop on a call spontaneously to discuss or clarify some things. We use Whereby for ad-hoc meetings, and to make the call setup process even easier, we created a custom Mattermost slash command `/call.`\n\nWhen someone invokes it, it posts their Whereby URL in the Mattermost channel with the message: \"Please join a call with me at: [Whereby-URL]\".\n\nRead our [blog post](https://n8n.io/blog/the-ultimate-guide-to-automate-your-video-collaboration-with-whereby-mattermost-and-n8n/) to learn how to set up this workflow for your team.\n\n![](https://lh4.googleusercontent.com/txLmgows1iMSroXKH_pgg9JBSSsJ53IMAnTNEacDLY4ifWjK0BXKjtWNBH3QA3YB8hdEtqo5ifVxeIThemyph8PF7HUiMh7YrW21d1skkBVUWgFThtt-r2fNbBGACSMVphlz0gdU)\n\n### /devrel command\n\nOur DevRel team handles many projects in parallel, from the documentation and blog posts to user interviews and community engagement. Naturally, various questions about content or technical details arise.\n\nTo get help, we created the slash command `/devrel`, which returns a link to a request form where team members can describe their issue or question, which is then converted into a Jira ticket for the team.\n\n![Message showing a link to a typeform posted in a Mattermost chat window.](https://lh3.googleusercontent.com/SqNvNjxelPSVB3_v3kw3uz6EO9WP1iZN9qapz_AycAoSAV5vr1CCDNqc7hhP7f8YEMLSGouiA8x_lU64R1jEF2UtLgZU7ammBVvOhFz29CyU_kveNJgJ3T1kfGVXq0WwwrYnov8L \"/devrel command\")\n\n### /docs command\n\nOur developers and technical writers are constantly writing and updating the documentation of n8n nodes. To help them keep track of the documentation status, we created the slash command `/docs`, which returns the number of undocumented nodes and the current coverage of the documentation in the Mattermost channel.\n\n![Message about the number of undocumented nodes and current coverage, posted by the n8n bot in a Mattermost channel.](https://lh6.googleusercontent.com/U0-yNrlfWJlLHAlz8p8hW_dm1ESIsbqyQ0JiYeT5K8K2VIrt_hJEr_EtzXI1WaCwG0pW4rLN-EYig9JiscPUxGKwH6fQmJgK0HKoXdlvykxeWFrK7sZ776NTO2uJp_beGlUOjb9r \"/docs command\")\n\n### /graphic command\n\nAlmost every week, we make a release of n8n with new nodes, features, or improvements, and share the news on our social media channels along with an image. This image used to be created manually by someone in our team --- a repetitive task that could easily be automated.\n\nFor that, we created a custom slash command `/graphic` that only requires the version number and links to the logos of the new nodes, and then outputs the release graphic directly into the chat window.\n\nThis workflow is particularly helpful for design and marketing teams.\n\n![Tweet posted by n8n_io announcing the release of version 0.110.3, with an attached image showing the version number and three logos of new nodes.](https://lh6.googleusercontent.com/RyMd5siNUWLAWs7MIaWcuzmTnl1_5PXvQYWbLFkIqmj2xy2zkXNm_AuHMtFwXq77_GLwhLLOtSo7dR8u6w6E1rQp0PD0uKC3NqZSMcwcBf1knWtG5xibUvxPnHDHS1DMzd3Ce7_L \"/graphic command\")\n\n## Conclusion\n\nTo sum up, we showed you five different n8n workflows used with Mattermost that make our lives easier and team happier. n8n's open ecosystem of integrations allows you to connect and incorporate Mattermost with your favorite services and products to create a unified automation workflow for a wide range of use cases.\n\n> This post was originally published on the [n8n blog](https://n8n.io/blog/5-workflow-automations-for-mattermost-that-we-love-at-n8n/)."
    },
    {
      "id": "/2021/03/21/poetry-workflow",
      "metadata": {
        "permalink": "/blog/2021/03/21/poetry-workflow",
        "source": "@site/blog/2021-03-21-poetry-workflow.md",
        "title": "How to automate your reading habit just in time for World Poetry Day",
        "description": "March 21st is World Poetry Day, a day dedicated to one of the most treasured ways of expression around the world and linguistic diversity. I have always loved reading poetry with creative rhymes and melodic rhythm, and lately I have come to appreciate free verse poetry as well. In any case, I would like to make time to read more poetry and discover new international poets.",
        "date": "2021-03-21T00:00:00.000Z",
        "formattedDate": "March 21, 2021",
        "tags": [
          {
            "label": "n8n",
            "permalink": "/blog/tags/n-8-n"
          },
          {
            "label": "NLP",
            "permalink": "/blog/tags/nlp"
          },
          {
            "label": "tutorials",
            "permalink": "/blog/tags/tutorials"
          }
        ],
        "readingTime": 3.95,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "post",
          "title": "How to automate your reading habit just in time for World Poetry Day",
          "tags": [
            "n8n",
            "NLP",
            "tutorials"
          ],
          "share-description": "Learn how to create a no-code workflow that gets international poems, translates them into one language, and sends you a poem in Telegram every day.",
          "canonical_url": "https://n8n.io/blog/world-poetry-day-workflow/"
        },
        "prevItem": {
          "title": "5 workflow automations for Mattermost that we love at n8n",
          "permalink": "/blog/2021/04/09/mattermost-workflows"
        },
        "nextItem": {
          "title": "Designing Bots by Amir Shevat",
          "permalink": "/blog/2021/03/14/book-review-designing-bots"
        }
      },
      "content": "March 21st is [World Poetry Day](https://www.un.org/en/observances/world-poetry-day), a day dedicated to one of the most treasured ways of expression around the world and linguistic diversity. I have always loved reading poetry with creative rhymes and melodic rhythm, and lately I have come to appreciate free verse poetry as well. In any case, I would like to make time to read more poetry and discover new international poets.\n\nTo automate this goal, I created an n8n workflow that sends a poem translated into Romanian in my Telegram chat every day. In this tutorial, I'll show you how to set it up in four steps.\n\n\n## The no-code workflow\n\nIf this is your first n8n workflow, have a look at our [quickstart guide](https://docs.n8n.io/getting-started/quickstart.html) first to learn how to install n8n. Once you have your n8n Editor UI open, there are two ways to follow this tutorial: either copy the workflow from [here](https://n8n.io/workflows/975) into your Editor UI and deactivate the nodes, so that you can execute and test each node separately, or add the nodes one at a time.\n\n![n8n Editor UI showing the workflow](https://lh4.googleusercontent.com/hK9r8CV6evcNaPzrTq-0KczIRvjbZWGDn3PDJzA1ZgnyX5DT_Gm4QR6LaK9iekbifXvLI00oq04_gdTAO12GzZz7Bp2BO7H-u_VkaP6gDE_BX2N4OH8SGdP4l7w4HLKWar-xjazP)\n\n\nThis workflow consists of four nodes:\n\n1.  [**Cron node**](https://docs.n8n.io/nodes/n8n-nodes-base.cron/): triggers the workflow every day at 10:00. You can change the time and interval based on your use case.\n2.  [**HTTP Request node**](https://docs.n8n.io/nodes/n8n-nodes-base.httpRequest/): makes an HTTP request to the [Poemist API](https://www.poemist.com/api/v1/randompoems) that returns random poems.\n3.  [**LingvaNex node**](https://docs.n8n.io/nodes/n8n-nodes-base.lingvaNex/): translates the returned poems into Romanian. You can choose from over 100 languages.\n4.  [**Telegram node**](https://docs.n8n.io/nodes/n8n-nodes-base.telegram/): posts the translated poems in the chat.\n\nNow let's see how to set up each node.\n\n### 1\\. Schedule the workflow\n\nFirst, you need to set the time and interval at which you want to receive poems in the chat. For this, set up the **Cron node**, where you can choose from seven modes and set the exact hour and minute**.** For this example, the workflow will be triggered every day at 10:00.\n\n![n8n Editor UI showing the setting for the Cron node](https://lh6.googleusercontent.com/hNquM3fL3pJn-LnjvXBYDg2VPTWjO1KtHkmdt-WbpRIQy8w2jmCcNDhCuZ-lKtIq0VY3AABhLt8CGrcr6vruayAZ5l71-MvoE3yz6yHAFZ1RdIro89rEHlBt7b9zmQW1A8ydMNOr)\n\n\n### 2\\. Get poems from Poemist\n\nNow, you need to get the poems. For this, set up the **HTTP Request node** to get data from [Poemist](https://www.poemist.com/). Paste the [Poemist URL](https://www.poemist.com/api/v1/randompoems) and execute the node to test that it's working.\n\n![n8n Editor UI showing the setting for the HTTP Request node](https://lh3.googleusercontent.com/j09KrCZMQ53LjjtBMN1Rd6X5rWFjhLFQCFW-AcKIvxdQY_WlC7v1rLLmZUoSt9dsbzwaeUK5u_SS_2W2QfdisISJS7-b6c6JzCipNLE5I5xDqrCSEwPNJknfoie_Nr2246OFGWT9)\n\n### 3\\. Translate poems with LingvaNex\n\nThe Poemist API includes poems in different languages, which on one hand is great for linguistic diversity, but on the other hand it can be challenging if you don't know the language.\n\nTo make sure you can read all poems, translate them into a language of your choice using the **LingvaNex node**. For this, you need to sign up on [LingvaNex](https://lingvanex.com/registration/) to get credentials. You can find detailed instructions [here](https://docs.n8n.io/credentials/lingvaNex/). Copy your credentials in *LingvaNex API*, then set *Translate to* Romanian (or any other language you prefer).\n\n![n8n Editor UI showing the setting for the LingvaNex node](https://lh5.googleusercontent.com/dzicYRHYyuKfKGcAL66sArOwXQBPUo1DfgiFobnFDMSSwM-oCun1dPNEJvQQ4M-LL2-kk0sSUasSPPY3hsZtJXgy20gh129p7aH5BLtTXmfagJOCRQv2XOaCAEHHWnE-UIrKDH05)\n\nIn the *Text* field, select:\n\n-   The **title**: *Current Node > Input data > JSON > 0 > title*\n-   The **author**: *Current Node > Input data > JSON > 0 > poet > name*\n-   The **poem**: *Current Node > Input data > JSON > 0 > content*\n\nNow execute the node to test that it's working.\n\n![n8n Editor UI showing the expression setting for the LingvaNex node](https://lh6.googleusercontent.com/w1vkcpm0TVsFh3Jds0nNJp4kINVdSIYbTKVa2ALX6_8_mEykiUG7TDlGMy669tpFicfEQ4nvFLbOwo5Bdb6BGEJgpIJTPoQbnGGWiDrpu0myQuq3PW8itAQY2fPBpH-Op1I4s0wY)\n\n### 4\\. Send poems to Telegram\n\nFinally, you need to send the requested poems to your Telegram chat. In your Telegram account, start a chat with [Botfather](https://telegram.me/BotFather) and follow the instructions to create a bot and get credentials. Then, go to the Telegram node and add the credentials to *Telegram API*.\n\nNext, you need to fill in the *Chat ID* of the newly created bot. There are two ways to get your Telegram Chat ID: with the [Telegram Trigger node](https://docs.n8n.io/nodes/n8n-nodes-base.telegram/#faqs) or with a Telegram bot. You can find instructions for both methods [here](https://docs.n8n.io/credentials/telegram/).\n\n![n8n Editor UI showing the setting for the Telegram node](https://lh6.googleusercontent.com/OmCU1qVKJ50mkimz8SZWmHaZKDKeOxzzxgLNSMI9IkeDxC8YPunmLou2XjOpgsJ0JwhKBrj6IzwRm1eQEPm9_nZiSAXo6BNyZEabbcBLWZtXXdXiS4e_OwP01Gc-fHcC5EYk9McW)\n\nAfter completing the *Chat ID*, you need to set up the text you want to send to the chat. Open the *Text* field and select the poem translation from *Current node > Input Data > JSON > result*. You can add more text or emojis to prettify the message.\n\n![n8n Editor UI showing the text field setting for the Telegram node](https://lh5.googleusercontent.com/bWPOA6V5R1sigS_FJcBwtX_3Me5_n_W0kHqFelHCwh2lFJj8dS7g2Qm5TCV6kCQpR4mXzbokNRkUMSBxNUlSmNVDBKqCzGZbZ4826Rs5Bn7qNbgWQ45_JewFWu8aboTxn5oJvScl)\n\nNow execute the whole workflow and check your Telegram for an incoming message:\n\n![Message of a poem in a Telegram chat](https://n8n.io/blog/content/images/2021/03/chris-montgomery-smgTvepind4-unsplash-1.jpg)\n\nFinally, save and activate the workflow, so that it runs every day at the scheduled time.\n\n## What's next?\n\nThat was it! Now you know how to get international poems, translate them, and send them to a Telegram chat to get your daily dose of poetry.\n\n> This post was originally published on the [n8n blog](https://n8n.io/blog/world-poetry-day-workflow/)."
    },
    {
      "id": "/2021/03/14/book-review-designing-bots",
      "metadata": {
        "permalink": "/blog/2021/03/14/book-review-designing-bots",
        "source": "@site/blog/2021-03-14-book-review-designing-bots.md",
        "title": "Designing Bots by Amir Shevat",
        "description": "cover",
        "date": "2021-03-14T00:00:00.000Z",
        "formattedDate": "March 14, 2021",
        "tags": [
          {
            "label": "books",
            "permalink": "/blog/tags/books"
          }
        ],
        "readingTime": 2.09,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "post",
          "title": "Designing Bots by Amir Shevat",
          "tags": [
            "books"
          ]
        },
        "prevItem": {
          "title": "How to automate your reading habit just in time for World Poetry Day",
          "permalink": "/blog/2021/03/21/poetry-workflow"
        },
        "nextItem": {
          "title": "Exploring endangered languages with pandas",
          "permalink": "/blog/2021/02/27/endangered-languages"
        }
      },
      "content": "![cover](blog_images/books_designingBots.jpg)\n\n**Title:** Designing Bots\n**Author:** Amir Shevat\n**Publisher:** O’Reilly Media\n**Published:** May 2017\n**Pages:** 328\n**Rating:** 🌕🌕🌕🌕🌖\n\n\nChatbots are probably the cutest possible tech application! As soon I learned enough Python, I tried to build my own artificial friend with which to do small talk in the terminal. I had a lot of fun programming it, so later on I started building chatbots with [Rasa](http://rasa.com/) and [DialogFlow](https://cloud.google.com/dialogflow) for customer service and [student support](https://github.com/lorenanda/rasa-chatbot). But a good chatbot represents more than just a set of hard-coded lines of dialogue.\n\nIn my search for resources, I found the book ***Designing Bots*** by **Amir Shevat**, director of developer relations at Slack and pioneer in the field of consumer chatbots. His book focuses on the design (not development!) of conversational user interfaces, but also covers distribution, marketing, architecture, and monetization. These broad topics make it useful for developers, designers, product managers, and entrepreneurs alike.\n\nThe book is well-organized in four main parts:\n\n* Overview of bot platforms (Slack, Facebook Messenger, Alexa, and Kik) and types of bots (personal vs. team, super vs. domain-specific, business vs. consumer, voice vs. text).\n* Theory about use cases (notifications, productivity, commerce, customer service), conversational design (stories, branding, personality), promotion, and monetization of bots.\n* Practical implementation of two examples: a consumer bot on Facebook Messenger and a business bot on Slack.\n* Outlook on the future of conversational AI and market trends.\n\nThe part I enjoyed most was **Chapter 6**, about **Branding, Personality, and Human Involvement**. Branding refers to how users perceive your product, so it’s important to design a coherent and cohesive image of your bot, reflecting the values and culture of your business. Branding includes the name of the bot, its logo, the language it uses, and even how it handles ambiguous or difficult conversations. All in all, it has its own personality and there are seven aspects to consider when designing a bot personality:\n\n* environment (work vs. consumer)\n* audience (age, interests, location)\n* tasks (shopping, information, help)\n* runtime variations (inclusion of jokes)\n* locally relevant social acceptance (cultural differences in language)\n* existing branding (match the company brand)\n* values (reflect the company’s values)\n\nI hadn’t realized how complex bot design can be and how much work goes into creating a performant and likeable chatbot. Overall, I found the information provided in this book interesting and helpful, and recommend it to anyone interested in chatbot design. Now, I look forward to developing my chatbot with this new knowledge in mind!"
    },
    {
      "id": "/2021/02/27/endangered-languages",
      "metadata": {
        "permalink": "/blog/2021/02/27/endangered-languages",
        "source": "@site/blog/2021-02-27-endangered-languages.md",
        "title": "Exploring endangered languages with pandas",
        "description": "On the occasion of the International Mother Language Day (21st February), I did and exploratory analysis and data visualisation of the World Language Family Map dataset with pandas and matplotlib.",
        "date": "2021-02-27T00:00:00.000Z",
        "formattedDate": "February 27, 2021",
        "tags": [
          {
            "label": "data science",
            "permalink": "/blog/tags/data-science"
          },
          {
            "label": "linguistics",
            "permalink": "/blog/tags/linguistics"
          },
          {
            "label": "Python",
            "permalink": "/blog/tags/python"
          },
          {
            "label": "tutorials",
            "permalink": "/blog/tags/tutorials"
          }
        ],
        "readingTime": 3.125,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "post",
          "title": "Exploring endangered languages with pandas",
          "gh-repo": "lorenanda/world-languages",
          "gh-badge": [
            "star",
            "fork",
            "follow"
          ],
          "tags": [
            "data science",
            "linguistics",
            "Python",
            "tutorials"
          ]
        },
        "prevItem": {
          "title": "Designing Bots by Amir Shevat",
          "permalink": "/blog/2021/03/14/book-review-designing-bots"
        },
        "nextItem": {
          "title": "Pourquoi est-il important de protéger les langues en danger",
          "permalink": "/blog/2021/02/21/langues-en-danger"
        }
      },
      "content": "On the occasion of the [International Mother Language Day](https://www.un.org/en/observances/mother-language-day) (21st February), I did and exploratory analysis and data visualisation of the [World Language Family Map dataset](https://www.kaggle.com/rtatman/world-language-family-map) with pandas and matplotlib.\n\n## Setup\n\nThe first step in the exploratory analysis is to read in the data and get an overview of the features: how many rows and columns are included and what are the names of the columns.\n\n```python\nimport pandas as pd\ndf = pd.read_csv('languoid.csv')\ndf.head(10)\ndf.shape\nlist(df.columns)\n```\n\nThe dataset contains 23243 observations and 18 language features. For this project I am interested in only three features: level, name, and status. To make things easier, I select only these three features and create a new dataframe with the subset data:\n\n```python\nlns_df = df[['level', 'name', 'status']]\n```\n\n## Exploratory Analysis\n\n### Overview\n\nI start the exploratory analysis with an overview of the selected features in `lns_df`, so I want to see how many unique values each feature has:\n\n```python\nlns_df['level'].unique()\n```\n\n**Level** is a categorical variable with three levels:\n\n- **dialect**: a particular form of a language which is peculiar to a specific region or social group.\n- **family**: a group of languages related through descent from a common ancestral language or parental language.\n- **language**: an official language spoken in a region.\n\n```python\nlns_df['status'].unique()\n```\n\n**Status** is a categorical variable with 6 levels. They are based on UNESCO’s Language Vitality and Endangerment framework that establishes six degrees of vitality/endangerment. From safest to extinct, these are:\n\n- **safe**: language is spoken by all generations; intergenerational transmission is uninterrupted.\n- **vulnerable**: most children speak the language, but it may be restricted to certain domains (e.g., home).\n- **definitely endangered**: children no longer learn the language as mother tongue in the home.\n- **severely endangered**: language is spoken by grandparents and older generations; while the parent generation may understand it, they do not speak it to children or among themselves.\n- **critically endangered**: the youngest speakers are grandparents and older, and they speak the language partially and infrequently.\n- **extinct**: there are no speakers left.\n\n### Language level\n\nNext, more specifically I want to see how many language families, languages, and dialects are in the dataset:\n\n```python\nlang_sum = pd.value_counts(df['level'].values, sort=False)\n```\n\nThere are 8444 language families, 4268 languages, and 10531 dialects. It makes sense that there are way more dialects than languages and language families, since a language family encompasses several languages, which in turn have different dialects. It’s easier to put these numbers into perspective by visualising them:\n\n```python\nimport matplotlib.pyplot as plt\n%matplotlib inline\nlang_sum.plot(kind='bar')\n```\n\n![Barplot](blog_images/endangered_languages_barplot.png)\n\n### Language status\n\nNow let's see the total number of languages in each status:\n\n```python\nstatus_sum = pd.value_counts(lns_df['status'], sort=True)\nstatus_sum.plot(kind='pie')\n```\n\nOver 80% of languages in the dataset are safe, which leaves 14% of data on endangered languages to analyse. Of these, most are extinct, and the fewest are critically endangered. Now let’s get a summary table of languages in each level by status:\n\n```python\nlevel_status = pd.crosstab(df.level, df.status)\nlevel_status.plot(kind='bar', stacked=True)\n```\n\n![Status_plot](blog_images/endangered_languages_status.png)\n\n### Endangered languages\n\nAfter getting an overview of the language status and level situations, I want to see what specific languages are in each group. Specifically, I was curious what language families were extinct:\n\n```python\next_fam = df[(df.level == 'family') & (df.status == 'extinct')]\next_fam_names = ext_fam[['name']]\n```\n\nThere are three extinct language families: Chimakuan, Djiwarli-Thiin, and Garrwan.\n\n## Next steps\n\nThis exploratory analysis is only a starting point, there are many other questions you can explore from this dataset. For example, find what dialects are critically endangered, what is the geographic distribution of endangered languages, or maybe analyse and visualise the data with other libraries than pandas and matplotlib. Have a look at my [Jupyter notebook](https://github.com/lorenanda/world-languages) and play around with the data!"
    },
    {
      "id": "/2021/02/21/langues-en-danger",
      "metadata": {
        "permalink": "/blog/2021/02/21/langues-en-danger",
        "source": "@site/blog/2021-02-21-langues-en-danger.md",
        "title": "Pourquoi est-il important de protéger les langues en danger",
        "description": "À l'occassion de la Journée internationale de la langue maternelle (21 février), j'ai écrit un essay pour attirer l'attention sur la diversité linguistique et l'importance de préserver les langues maternelles de chacun(e).",
        "date": "2021-02-21T00:00:00.000Z",
        "formattedDate": "February 21, 2021",
        "tags": [
          {
            "label": "linguistics",
            "permalink": "/blog/tags/linguistics"
          },
          {
            "label": "francais",
            "permalink": "/blog/tags/francais"
          },
          {
            "label": "thoughts",
            "permalink": "/blog/tags/thoughts"
          }
        ],
        "readingTime": 4.115,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "post",
          "title": "Pourquoi est-il important de protéger les langues en danger",
          "tags": [
            "linguistics",
            "francais",
            "thoughts"
          ],
          "comments": true
        },
        "prevItem": {
          "title": "Exploring endangered languages with pandas",
          "permalink": "/blog/2021/02/27/endangered-languages"
        },
        "nextItem": {
          "title": "January's regression challenge for Kaggle playground",
          "permalink": "/blog/2021/01/21/kaggle-playground-jan"
        }
      },
      "content": "**À l'occassion de la [Journée internationale de la langue maternelle](https://www.un.org/fr/observances/mother-language-day) (21 février), j'ai écrit un essay pour attirer l'attention sur la diversité linguistique et l'importance de préserver les langues maternelles de chacun(e).**\n\nIl y a **7102 langues humaines** reconnues dans le monde. C'est une richesse linguistique et culturelle extraordinaire, mais elle est toutefois ménacée, car environ **[3000 langues sont en danger](https://www.ethnologue.com/endangered-languages)**.\n\nL'UNESCO distingue [cinque degrés de danger](http://www.unesco.org/new/en/communication-and-information/access-to-knowledge/linguistic-diversity-andmultilingualism-on-internet/atlas-of-languages-in-danger/) pour une langue:\n\n1.  **Vulnérable**: la plupart des enfants parlent la langue, mais elle peut être restreinte à certains domaines (par exemple: la maison).\n2.  **En danger**: les enfants n'apprennent plus la langue comme langue maternelle à la maison.\n3.  **Sérieusement en danger**: la langue est parlée par les grands-parents ; alors que la génération des parents peut la comprendre, ils ne la parlent pas entre eux ou avec les enfants.\n4.  **En situation critique**: es locuteurs les plus jeunes sont les grands-parents et leurs ascendants, et ils ne parlent la langue que partiellement et peu fréquemment.\n5.  **Éteinte**: il ne reste plus de locuteurs.\"\n\nLes principaux [**critères d'évaluation** du degré de danger](https://www.ethnologue.com/endangered-languages) d'une langue sont:\n\n-   le nombre de locuteurs,\n-   la population ethnique,\n-   l'âge moyen des locuteurs,\n-   les domaines dans lesquels la langues est utilisée,\n-   l'attitude des locuteurs vers la langues,\n-   la transmission générationnelle de la langue.\n\nLa situation semble alarmante: au ce moment même, presque 50% des langues humaines périssent lentement. Toutefois, la question qui se pose alors est: pourquoi faudrait-il se soucier de la disparition d'une langue qui apparemment ne joue aucun rôle dans le contexte global? Quelle est l'importance d'une „mini-langue\" dans un monde qui communique en [**six langues majeures**](http://www.un.org/depts/DGACM/faqs.shtml) (Anglais, Francis, Espagnol, Arabe, Chinois, Russe), dont l'Anglais a le statut de lingua-franca? Insignifiante, si on considère une langue seulement comme un instrument de communication.\n\nPourtant, je vais argumenter qu'**une langue serve un objectif plus grand que le simple échange d'informations entre les gens**. Le point de départ pour cette discussion est indubitablement la théorie du déterminisme linguistique, selon laquelle la langue (native) détermine la pensée et les processus de réflexion humaines (la catégorisation, la mémoire, la perception). Peut-être la plus populaire forme de déterminisme linguistique est **l'Hypothèse de Sapir-Whorf,** formulée par Edward Sapir et Benjamin Lee Whorf. Les deux ont avancé quatre arguments principaux à l'appui de cette théorie:\n\n-   Chaque langue est une disposition sémantique, une ordre symbolique du monde, c'est-à-dire que chaque langue projete le monde d'une facon unique.\n-   Chaque langue reflète une interpretation personelle (d'une communauté) du monde, elle représente la vision du monde d'un peuple.\n-   La structure de chaque langue (la grammaire) contient une philosophie latente, une logique spécifique ou, en les paroles de Whorf, une „métaphysique latente\".\n-   Chaque langue détermine les modèle de pensée et donc influence le comportement de ses locuteurs. Par conséquent, une langue unifie ses locuteurs en conférent un sens de communauté.\n\nAlors que la plupart des langues sont liées dans une certaine mesure, en descendant de la meme famille linguistique (indo-européenne, sino-tibétane etc.) il a y a des langues avec des **structures tout à fait uniques**. Par exemple:\n\n-   *[Yuchi](http://www.yuchilanguage.org)*, une langue amérindienne isolée parlée dans le Nord-Ouest de l'Oklahoma, est clasifiée comme langue isolée parce qu'elle présente ds caractéristiques uniques. Par exemple, les substantives inanimés sont repartis en trois groupes: objets verticaux, objets horizontaux et objets ronds.\n-   *Oro Win* est une des 5 langues reconnues pour utiliser le son officiellement défini comme „aphone dentaire bilabial trille affriqu\".\n-   *Ket* est la seule langue sibérienne qui utilise un system tonal dans lequel le timbre de la voix peut donner des senses différents aux mots qui sont similaire acoustiquement.\n\nCes exemples prouvent le troisième point de l'Hypothèse de Sapir-Whorf: la richesse infinie et la complexité fascinante des langues humaines. L'extinction de chacune de ces langues signifierait la perte d'un aperçu dans les méchanisme de la langue et du langage.\n\nComme partie intégrante d'une culture, une langue reflète ses particularitées (coutumes, croyances, sens de l'humour, valeurs etc.), c'est-à-dire elle transfère de préciux éléments d'information sur la **culture** respéctive. En d'autres termes, une langue et l'ADN d'un people.\n\nDe plus, on doit regarder la problème des langues en danger d'un point de vue légal: la préservation d'une langue est un **droit de ses locuteurs**. Garder sa langue est pour beaucoup de communautées un moyen de résister à l'assimilation d'une communautée et implicitement une langue dominante. Ainsi, une langue confère ses locuteurs une certaine pouvoir.\n\nLa préservation et révitalisation des langues en danger apportent un atout incontestable aux locuteurs: le **bilinguisme**. Outre la fonction pratique de faciliter la communication entre deux langues/cultures, le bilinguisme offre des avantages cognitifs reconnus scientifiquement: capacité améliorée de mémoire et de décision.\n\n**En conclusion**, une langue est une source d'histoire, identité, culture, personalité d'un peuple. À grande échelle, une langue est une partie vitale du patrimoine intangible de l'humanité. Avec chaque langue éteinte, un morceau de l'histoire universelle, de l'humanité meme, disparrait."
    },
    {
      "id": "/2021/01/21/kaggle-playground-jan",
      "metadata": {
        "permalink": "/blog/2021/01/21/kaggle-playground-jan",
        "source": "@site/blog/2021-01-21-kaggle-playground-jan.md",
        "title": "January's regression challenge for Kaggle playground",
        "description": "At the beginning of the new year 2021, Kaggle created a new format of competitions aimed at beginners. On the 1st of each month, a month-long Playground competition is launched, where you can practice your ML skills on simple tabular datasets. Apart from competitive experience, the top 3 teams get to win some Kaggle merchandise!",
        "date": "2021-01-21T00:00:00.000Z",
        "formattedDate": "January 21, 2021",
        "tags": [
          {
            "label": "data science",
            "permalink": "/blog/tags/data-science"
          },
          {
            "label": "Python",
            "permalink": "/blog/tags/python"
          },
          {
            "label": "tutorials",
            "permalink": "/blog/tags/tutorials"
          }
        ],
        "readingTime": 1.905,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "post",
          "title": "January's regression challenge for Kaggle playground",
          "gh-repo": "lorenanda/kaggle-playground",
          "gh-badge": [
            "star",
            "fork",
            "follow"
          ],
          "tags": [
            "data science",
            "Python",
            "tutorials"
          ],
          "comments": true
        },
        "prevItem": {
          "title": "Pourquoi est-il important de protéger les langues en danger",
          "permalink": "/blog/2021/02/21/langues-en-danger"
        },
        "nextItem": {
          "title": "My year in code - 2020 review",
          "permalink": "/blog/2020/12/29/my-year-in-code"
        }
      },
      "content": "At the beginning of the new year 2021, Kaggle created a new format of competitions aimed at beginners. On the 1st of each month, a month-long Playground competition is launched, where you can practice your ML skills on simple tabular datasets. Apart from competitive experience, the top 3 teams get to win some Kaggle merchandise!\n\n## Exploratory Analysis\n\n[January's challenge](https://www.kaggle.com/c/tabular-playground-series-jan-2021) is about regression and contains a tabular dataset split into train and test sets.\n\nThe train set contains 300.000 data points with 14 features (continuous between 0 and 1) and 1 target variable (continuous between 6 and 10). The test set, used for making predictions, contains 200.000 data points with the same 14 features. The results are evaluated with the [RMSE score](https://en.wikipedia.org/wiki/Root-mean-square_deviation).\n\nFirst of all, I did a quick exploratory analysis. I was mainly interested in the correlation matrix, since in datasets with lots of features like this one, it’s likely that some of the variables are correlated – and it turned out this was the case:\n\n![Matrixplot](blog_images/kaggle-challenge-jan-matrix.png)\n\n## Machine Learning Models\n\nMoving on to the modelling part, I first split the train set further into a train and test set, then tried out four models:\n\n- **Linear Regression** just for fun, though I expected it to be too simplistic for this data set.\n- **Random Forest Regressor** with and without Recursive Feature Elimination. Contrary to my expectation, I got a better score with all features included.\n- **Gradient Boosting Regressor** with 100 estimators and a depth of 5 was slightly better that RFR.\n- **XGBoost with GridSearchCV** for finding the optimal hyperparameters. As expected, this model performed best, with little tweaking and from what I see it’s widely used in Kaggle competitions.\n\n## Results\n\nI submitted the predictions made by each model on Kaggle and here are the RMSE scores I achieved:\n\n**MODEL** | **RMSE**\nXGBoost | 0.69995\nGradient Boosting | 0.70723\nRandom Forest Regressor | 0.71094\nRandom Forest Regressor + RFE | 0.71459\nLinear Regression | 0.724\n\nMy best score placed me on position **281 out of 913**. As always, there’s room for improvement, maybe by tweaking the XGBoost hyperparameters or trying out other models.\n\nIn conclusion, though the data set was uninformative and not really insightful, this Kaggle challenge was a good exercise for trying out regression models."
    },
    {
      "id": "/2020/12/29/my-year-in-code",
      "metadata": {
        "permalink": "/blog/2020/12/29/my-year-in-code",
        "source": "@site/blog/2020-12-29-my-year-in-code.md",
        "title": "My year in code - 2020 review",
        "description": "2020 has definitely been an objectively crappy year... But in terms of programming experience, this year has been by far my most productive and enriching! It was my third year of coding, and second year of doing for work. I've learned a lot on the job and through self-study, developed my Python skills and even learned two new languages. Here's how my coding year progressed:",
        "date": "2020-12-29T00:00:00.000Z",
        "formattedDate": "December 29, 2020",
        "tags": [
          {
            "label": "data science",
            "permalink": "/blog/tags/data-science"
          }
        ],
        "readingTime": 3.585,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "post",
          "title": "My year in code - 2020 review",
          "tags": [
            "data science"
          ],
          "comments": true
        },
        "prevItem": {
          "title": "January's regression challenge for Kaggle playground",
          "permalink": "/blog/2021/01/21/kaggle-playground-jan"
        },
        "nextItem": {
          "title": "Detecting emotions from speech with neural networks",
          "permalink": "/blog/2020/12/20/bootcamp12"
        }
      },
      "content": "2020 has definitely been an objectively crappy year... But in terms of programming experience, this year has been by far my most productive and enriching! It was my third year of coding, and second year of doing for work. I've learned a lot on the job and through self-study, developed my Python skills and even learned two new languages. Here's how my coding year progressed:\n\n## January\n\n-   Started a new job as a student in Data Science/Analytics at a digital funeral home (it's really more exciting than it sounds).\n-   Learned BigQuery on the job and improved my SQL skills.\n-   Started learning JavaScript for automating Google Sheets and creating custom DataStudio visualisations.\n\n## February\n\n-   Automated boring tasks at work with Python.\n-   Analysed the text and sentiment of business reviews with Python.\n-   Worked for the first time with speech-to-text for data analysis, using Python and Google Cloud.\n\n## March\n\n-   Created my first chatbot with NLTK.\n-   Built a simple FAQ chatbot with Rasa.\n-   Learned to create a chatbot with Dialogflow.\n\n## April\n\n-   Completed two DataCamp projects in R to refresh my knowledge.\n-   Enrolled in a uni course \"Introduction to NLP with Python\".\n-   Started doing coding challenged on [codewars](https://www.codewars.com/users/lorenanda), LeetCode, and [HackerRank](https://www.hackerrank.com/datalingo).\n\n## May\n\n-   Created my data-related Instagram account [@datalingo](https://www.instagram.com/datalingo/).\n-   Completed the Natural Language Processing course on [Kaggle](https://www.kaggle.com/learn/certification/anerol/natural-language-processing).\n-   Got my first coding challenges on Python and SQL for data-related job applications (and passed 2/3!).\n\n## June\n\n-   Started creating my [Data Science portfolio on GitHub](https://github.com/lorenanda).\n-   Created several short [programs for automating](https://github.com/lorenanda/task-automation) various tasks.\n-   Completed home assignments on Python and NLP for the uni course \"NLP with Python\".\n\n## July\n\n-   Used GitLab (instead of GitHub) for a team project.\n-   Completed a uni team project on [sentiment analysis of German product reviews](https://github.com/lorenanda/Sentimentanalyse-HU-SS20).\n-   Passed my uni course \"NLP with Python\"!\n\n## August\n\n-   Completed my uni-related [psych-verbs project](https://github.com/lorenanda/psych-verbs), about the classification of Romanian verbs of emotion.\n-   Learned [Python for financial analysis](https://www.udemy.com/certificate/UC-83f50bf5-6c02-43d7-984b-7b6659f1f00f/).\n-   Learned [MongoDB basics](https://university.mongodb.com/course_completion/5500f5d7-fe4c-44fa-8824-354596083801).\n\n## September\n\n-   Started a [Data Science Bootcamp](/tags/#bootcamp)!\n-   Learned to implement algorithms and data structures in Python.\n-   Learned Julia for data analysis.\n-   Created an [animated scatterplot of Gapminder data](https://github.com/lorenanda/animated-scatterplot).\n\n\n## October\n\n-   Analysed and predicted the survival of [Titanic passengers using ML classification](https://github.com/lorenanda/titanic) models.\n-   [Analysed and predicted the demand](https://github.com/lorenanda/bike-demand-prediction) for bike shares based on weather data using regression models.\n-   Competed in two Kaggle challenges (Titanic and Capital Bikeshare).\n-   [Built a bot](https://github.com/lorenanda/lyrics-classification) that scrapes music lyrics and predicts the artist from input lyrics.\n-   [Analysed and predicted the temperature](https://github.com/lorenanda/weather-forecast) in Berlin using ARIMA models and Prophet.\n\n## November\n\n-   Learned AWS (EC2 and RDS) and built a [business dashboard with Metabase](https://github.com/lorenanda/northwind-dashboard) connected to a PostgreSQL database. \n-   Learned Docker and built a pipeline for streaming tweets, analysing their polarity, and posting them in a Slack channel. [`check it out`](https://github.com/lorenanda/tweets-docker-pipeline)\n-   Co-created a [Markov Chain Monte Carlo simulation of customers](https://github.com/lorenanda/Supermarket_MCMC_simulation) in a supermarket.\n-   Learned Deep Learning and built my first neural network models for [classifying images of clothes](https://github.com/lorenanda/fashion-mnist).\n-   Built a Neural Network [model that generates text](https://github.com/lorenanda/POEtry-generation) in the style of E.A. Poe poems.\n-   Created a [Slackbot for student support with Rasa](https://github.com/lorenanda/rasa-chatbot).\n\n## December\n\n-   Co-created a [movie recommender system](https://github.com/lorenanda/movie-recommender) and improved my front-end skills (HTML, CSS, Flask) in the process.\n-   Created [a program](https://github.com/lorenanda/speech-emotion-recognition) that detects emotions from speech and classifies live voice recordings, as my graduation project for the Data Science Bootcamp. \n-   Created [a tldr; program](https://github.com/lorenanda/tldr) that does sentiment analysis, named entity recognition, and summarization of web-scraped speeches from the German government, so you don't have to read it all. \n-   Started a mini-project of [holiday-related programs](https://github.com/lorenanda/holidays) that automate and liven up holiday wishes.\n-   Made my first open-source [contribution to the codebase of SpaCy](https://github.com/explosion/spaCy/pull/6621), by updating the Romanian stop words.\n-   Learned Java on Codecademy and Udemy over the holidays.\n\n## 202~~0~~1\n\nOverall, it was a pretty codeful year! I did put a tremendous amount of work into developing my programming skills and spent many nights and weekends on learning and fixing bugs. But the feeling of satisfaction I get now looking back at what I accomplished makes it all worth it. For 2021, I have many ideas for exciting personal and work projects, so I quite look forward to the new year!"
    },
    {
      "id": "/2020/12/20/bootcamp12",
      "metadata": {
        "permalink": "/blog/2020/12/20/bootcamp12",
        "source": "@site/blog/2020-12-20-bootcamp12.md",
        "title": "Detecting emotions from speech with neural networks",
        "description": "Project completed in week 12 (14.12.-18.12.20) of the Data Science Bootcamp at Spiced Academy in Berlin.",
        "date": "2020-12-20T00:00:00.000Z",
        "formattedDate": "December 20, 2020",
        "tags": [
          {
            "label": "data science",
            "permalink": "/blog/tags/data-science"
          },
          {
            "label": "bootcamp",
            "permalink": "/blog/tags/bootcamp"
          },
          {
            "label": "tutorials",
            "permalink": "/blog/tags/tutorials"
          },
          {
            "label": "Python",
            "permalink": "/blog/tags/python"
          },
          {
            "label": "NLP",
            "permalink": "/blog/tags/nlp"
          }
        ],
        "readingTime": 7.355,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "post",
          "title": "Detecting emotions from speech with neural networks",
          "gh-repo": "lorenanda/speech-emotion-recognition",
          "gh-badge": [
            "star",
            "fork",
            "follow"
          ],
          "tags": [
            "data science",
            "bootcamp",
            "tutorials",
            "Python",
            "NLP"
          ],
          "comments": true
        },
        "prevItem": {
          "title": "My year in code - 2020 review",
          "permalink": "/blog/2020/12/29/my-year-in-code"
        },
        "nextItem": {
          "title": "Best practices for software engineering",
          "permalink": "/blog/2020/12/12/bootcamp11"
        }
      },
      "content": ">Project completed in week 12 (14.12.-18.12.20) of the Data Science Bootcamp at Spiced Academy in Berlin.\n\nI did it! I graduated from the Data Science Bootcamp! On Friday I presented my final project, which was about detecting emotions from speech with neural networks. It was one of the most challenging project I've worked on, because I had to learn something new (how to process audio data and make live voice predictions) and prepare everything nicely in only 7 days. Here's how it went...\n\n\n## Project planning\n\nFirst and foremost, I designed a project plan, after having a brief look at the data set. From my work experience and the assignments completed in the past three months, I've learned that this step is crucial for the success of a coding project. Planning helps me (and the team) organize my ideas, break down the big project into smaller tasks, identify issues, and track the progress -- and not despair at the amount of work to be done in a short time. \n\nFor this purpose, I created a simple [Kanban board](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwjEp7_GwP_tAhXD6aQKHXP5ClMQFjAAegQIAhAC&url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FKanban_(development)&usg=AOvVaw2B54c6DIMX8rua56XtMTP9) directly in the GitHub repository of my project, so that I have the code and tasks in one place.\n\n<figure><img src=\"https://lorenaciutacu.files.wordpress.com/2021/01/screenshot_2020-12-11-lorenanda-speech-emotion-recognition.png\" alt=\"Project board in GitHub\" width=\"100%\" height=\"auto\" style=\"width: 100%; height: auto !important; max-width:960px;-ms-interpolation-mode: bicubic;\"/><figcaption><em>Project board in GitHub</em></figcaption></figure>\n\nTo create a project board linked to a repository in GitHub:\n1. In your desired repository, click on the tab `Projects`, then on `Create project`.\n2. Enter the `Project board name`.\n3. (Optional) Enter a `Description` of the project and select a `Project template`.\n4. Click on `Create project`.\n\n## Data set\n\nI used the **RAVDESS data set**, which contains 1440 audio files. These are voice recordings of 24 actors (12 male, 12 female) who say two sentences in two different intensities (normal and strong) with eight intonations that express different emotions: calm, happy, sad, angry, fearful, surprised, disgusted, and neutral. There are 192 recordings for each emotion, except for neutral, which doesn't have recordings in strong intensity. \n\nTo sum up, the original RAVDESS data set includes:\n\n* 1440 recordings\n* 24 speakers\n* 12 male, 12 female\n* 2 sentences\n* 2 intensities\n* 8 intonations / emotions\n* 192 recordings for 7 emotions\n* 96 recordings for 1 emotion\n\n<figure><img src=\"https://lorenaciutacu.files.wordpress.com/2020/12/plot_emotions.png\" alt=\"RAVDESS data set distribution\" width=\"100%\" height=\"auto\" style=\"width: 100%; height: auto !important; max-width:960px;-ms-interpolation-mode: bicubic;\"/><figcaption><em>RAVDESS data set distribution</em></figcaption></figure>\n\n### Oversampling\nThe data set was imbalanced, so I used the `RandomOversample` method to create new features for the neutral class. \n\n```python\ndef oversample(X, y):\n    X = joblib.load(\"speech_emotion_recognition/features/X.joblib\")\n    y = joblib.load(\"speech_emotion_recognition/features/y.joblib\")\n    print(Counter(y)) \n\n    oversample = RandomOverSampler(sampling_strategy=\"minority\")\n    X_over, y_over = oversample.fit_resample(X, y)\n\n    X_over_save, y_over_save = \"X_over.joblib\", \"y_over.joblib\"\n    joblib.dump(X_over, os.path.join(\"speech_emotion_recognition/features/\", X_over_save))\n    joblib.dump(y_over, os.path.join(\"speech_emotion_recognition/features/\", y_over_save))\n```\n\nOversampling added 96 new datapoints, so in the end I had **1536 audio files** to work with. \n\nAnother imbalance was gender-related: there were slightly more recordings by males and in normal intensity. I didn't deal with this imbalance because it wasn't significant to my project, since I only wanted to predict the emotion. However, it would be interesting to explore in the future.\n\n### Feature extraction\nThere are many features that can be extracted from audio files, but I decided to work with the **Mel Frequency Cepstral Coefficient (MFCC)**.\n\n> Mel-frequency cepstrum (MFC) is a representation of the short-term power spectrum of a sound, based on a linear cosine transform of a log power spectrum on a nonlinear mel scale of frequency. Mel-frequency cepstral coefficients (MFCCs) are coefficients that collectively make up an MFC.\n>\n> The difference between the cepstrum and the mel-frequency cepstrum is that in the MFC, the frequency bands are equally spaced on the mel scale, which approximates the human auditory system's response more closely than the linearly-spaced frequency bands used in the normal spectrum. This frequency warping can allow for better representation of sound, for example, in audio compression. \n>\n> [source](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum)\n\n\nTo extract the MFCC from the audio files, I used the Python library [`librosa`](https://librosa.org/doc/latest/index.html):\n\n```python\ndef extract_features(path, save_dir):\n    feature_list = []\n\n    start_time = time.time()\n    for dir, _, files in os.walk(path):\n        for file in files:\n            y_lib, sample_rate = librosa.load(\n                os.path.join(dir, file), res_type=\"kaiser_fast\"\n            )\n            mfccs = np.mean(\n                librosa.feature.mfcc(y=y_lib, sr=sample_rate, n_mfcc=40).T, axis=0\n            )\n\n            file = int(file[7:8]) - 1\n            arr = mfccs, file\n            feature_list.append(arr)\n\n    print(\"Data loaded in %s seconds.\" % (time.time() - start_time))\n\n    X, y = zip(*feature_list)\n    X, y = np.asarray(X), np.asarray(y)\n    print(X.shape, y.shape)\n\n    X_save, y_save = \"X.joblib\", \"y.joblib\"\n    joblib.dump(X, os.path.join(save_dir, X_save))\n    joblib.dump(y, os.path.join(save_dir, y_save))\n\n    return \"Preprocessing completed.\"\n```\n\nThe visual representation of MFCC looks like this: \n\n<figure><img src=\"https://lorenaciutacu.files.wordpress.com/2020/12/mfcc1.png\" alt=\"MFCC plot\" width=\"100%\" height=\"auto\" style=\"width: 100%; height: auto !important; max-width:960px;-ms-interpolation-mode: bicubic;\"/><figcaption><em>MFCC plot</em></figcaption></figure>\n\n## Machine Learning Models\n\nI trained three different neural networks models on the MFCC and emotion labels:\n\n-   **Multi-Layer Perceptron (MLP)** \n    ```python\n    def mlp_classifier(X, y):\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n\n    mlp_model = MLPClassifier(\n        hidden_layer_sizes=(100,),\n        solver=\"adam\",\n        alpha=0.001,\n        shuffle=True,\n        verbose=True,\n        momentum=0.8,\n    )\n    ```\n\n-   **Convolutional Neural Network (CNN)**\n    ```python\n    def cnn_model(X, y):\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n\n    x_traincnn = np.expand_dims(X_train, axis=2)\n    x_testcnn = np.expand_dims(X_test, axis=2)\n\n    model = Sequential()\n    model.add(Conv1D(16, 5, padding=\"same\", input_shape=(40, 1)))\n    model.add(Activation(\"relu\"))\n    model.add(Conv1D(8, 5, padding=\"same\"))\n    model.add(Activation(\"relu\"))\n    model.add(\n        Conv1D(\n            8,\n            5,\n            padding=\"same\",\n        )\n    )\n    model.add(Activation(\"relu\"))\n    model.add(BatchNormalization())\n    model.add(Activation(\"relu\"))\n    model.add(Flatten())\n    model.add(Dense(8))\n    model.add(Activation(\"softmax\"))\n\n    model.compile(\n        loss=\"categorical_crossentropy\",\n        optimizer=\"adam\", \n        metrics=[\"accuracy\"],\n    )\n\n    cnn_history = model.fit(\n        x_traincnn,\n        y_train,\n        batch_size=50, \n        epochs=100,\n        validation_data=(x_testcnn, y_test),\n    )\n    ```\n\n-   **Long Short-Term Memory (LSTM)**\n    ```python\n    def lstm_model(X, y):\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n    X_train_lstm = np.expand_dims(X_train, axis=2)\n    X_test_lstm = np.expand_dims(X_test, axis=2)\n\n    lstm_model = Sequential()\n    lstm_model.add(LSTM(64, input_shape=(40, 1), return_sequences=True))\n    lstm_model.add(LSTM(32))\n    lstm_model.add(Dense(32, activation=\"relu\"))\n    lstm_model.add(Dropout(0.1))\n    lstm_model.add(Dense(8, activation=\"softmax\"))\n\n    lstm_model.compile(\n        optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n    )\n\n    lstm_model.summary()\n    lstm_history = lstm_model.fit(X_train_lstm, y_train, batch_size=32, epochs=100)\n    ```\n\nAfter several iterations of tweaking the hyperparameters, I found that generally the models performed better with low learning rates (0.001), `adam` optimizer, and less layers. All models overfit (they couldn't generalize on unseen data), but this seems to be a common issue in neural networks and on audio data. \n\nAs expected, MLP had the lowest accuracy, since it's a very basic model (a simple feed-forward artificial neural network). CNN and LSTM had similar train accuracy (80%), but CNN performed better on test data (60%) than LSTM (51%). To give you some context, state-of-the-art models for speech classification have an accuracy of 70-80%, so I was quite happy with my CNN model accuracy.\n\n<figure><img src=\"https://lorenaciutacu.files.wordpress.com/2020/12/models_accuracy.png\" alt=\"Accuracy of different ML models\" width=\"100%\" height=\"auto\" style=\"width: 100%; height: auto !important; max-width:960px;-ms-interpolation-mode: bicubic;\"/><figcaption><em>Accuracy of different ML models</em></figcaption></figure>\n\nIt was particularly interesting to look at the actual vs. predicted emotions, to see what emotions were misclassified. From the correlations matrices of CNN and LSTM, I noticed that both models misclassified emotions that sound similar or are ambiguous (even for humans), like sad-calm or angry-happy.\n\n<figure><img src=\"https://lorenaciutacu.files.wordpress.com/2020/12/lstm_confusionmatrix.png\" alt=\"Confusion matrix of LSTM\" width=\"100%\" height=\"auto\" style=\"width: 100%; height: auto !important; max-width:960px;-ms-interpolation-mode: bicubic;\"/><figcaption><em>Confusion matrix of LSTM</em></figcaption></figure>\n\n<figure><img src=\"https://lorenaciutacu.files.wordpress.com/2020/12/cnn_confusionmatrix.png\" alt=\"Confusion matrix of CNN\" width=\"100%\" height=\"auto\" style=\"width: 100%; height: auto !important; max-width:960px;-ms-interpolation-mode: bicubic;\"/><figcaption><em>Confusion matrix of CNN</em></figcaption></figure>\n\n## Predictions\n\nThe exciting part was to make predictions on new data, more specifically on [movie sound clips](http://www.moviesoundclips.net/) and my own voice in real-time. To record my voice, I used the Python library [`sounddevice`](https://python-sounddevice.readthedocs.io/en/0.4.1/):\n\n```python\nimport soundfile as sf\nimport sounddevice as sd\nfrom scipy.io.wavfile import write\n\n\ndef record_voice():\n    fs = 44100  # Sample rate\n    seconds = 3  # Duration of recording\n    # sd.default.device = \"Built-in Audio\"  # Speakers full name here\n\n    print(\"Say something:\")\n    myrecording = sd.rec(int(seconds * fs), samplerate=fs, channels=2)\n    sd.wait()  # Wait until recording is finished\n    write(\"speech_emotion_recognition/recordings/myvoice.wav\", fs, myrecording)\n    print(\"Voice recording saved.\")\n```\n\nI then tested the CNN and LSTM models on pre- and live-recorded audio files:\n\n```python\ndef make_predictions(file):\n    cnn_model = keras.models.load_model(\n        \"speech_emotion_recognition/models/cnn_model.h5\"\n    )\n    lstm_model = keras.models.load_model(\n        \"speech_emotion_recognition/models/lstm_model.h5\"\n    )\n    prediction_data, prediction_sr = librosa.load(\n        file,\n        res_type=\"kaiser_fast\",\n        duration=3,\n        sr=22050,\n        offset=0.5,\n    )\n\n    mfccs = np.mean(\n        librosa.feature.mfcc(y=prediction_data, sr=prediction_sr, n_mfcc=40).T, axis=0\n    )\n    x = np.expand_dims(mfccs, axis=1)\n    x = np.expand_dims(x, axis=0)\n    predictions = lstm_model.predict_classes(x)\n\n    emotions_dict = {\n        \"0\": \"neutral\",\n        \"1\": \"calm\",\n        \"2\": \"happy\",\n        \"3\": \"sad\",\n        \"4\": \"angry\",\n        \"5\": \"fearful\",\n        \"6\": \"disgusted\",\n        \"7\": \"surprised\",\n    }\n\n    for key, value in emotions_dict.items():\n        if int(key) == predictions:\n            label = value\n\n    print(\"This voice sounds\", predictions, label)\n```\n\nBoth models identified the correct or plausible emotion from recorded speech!\n\n## Next steps\n\nIt was super exciting to work on this project and I'm already thinking of improving and extending it in some ways:\n\n-   Try other models (not necessarily neural networks).\n-   Extract other audio features to see if they are better predictors than the MFCC.\n-   Train on larger data sets, since 1500 files and only 200 samples per emotion is not enough.\n-   Train on natural data, i.e. on recordings of people speaking in unstaged situations, so that the emotional speech sounds more realistic.\n-   Train on more diverse data, i.e. on recordings of people of different cultures and languages. This is important because the expression of emotions varies across cultures and is influenced also by individual experiences.\n-   Combine speech with facial expressions and text (speech-to-text) for multimodal sentiment analysis."
    },
    {
      "id": "/2020/12/12/bootcamp11",
      "metadata": {
        "permalink": "/blog/2020/12/12/bootcamp11",
        "source": "@site/blog/2020-12-12-bootcamp11.md",
        "title": "Best practices for software engineering",
        "description": "Project completed in week 11 (06.12.-11.12.20) of the Data Science Bootcamp at Spiced Academy in Berlin.",
        "date": "2020-12-12T00:00:00.000Z",
        "formattedDate": "December 12, 2020",
        "tags": [
          {
            "label": "data science",
            "permalink": "/blog/tags/data-science"
          },
          {
            "label": "bootcamp",
            "permalink": "/blog/tags/bootcamp"
          },
          {
            "label": "swe",
            "permalink": "/blog/tags/swe"
          }
        ],
        "readingTime": 1.48,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "post",
          "title": "Best practices for software engineering",
          "tags": [
            "data science",
            "bootcamp",
            "swe"
          ],
          "comments": true
        },
        "prevItem": {
          "title": "Detecting emotions from speech with neural networks",
          "permalink": "/blog/2020/12/20/bootcamp12"
        },
        "nextItem": {
          "title": "Creating a movie recommender system with Python and Flask",
          "permalink": "/blog/2020/12/05/bootcamp10"
        }
      },
      "content": ">Project completed in week 11 (06.12.-11.12.20) of the Data Science Bootcamp at Spiced Academy in Berlin.\n\nThis was the last week of lectures and assigned projects, in which we learned general software engineering techniques and best practices for individual and team coding.\n\n### Software engineering metaphors\n\nOur teacher introduced us to three popular metaphors that conceptualize software engineering:\n\n-   **Bazaar** represents independently written code for open-source software, to which multiple developers can contribute as they see fit.\n-   **Cathedral** represents long complex code for a software built by a group of developers, based on a central plan and software infrastructure, meant to be deployed into a product.\n-   **Garden** represents maintaining code, cleaning it up and refactoring, deleting irrelevant/old code and replacing with code that meets the current needs.\n\n### 10 tips for software development\n\nMore importantly, we got 10 tips for efficient software development and clean code:\n\n1.  **Know your league** (Bazaar vs. Cathedral)\n2.  **Structure your code**\n3.  **Use git**\n4.  **Master debugging**\n5.  **Add essential files** (`requirements.txt`, `.gitignore`)\n6.  **Clean code**\n7.  **Add tests**\n8.  **Continuous integration**\n9.  **Verification and validation**\n10. **Write good documentation**\n\n### Tests\n\nTesting code is a really important part of software development or any complex project. Testing ensures that everything runs well and doesn't break as you make changes to the code. There are four main types of software tests:\n\n1.  **Unit Test:** for a single function/class/module.\n2.  **Integration Test:** for multiple components together.\n3.  **Regression Test:** re-run tests after changes to code.\n4.  **Acceptance Test:** user-level, A/B-testing, focus groups.\n\nI started applying these tips right away on the project that I'm currently working on and also started cleaning up a bit the code of my past projects. Next week I'll be working only on my final project and preparing the presentation for the graduation on Friday!"
    },
    {
      "id": "/2020/12/05/bootcamp10",
      "metadata": {
        "permalink": "/blog/2020/12/05/bootcamp10",
        "source": "@site/blog/2020-12-05-bootcamp10.md",
        "title": "Creating a movie recommender system with Python and Flask",
        "description": "Project completed in week 10 (30.11.-04.12.20) of the Data Science Bootcamp at Spiced Academy in Berlin.",
        "date": "2020-12-05T00:00:00.000Z",
        "formattedDate": "December 5, 2020",
        "tags": [
          {
            "label": "data science",
            "permalink": "/blog/tags/data-science"
          },
          {
            "label": "bootcamp",
            "permalink": "/blog/tags/bootcamp"
          },
          {
            "label": "Python",
            "permalink": "/blog/tags/python"
          }
        ],
        "readingTime": 2.285,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "post",
          "title": "Creating a movie recommender system with Python and Flask",
          "gh-repo": "lorenanda/movie-recommender",
          "gh-badge": [
            "star",
            "fork",
            "follow"
          ],
          "tags": [
            "data science",
            "bootcamp",
            "Python"
          ],
          "comments": true
        },
        "prevItem": {
          "title": "Best practices for software engineering",
          "permalink": "/blog/2020/12/12/bootcamp11"
        },
        "nextItem": {
          "title": "Classifying clothes images with neural networks",
          "permalink": "/blog/2020/11/28/bootcamp9"
        }
      },
      "content": ">Project completed in week 10 (30.11.-04.12.20) of the Data Science Bootcamp at Spiced Academy in Berlin.\n\nThis was a really exciting week, because we had a team project which combined the power of Machine Learning algorithms with the beauty of Web Development!\n\n### Making a recommender system\n\nSpotify's Discover Weekly, Netflix's \"Watch Next\", Amazon's \"You might like...\" -- these are all examples of recommender systems. They basically use the data (history) of their users (what music they listened to, what series they watched, what they bought) to discover patterns in their preferences and recommend more similar products (and in this way keep them consuming).\n\nThere are two main approaches to recommender systems:\n\n* memory-based (heuristic, non-parametric)\n  * content filtering: TF-IDF\n  * collaborative filtering: KNN\n* model-based (algorithmic, parametric)\n  * content filtering: Bayesian classification, neural networks\n  * collaborative filtering: Bayesian networks, SVD, neural networks\n\n\nIn our movie recommender project, we used two algorithms:\n\n-   **Non-negative Matrix Factorization** (**[NMF](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html#sklearn.decomposition.NMF)**)\n-   **Singular Value Decomposition **(**[SVD](https://surprise.readthedocs.io/en/stable/matrix_factorization.html)**)\n\n### Making a web-page with Flask\n\nI chose to focus on the web development part of the project, in order to refresh my HTML and CSS skills. I built the web page with **Flask**, a lightweight Python framework for web-development which makes it really easy to create simple web apps. Flask also uses Jinja templates, files that contain static data or placeholders for dynamic data. In this case, I used Jinja to render the movies from our dataset and the additional information (posters, trailers) that we got with the[ TMDB API.](https://developers.themoviedb.org/3/)\n\nOn the **main page**, we display 15 movies randomly selected from the 50 most rated movies in the dataset, to ensure that there is a high probability that the user knows at least some of these movies. Users are asked to rate as many of them as possible or leave the slider at 0 if they haven't seen the movie. In the second step, we ask users what kind of movies they prefer: old, new, or any. The answers to these two questions (movie ratings and year preference) are our input data, which is used for the NMF and SVD algorithms to make predictions.\n\nAfter submitting their responses (and waiting a bit, because the algorithms take a while to calculate), the users are taken to the **recommendations page.** Here we display 5 movies that are similar to movies the users have rated highest and belong to the preferred year category, as calculated by the algorithm.\n\nFriday Lightning Talk\n---------------------\n\nThe lightning talk of this week will actually take place next week on Wednesday, to give us more time to work on this complex project, while also brainstorming and trying out ideas for the final project. But we are almost done, so here is a demo of our recommender system:\n\n[![](https://lorenaciutacu.files.wordpress.com/2020/12/demo-1.gif?w=1024)](https://lorenaciutacu.files.wordpress.com/2020/12/demo-1.gif)"
    },
    {
      "id": "/2020/11/28/bootcamp9",
      "metadata": {
        "permalink": "/blog/2020/11/28/bootcamp9",
        "source": "@site/blog/2020-11-28-bootcamp9.md",
        "title": "Classifying clothes images with neural networks",
        "description": "Project completed in week 9 (23.11.-27.11.20) of the Data Science Bootcamp at Spiced Academy in Berlin.",
        "date": "2020-11-28T00:00:00.000Z",
        "formattedDate": "November 28, 2020",
        "tags": [
          {
            "label": "data science",
            "permalink": "/blog/tags/data-science"
          },
          {
            "label": "bootcamp",
            "permalink": "/blog/tags/bootcamp"
          },
          {
            "label": "tutorials",
            "permalink": "/blog/tags/tutorials"
          },
          {
            "label": "Python",
            "permalink": "/blog/tags/python"
          }
        ],
        "readingTime": 4.055,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "post",
          "title": "Classifying clothes images with neural networks",
          "gh-repo": "lorenanda/fashion-mnist",
          "gh-badge": [
            "star",
            "fork",
            "follow"
          ],
          "tags": [
            "data science",
            "bootcamp",
            "tutorials",
            "Python"
          ],
          "comments": true
        },
        "prevItem": {
          "title": "Creating a movie recommender system with Python and Flask",
          "permalink": "/blog/2020/12/05/bootcamp10"
        },
        "nextItem": {
          "title": "Creating a Markov chain Monte Carlo simulation in Python",
          "permalink": "/blog/2020/11/21/bootcamp8"
        }
      },
      "content": ">Project completed in week 9 (23.11.-27.11.20) of the Data Science Bootcamp at Spiced Academy in Berlin.\n\nThis week we dived into Deep Learning and learned about different types neural networks (NN) and their applications in various domains. The main goal of this project was to learn and understand what each hyperparameter in a NN model does and how to tune it, so this week was more theoretical and math-heavy than usual.\n\n### Building a Neural Network\n\nFor my first deep learning project, I used the famous [**Fashion MNIST dataset**](https://github.com/zalandoresearch/fashion-mnist) created by Zalando. The dataset contains 60K images of 10 clothing items (e.g., tops, sandals, trousers). In order to classify the images in the correct item category, I tried two types of NN:\n\n-   **Artificial Neural Network (ANN)**: represents a group of multiple perceptrons/ neurons at each layer. ANN is also called Feed-Forward Neural Network, because the inputs are processed only forward. An ANN consists of three layers: input, hidden, and output.\n-   **Convolutional Neural Network (CNN)**: uses filters to extract features and capture the spatial information from images. CNNs are the go-to model for image classification.\n\nIn this post, I will present only the CNN model, since it's the one that performed best in my project. Here's an overview of my model:\n\n```python\nmodel = keras.models.Sequential()\nmodel.add(Conv2D(32, kernel_size=(3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))\nmodel.add(MaxPooling2D((2, 2)))\nmodel.add(Flatten())\nmodel.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))\nmodel.add(Dense(10, activation='softmax'))\n```\n\nFirst [line 1] I instantiated the model. Then I started adding several layers of with different hyperparameters.\n\n-   `Conv2D` is a 2D convolution layer which creates a convolution kernel that is filled with layers input to produce a tensor of outputs. I used 32 filters, as it's recommended to use powers of 2 as values.\n-   `kernel_size` determines the height and width of the kernel, passed as a tuple.\n-   `activation` specifies the activation function, which transforms the summed weighted input from the node into the activation of the node. `relu` (Rectified Linear Activation Function) outputs the input directly if it is positive and 0 if it is negative.\n-   `kernel_initializer` refers to the functions for initializing the weights, which in this case is uniform distribution.\n-   `input_shape` represents the dimension of the images (28×28 px) and their color code (1 for black-and-white). This hyperparameter needs to be specified only in the first layer.\n\nNext [3] I added a `MaxPooling2D` layer, which downsamples the input representation by taking the maximum value over the window defined by `pool_size` (2, 2) for each dimension along the features axis.\n\nThen [4] I added a `Flatten` layer that flattens the images, so that the pixel values are between 0 an 1. This is done because when working with images, if the values are positive and large, a ReLU neuron becomes almost a linear unit, losing many of its advantages.\n\nLastly [5,6] I added two `Dense` layers, which are fully connected layers, where the first parameter declares the number of desired units. So in [5] I have a layer with has 100 neurons with ReLU activation. The last layer [6] has 10 hidden layers (number of clothing items) and `softmax` activation, which is used for multi-class classification.\n\n### Tuning the hyperparameters\n\nFinally, I compiled the model:\n\n```python\nmodel.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\n```\n\n-   `optimizer` defines the stochastic gradient descent algorithm that is used. I've tried both `sgd` (Stochastic Gradient Descent) and `adam` (Adaptive Moment Estimation), and stuck with the latter because it is more advanced it it generally performs better.\n-   `loss` defines the cost function.\n-   `metrics` is a list of all the evaluation scores I want to compute. In this case, accuracy is enough.\n\n```python\nmodel.fit(\n    xtrain,\n    to_categorical(ytrain),\n    epochs=15,\n    batch_size=32,\n    validation_data=(xtest, to_categorical(ytest))\n    )\n```\n\n-   `epochs` represents the number of iterations on the training data.\n-   `batch_size` is the number of images to feed tot he model in one go, it normally ranges from 16 to 512, but in any case it's smaller than the total number of samples.\n-   `validation_data` represents the part of the dataset kept for testing the model.\n-   `to_categorical` one-hot-encodes the labels (clothing items).\n\n### Evaluating the model performance\n\nThe CNN had an **accuracy of 99.43% on the train set **and** 90.69% on the test set**. This is a really good score! I think it could've been even better if I had let the model train longer (i.e. more epochs).\n\n![cover](https://lorenaciutacu.files.wordpress.com/2020/11/screenshot_2021-02-14-lorenanda-fashion-mnist.png?w=768)\n\nFriday Lightning Talk\n---------------------\n\nThis Friday talk was a bit different from the previous ones. Instead of presenting our projects, we had to read and present a paper about a Deep Learning application, like [generative art](https://ai.googleblog.com/2020/11/using-gans-to-create-fantastical.html), [object recognition](https://pjreddie.com/darknet/yolo/), or [text generation](https://www.theguardian.com/commentisfree/2020/sep/08/robot-wrote-this-article-gpt-3). Of course, I chose the latter topic and tried LSTM to [generate poems by E.A. Poe](https://github.com/lorenanda/POEtry-generation). But I talked about [GPT-3](https://arxiv.org/abs/2005.14165), a state-of-the-art deep learning model that can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. However, I focused not so much on the technical details, as on the[ ethics and implications ](https://dailynous.com/2020/07/30/philosophers-gpt-3/)of this technology. This opened an important discussion in our group, which I think should be included in the curriculum of every tech degree."
    },
    {
      "id": "/2020/11/21/bootcamp8",
      "metadata": {
        "permalink": "/blog/2020/11/21/bootcamp8",
        "source": "@site/blog/2020-11-21-bootcamp8.md",
        "title": "Creating a Markov chain Monte Carlo simulation in Python",
        "description": "Project completed in week 8 (16.11.-20.11.20) of the Data Science Bootcamp at Spiced Academy in Berlin.",
        "date": "2020-11-21T00:00:00.000Z",
        "formattedDate": "November 21, 2020",
        "tags": [
          {
            "label": "algorithms",
            "permalink": "/blog/tags/algorithms"
          },
          {
            "label": "data science",
            "permalink": "/blog/tags/data-science"
          },
          {
            "label": "bootcamp",
            "permalink": "/blog/tags/bootcamp"
          },
          {
            "label": "tutorials",
            "permalink": "/blog/tags/tutorials"
          },
          {
            "label": "Python",
            "permalink": "/blog/tags/python"
          }
        ],
        "readingTime": 3.23,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "post",
          "title": "Creating a Markov chain Monte Carlo simulation in Python",
          "gh-repo": "lorenanda/Supermarket_MCMC_simulation",
          "gh-badge": [
            "star",
            "fork",
            "follow"
          ],
          "tags": [
            "algorithms",
            "data science",
            "bootcamp",
            "tutorials",
            "Python"
          ],
          "comments": true
        },
        "prevItem": {
          "title": "Classifying clothes images with neural networks",
          "permalink": "/blog/2020/11/28/bootcamp9"
        },
        "nextItem": {
          "title": "Building a dockerized ETL pipeline for tweet streaming in Slack",
          "permalink": "/blog/2020/11/14/bootcamp7"
        }
      },
      "content": ">Project completed in week 8 (16.11.-20.11.20) of the Data Science Bootcamp at Spiced Academy in Berlin.\n\nThis week we learned to make a Markov Chain Monte Carlo (MCMC) simulation of new customers in a supermarket, based on data about customer paths from entrance to checkout through four aisles (fruit, drinks, dairy, and spices) recorded on five days from 7 am to 10 pm. This project was particularly challenging for two reasons: it involved object-oriented programming (OOP) and team work. In this post I'll give you an overview of our workflow.\n\n## Project planning\n\nI teamed up with two colleagues and right from the start we planned and divided our tasks. There are many things to track when working on a team project, but they can be classified into three main categories:\n\n| **Idea** | **Design** | **Implementation** |\n| ----- | ----- | ----- |\n| user story | state diagram | code-debug cycle |\n| requirements.txt | ML model architecture | skeleton code |\n| workflow tracker | class diagram | prototype |\n|  | entity-relationship diagram | baseline models |\n|  | flowchart |  |\n|  | data flow diagram |  |\n|  | pseudocode |  |\n\n## Markov-Chain Monte Carlo simulation\n\n**Markov-Chain Monte Carlo (MCMC)** is a method used to approximate the posterior distribution of a parameter of interest by random sampling in a probabilistic space. As the name says, it combines two properties:\n\n-   *Markov-Chain* uses each random sample (generated by a sequential process) as a stepping stone to generate the next random sample. While each new sample depends on the one before it, new samples do *not* depend on any samples before the previous one.\n-   *Monte Carlo *estimates the properties of a distribution by examining random samples from the distribution.\n\nThe first step in our project was to calculate the** transition probability matrix**, i.e. the probability that a customer moves from one aisle to another. For example, in our data there was a 28% probability that a customer goes from drinks to dairy.\n\nNext, we had to generate new customers and simulate their paths based on the calculated transition probabilities. For this, we had to use OOP. We created three classes:\n\n-   a *Customer class* that simulates a new customers and their path from entrance to checkout\n-   a *Supermarket class* that manages multiple Customer instances that are currently in the market\n-   a *SupermarketMap class* which visualizes the supermarket background. We used **numpy** and **OpenCV** to make the supermarket visualization, by basically creating and slicing arrays, and inserting icons in specific tiles.\n\n## A-star algorithm\n\nTo calculate the customers' path from entrance to checkout, we used the **[A* (star) algorithm](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwiJoKjah5btAhXSDewKHaYxAmkQFjAAegQIARAC&url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FA*_search_algorithm&usg=AOvVaw22lBtYLijVR0p3NaD3nVPy)**. This is a search algorithm used for finding the shortest path in environments where there are obstacles along the way (like walls or aisles in the supermarket). It works as a weighted graph, which in this case is our supermarket with 8 nodes (entrance, fruit, spices, dairy, drinks, three checkouts) and edges (paths) between them.\n\nA* starts from a specific starting node of a graph (entrance) and aims to find a path to the given goal node (checkout) having the smallest cost (least distance traveled, shortest time, etc.). Specifically, A* selects the path that minimizes $$f (n) = g(n) + h(n)$$, where:\n\n-   $$n$$ is the next node on the path.\n-   $$g(n)$$ is the cost of the path from the start node to $$n$$.\n-   $$h(n)$$ is a heuristic function that estimates the cost of the cheapest path from $$n$$ to the goal.\n\nA* terminates when the path it chooses to extend is a path from start to goal or if there are no paths eligible to be extended.\n\n## Friday Lightning Talk\n\nAt the end of the week, we gave a team presentation of our project. I presented the exploratory data analysis and visualization with Plotly, while my colleagues talked about the implementation of the A* algorithm and our experience and lessons learned from pair programming."
    },
    {
      "id": "/2020/11/14/bootcamp7",
      "metadata": {
        "permalink": "/blog/2020/11/14/bootcamp7",
        "source": "@site/blog/2020-11-14-bootcamp7.md",
        "title": "Building a dockerized ETL pipeline for tweet streaming in Slack",
        "description": "Project completed in week 7 (09.11.-13.11.20) of the Data Science Bootcamp at Spiced Academy in Berlin.",
        "date": "2020-11-14T08:30:00.000Z",
        "formattedDate": "November 14, 2020",
        "tags": [
          {
            "label": "data science",
            "permalink": "/blog/tags/data-science"
          },
          {
            "label": "bootcamp",
            "permalink": "/blog/tags/bootcamp"
          },
          {
            "label": "tutorials",
            "permalink": "/blog/tags/tutorials"
          },
          {
            "label": "Python",
            "permalink": "/blog/tags/python"
          }
        ],
        "readingTime": 6.675,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "post",
          "title": "Building a dockerized ETL pipeline for tweet streaming in Slack",
          "share-description": "Learn how I built an ETL pipeline for sentiment analysis and database storage with Python.",
          "gh-repo": "lorenanda/tweets-docker-pipeline",
          "gh-badge": [
            "star",
            "fork",
            "follow"
          ],
          "tags": [
            "data science",
            "bootcamp",
            "tutorials",
            "Python"
          ],
          "date": "2020-11-14 8:30:00 +0000",
          "last_modified_at": "2022-01-30 8:30:00 +0000"
        },
        "prevItem": {
          "title": "Creating a Markov chain Monte Carlo simulation in Python",
          "permalink": "/blog/2020/11/21/bootcamp8"
        },
        "nextItem": {
          "title": "Creating a Metabase dashboard on Postgres and AWS",
          "permalink": "/blog/2020/11/07/bootcamp6"
        }
      },
      "content": "> Project completed in week 7 (09.11.-13.11.20) of the Data Science Bootcamp at Spiced Academy in Berlin.\n\nThis week's project was the most complex and difficult so far. The **challenge** was to create a database of tweets, along with their sentiment score, and post positive tweets in a Slack channel. This pipeline had to be orchestrated with [Docker Compose](https://docs.docker.com/compose/).\n\nIn this post, I'll show you how I set up each step.\n\n**Note** that the purpose of this project was mainly educational, giving the bootcamp students the opportunity to learn and work with different tools. There are probably more efficient ways of achieving the same result (for example, using only one database).\n\n\n## 0\\. Prerequisites & tech stack\nHere's an overview of the apps, services, and libraries I used in this project:\n\n| Apps & Databases | Python libraries |\n|------------------|------------------|\n| Twitter          | [`tweepy`](https://www.tweepy.org/) & [`vader`](https://pypi.org/project/vaderSentiment/) |\n| Slack            | `slackclient`    |\n| [MongoDB](https://account.mongodb.com/account/register) | [`pymongo`](https://pymongo.readthedocs.io/en/stable/) |\n| [PostgreSQL](https://www.postgresql.org/download/) | [`psycopg2-binary`](https://www.psycopg.org/docs/install.html) & [`sqlalchemy`](https://www.sqlalchemy.org/) |\n| [Docker-Compose](https://docs.docker.com/compose/) | - |\n\n## 1\\. Collecting tweets\nTo collect tweets, I used the [Twitter API](https://developer.twitter.com/en/docs/twitter-api) along with the `tweepy` library.\n\nFirst, I [created an app on Twitter](https://developer.twitter.com/en/docs/apps/overview) and got my credentials (API key and Access Token). Then, I wrote the [Python code for streaming live tweets](https://github.com/lorenanda/tweets-docker-pipeline/tree/main/docker-compose/tweet_collector), using `tweepy` with my Twitter credentials. I chose to stream the hashtag *#OnThisDay* (thought it would be interesting to get a daily notification of what happened years ago) and collected the tweet text and user handle.\n\n```python\nfrom tweepy import OAuthHandler, Stream, API\nfrom tweepy.streaming import StreamListener\ntweet = {\n    'username': t['user']['screen_name'],\n    'text': t['text'],\n}\n\nstream_listener = StreamListener()\nstream = tweepy.Stream(auth=api.auth, listener=stream_listener)\nstream.filter(track=['OnThisDay'])\n```\n\n## 2\\. Storing tweets in MongoDB\nAfter collecting the tweets, I had to store them in MongoDB, a non-relational (NoSQL) database that stores data in JSON-like documents. Since the tweet data is collected as key-value pairs (JSON format), MongoDB is a good way to store this information.\n\nFirst, I had to create a MongoDB instance, set up a cluster, and create a database and a collection within it:\n\n1.  Create a MongoDB account\n2.  Set up a cluster: *cloud.mongodb.com > Clusters > Create New Cluster*\n3.  Create a database: *Cluster > Collections > Create Database*\n4.  Create a collection: *Cluster > Collections > Database > Create Collection*\n5.  Create a field: *Collection > Insert document > Type the field `text` below `_id`*\n6.  Allow access to the database: *Project > Security > Network Access > IP Access List > Add your IP address.*\n7.  Connect to the database from your terminal:\\\n    `mongo \"mongodb+srv://YourClusterName.mongodb.net/YourDatabaseName\" --username YourUsername`\n\nSecond, I wrote the [Python code for storing tweets in MongoDB](https://github.com/lorenanda/tweets-docker-pipeline/tree/main/docker-compose/tweet_collector) using the `pymongo` library.\n\n```python\nimport pymongo\n\nclient = pymongo.MongoClient(host='mongo_container', port=27018)\ndb = client.tweets_db\n\ndef warning_log(tweet):\n    logging.critical('\\n\\nTWEET: ' + tweet['username'] + 'just tweeted: ' + tweet['text'])\n    db.collections.onthisday.insert_one(tweet)\n```\n\nThe host `mongo_container` is one of the Docker containers, explained in section 5.\n\n## 3\\. Performing ETL job\nThe ETL (Extract, Transform, Load) job involves three actions: extracting tweets from MongoDB, analyzing their sentiment, and storing them into a new Postgres database. Here is the [Python code for the ETL job](https://github.com/lorenanda/tweets-docker-pipeline/tree/main/docker-compose/etl_job).\n\n### 3\\.1\\. Extracting tweets from MongoDB\nTo extract the tweet texts from MongoDB, I used again the `pymongo` library.\n\n```python\ndef extract_tweets():\n    tweets  = list(db.onthisday.find())\n    if tweets:\n        t = random.choice(tweets)\n        logging.critical(\"Random tweet: \"+t[\"text\"])\n        return t\n```\n### 3\\.2\\. Transforming tweets with sentiment scores\nTo analyze the sentiment of the tweets, I used the `VADER` library , which returns (among others) a compound sentiment score.\n\n```python\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n\ndef transform_tweets(tweet):\n\n    tweet_text = tweet['text'].replace(\"\\'\",\"\")\n\n    sia = SentimentIntensityAnalyzer()\n    tweet_sia = sia.polarity_scores(tweet_text)['compound']\n\n    return tweet_sia\n```\n### 3\\.3\\. Loading tweets into PostgreSQL\nTo load the tweets with their sentiment scores into a Postgres database, first you need––well, a Postgres database. I installed Postgres, then created a database and a table for tweets right from the terminal:\n\n1. Connect to Postgres: `psql`\n2. Create a database: `createdb twitter`\n3. Go into the created database: `psql twitter`\n4. Create columns in the database: `CREATE TABLE tweets (text varchar(280), score numeric(4,3);`\n\nThen, I wrote the Python code for inserting tweets into the `tweets` table, using the `sqlalchemy` library.\n\n```python\ndef load_tweets(tweet, sentiment):\n\n    insert_query = \"\"\"\n    INSERT INTO tweets VALUES ('{tweet[\"text\"]}', {tweet_sia});\n    \"\"\"\n\n    engine.execute(insert_query)\n    logging.critical(f'Tweet {tweet[\"text\"]} loaded into Postgres.')\n```\n## 4\\. Extracting tweets from Postgres\nAfter having a database of tweets and their sentiment score in place, I had to select and extract *some* tweets, that would be sent to Slack.\n\n```python\nquery = pg.execute(\n    \"SELECT text FROM tweets ORDER BY sentiment DESC LIMIT 1\")\nmsg = str(list(query))\noutput = f'NEW TWEET! {user} just tweeted: {msg} \\nSentiment score: {blob_score}'\n```\n\n## 5\\. Posting tweets with a Slackbot\nThe last step in the pipeline is posting tweets in a Slack channel. To do this, first I [created a Slackbot](https://slack.com/intl/en-de/help/articles/115005265703-Create-a-bot-for-your-workspace). \n\nThen, I wrote the Python code for posting tweets in a Slack channel, including the code from the previous step:\n\n```python\nimport time\nimport slack\nfrom sqlalchemy import create_engine\nimport config\n\nengine = config.PG_ENGINE\nwebhook_url = config.WEBHOOK_SLACK\n\n\nwhile True:\n    logging.critical(\"\\n\\nPositive tweet:\\n\")\n    query = pg.execute(\n        \"SELECT text FROM tweets ORDER BY sentiment DESC LIMIT 1\")\n    msg = str(list(query))\n    logging.critical(msg + \"\\n\")\n    output = f'NEW TWEET! {user} just tweeted: {msg} \\nSentiment score: {blob_score}'\n    data = {'text': output}\n    requests.post(url=webhook_url, json=data)\n\n    time.sleep(30)\n```\n\nAnd :tada: –– here's the tweet that was posted in Slack:\n\n![tweet message](blog_images/tweetbyslackbot.webp)\n\n## 6\\. Creating the Docker Compose pipeline\nThe final touch of this project is *orchestration*. The individual Python scripts for each step work when you run them manually, but the goal is to run this pipeline from beginnning to end with only one command. This is where **Docker Compose** comes in.\n\n> Compose is a tool for defining and running multi-container Docker applications. With Compose, you use a YAML file to configure your application’s services. Then, with a single command, you create and start all the services from your configuration.\n\nEach of the five previous steps (or the rectangles in my messy schema) represents a [**Docker container**](https://www.docker.com/resources/what-container), so in my `docker_compose.yml` file I had five containers (services): `tweet_container`, `postgres_container`, `mongo_container`, `etl_container`, and `slackbot_container`. \n\nFor the two database containers, I used Docker images, since they didn't depend on custom code stored in my project folders. For the other three containers, I referefenced the respective code location (`build`) and their dependencies (`depends_on`) (for example, the `tweet_collector` depends on `postgres` and `mongo`, since the tweets are stored in these databases).\n\nI also used [Docker `volumes`](https://docs.docker.com/storage/volumes/) to keep the data when the containers are stopped (data persistence).\n\n```yml\nversion: '3'\n\nservices:\n  tweet_container:\n    build: tweet_collector/\n    depends_on:\n      - postgres_container\n      - mongo_container\n    volumes:\n      - ./tweet_collector/:/app\n  \n  postgres_container:\n    build: postgresdb\n    image: postgres:13.0\n    ports:\n      - 5555:5432\n    environment:\n      - POSTGRES_USER=your_user\n      - POSTGRES_PASSWORD=your_password\n\n  mongo_container:\n    build: mongodb\n    image: mongo\n    ports:\n      - 27018:27018\n    volumes:\n      - ./mongodb:/app\n  \n  etl_container:\n    build: etl_job/\n    depends_on:\n      - postgres_container\n      - mongo_container\n    volumes:\n      - ./etl_job/:/app\n\n  slackbot_container:\n    build: slackbot/\n    depends_on:\n      - mongo_container\n      - postgres_container\n    volumes:\n      - ./slackbot/:/app\n```\n\nFinally, here some of the CLI commands I used for managing the Docker containers (you can find more in [their docs](https://docs.docker.com/engine/reference/commandline/cli/)):\n\n- `docker images` to list all the used images (postgres and mongo)\n- `docker ps -a` to list all my containers\n- `docker -v` to mount volumes\n- `docker build` to build an image from a Docker file\n- `docker run` to run the containers\n\nAnd that was it: my very first dockerized ETL pipeline –– a week's work and a few hours writing packed in a 6-minute blog post.\n\n## Friday Lightning Talk\n\nThis week I focused on the Transform part of the project, which was about Sentiment Analysis. When we learned about the VADER library, some colleagues asked how the training data is collected and who rates the training texts. To answer these questions, I decided to present one of my dearest personal projects in which I did exactly that: I created a list of emotion verbs and asked native speakers to rate their valence, arousal, and duration. You can read more about this project (and data analysis) [on GitHub](https://github.com/lorenanda/psych-verbs) and in this blog post from a student conference."
    },
    {
      "id": "/2020/11/07/bootcamp6",
      "metadata": {
        "permalink": "/blog/2020/11/07/bootcamp6",
        "source": "@site/blog/2020-11-07-bootcamp6.md",
        "title": "Creating a Metabase dashboard on Postgres and AWS",
        "description": "Project completed in week 6 (02.11.-06.11.20) of the Data Science Bootcamp at Spiced Academy in Berlin.",
        "date": "2020-11-07T00:00:00.000Z",
        "formattedDate": "November 7, 2020",
        "tags": [
          {
            "label": "data science",
            "permalink": "/blog/tags/data-science"
          },
          {
            "label": "bootcamp",
            "permalink": "/blog/tags/bootcamp"
          },
          {
            "label": "Python",
            "permalink": "/blog/tags/python"
          }
        ],
        "readingTime": 3.375,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "post",
          "title": "Creating a Metabase dashboard on Postgres and AWS",
          "gh-repo": "lorenanda/northwind-dashboard",
          "gh-badge": [
            "star",
            "fork",
            "follow"
          ],
          "tags": [
            "data science",
            "bootcamp",
            "Python"
          ],
          "comments": true
        },
        "prevItem": {
          "title": "Building a dockerized ETL pipeline for tweet streaming in Slack",
          "permalink": "/blog/2020/11/14/bootcamp7"
        },
        "nextItem": {
          "title": "Forecasting the weather in Berlin with ARIMA models",
          "permalink": "/blog/2020/10/31/bootcamp5"
        }
      },
      "content": ">Project completed in week 6 (02.11.-06.11.20) of the Data Science Bootcamp at Spiced Academy in Berlin.\n\nThis was another week packed with new information and experiences! In only five days, I managed to set up a Postgres database, deploy it on AWS RDS, connect it to Metabase to create a dashboard and in turn connect it to AWS EC2 to make the dashboard run continuously.\n\n### Getting the data\n\nThe Northwind database is a sample database created by Microsoft for tutorials and testing purposes. It contains information about the business of \"Northwind Trades\", a fictional company that sells various food products. Northwind includes information about orders, inventory, purchasing, suppliers, shipping, and employees. For my project, to make things a bit more complicated, I got the data in the form of several .csv files [from here](https://github.com/pawlodkowski/northwind_data_clean). I also got a .csv file with the country names and their ISO_A2 codes from here, in order to plot country data on a world map (I only realized this when creating the dashboard, so now you can spare yourself the frustration).\n\n### Setting up a Postgres database\n\nI installed [Postgres](https://www.postgresql.org/download/) (and pgAdmin, to query easier than in the command line), set up a server, and created my local Northwind database.\n\nFirst, I created tables for each of the files: each table was named like the .csv file it was designed for (e.g. *order_details.csv* file -> *order_details* table). In each table, I added columns that designated the columns in the respective .csv file. Here it's important to note that the column names in the .csv files are written in camel case (e.g. *orderID*, *shipCountry*) and in PostgreSQL unquoted names are case-insensitive, but quoted names are case-sensitive. This means that you have to either specify the column names in quotes or rename them in lowercase only, in order for the data to be imported; I chose the latter option and renamed the columns (e.g. *order_id*, *ship_country*).\n\nNext, I imported the .csv files into the newly created tables and set **primary keys** (the main key, like the index, of a table) and **foreign keys** (point to another table to connect) for the tables, in order to be able to query across them.\n\n### Hosting the project in the cloud\n\nAfter the database was all set up, it was time for it to be deployed in the cloud. I used Amazon Web Services (AWS), more specifically two of their services: [RDS](https://aws.amazon.com/rds/) and [EC2](https://aws.amazon.com/ec2/?ec2-whats-new.sort-by=item.additionalFields.postDateTime&ec2-whats-new.sort-order=desc).\n\nFirst, I deployed my local database to RDS, which basically means that I sent my database to run on a bigger, faster, stronger computer in Frankfurt. In RDS, you can select from multiple locations around the world, and though this doesn't alter the way your database is run, you might want to choose a location closest to your actual physical location, so that it runs faster, and also consider the data privacy measures in different countries.\n\nNext, I set up an EC2 instance, which basically means that I rented a virtual machine on which to run my dashboard continuously.\n\n### Creating a dashboard\n\nNow to the creative part! I connected [Metabase](https://www.metabase.com/) to my remote database and EC2 instance and started creating a dashboard for KPIs of Northwind Trades. I focused only on three areas from my data and split it into two parts:\n\n-   The **Sales** part included data from the Orders and Products tables. I visualized a gauge with the total number of orders, a trend count of the total order value compared to the previous year, a line plot of the number of orders per day, a bar chart of the best-selling products, and two choropeth maps of the number of orders in the world and in the US states.\n-   The **Team** part included data from the Employees, Customers, and Orders tables. I visualized a table with the name of the employee who supported most customers, one with the employee who filled most orders, and one with the names, roles, birthdays, and seniority in the company of all employees.\n\nFriday Lightning Talk\n---------------------\n\nA the end of this week, I presented my dashboard and talked about the challenges I've encountered while creating it, as well as solutions and possible improvements.\n\n![cover](https://lorenaciutacu.files.wordpress.com/2020/11/dashboard_demo.gif?w=1400)"
    },
    {
      "id": "/2020/10/31/bootcamp5",
      "metadata": {
        "permalink": "/blog/2020/10/31/bootcamp5",
        "source": "@site/blog/2020-10-31-bootcamp5.md",
        "title": "Forecasting the weather in Berlin with ARIMA models",
        "description": "Project completed in week 5 (26.10.-30.10.20) of the Data Science Bootcamp at Spiced Academy in Berlin.",
        "date": "2020-10-31T00:00:00.000Z",
        "formattedDate": "October 31, 2020",
        "tags": [
          {
            "label": "data science",
            "permalink": "/blog/tags/data-science"
          },
          {
            "label": "bootcamp",
            "permalink": "/blog/tags/bootcamp"
          },
          {
            "label": "tutorials",
            "permalink": "/blog/tags/tutorials"
          },
          {
            "label": "Python",
            "permalink": "/blog/tags/python"
          }
        ],
        "readingTime": 3.485,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "post",
          "title": "Forecasting the weather in Berlin with ARIMA models",
          "gh-repo": "lorenanda/weather-forecast",
          "gh-badge": [
            "star",
            "fork",
            "follow"
          ],
          "tags": [
            "data science",
            "bootcamp",
            "tutorials",
            "Python"
          ],
          "comments": true
        },
        "prevItem": {
          "title": "Creating a Metabase dashboard on Postgres and AWS",
          "permalink": "/blog/2020/11/07/bootcamp6"
        },
        "nextItem": {
          "title": "Building a lyrics scraper and classifier in Python",
          "permalink": "/blog/2020/10/24/bootcamp4"
        }
      },
      "content": ">Project completed in week 5 (26.10.-30.10.20) of the Data Science Bootcamp at Spiced Academy in Berlin.\n\nI found this week's project quite challenging, because I haven't worked with time series and forecasting before. But this means I had a lot of new things to learn!\n\n### Map it out nicely\n\nI started this week's challenge with the easy part: creating a choropeth map with the [Folium library](https://python-visualization.github.io/folium/). The only trick was to find a JSON file with Berlin districts coordinates, then I managed to plot the average recorded temperature in Berlin Mitte.\n\n[![](https://lorenaciutacu.files.wordpress.com/2020/10/berlin_mitte_temp.jpg?w=1024)](https://lorenaciutacu.files.wordpress.com/2020/10/berlin_mitte_temp.jpg)\n\n### It's getting hot in here!\n\nWhile further exploring the data, I found two interesting facts:\n\n-   Since 1867, the hottest day in Berlin Mitte was July 11th 1984, with 30.8°C, and the coldest day was February 10th 1929, with -22.6°C.\n-   There were missing values for 145 consecutive days, between April 25th and November 25th 1945. In the context of [the Battle of Berlin](https://en.wikipedia.org/wiki/Battle_of_Berlin), I guess there were bigger issues to take care of than recording the daily temperature. I filled in these missing values with the average temperature recorded in the same time frame in the previous year.\n\n[![](https://lorenaciutacu.files.wordpress.com/2020/11/lineplot_allyears.png?w=990)](https://lorenaciutacu.files.wordpress.com/2020/11/lineplot_allyears.png)\n\nIn any case, you can see a clear positive trend in the average temperature, proving that global warming is real.\n\n[![](https://lorenaciutacu.files.wordpress.com/2020/11/horizontal_heatlines_berlin.png?w=1024)](https://lorenaciutacu.files.wordpress.com/2020/11/horizontal_heatlines_berlin.png)\n\nAverage daily temperature in Berlin-Mitte between 1867-2020, going from cold (blue) to hot (red)\n\n### I've seen the future (with a large estimation error)\n\nThe most common method for analyzing time series and making forecasts are **AutoRegressive Integrated Moving Average (ARIMA)** models. The name is quite self-explanatory, but I'll break it down to make it more clear:\n\n-   **Auto-Regressive** (AR): the future value depends only on the previous values.\n-   **Integrated** (I): the data is stationary, which means it has:\n    -   a constant mean (no trend)\n    -   a constant standard deviation\n    -   no seasonality\n-   **Moving Average** (MA): the future value depends only on the lagged forecast errors.\n\nEach of these three parts takes one parameter. Forr the implementation in Python it's important to know how to find the optimal value for each of them:\n\n-   **p (AR)**: the number of lags of the predicted value (y) to be used a predictors. Check with the [Partial Autocorrelation plot (PACF)](https://www.statsmodels.org/dev/generated/statsmodels.graphics.tsaplots.plot_pacf.html?highlight=pacf#statsmodels.graphics.tsaplots.plot_pacf).\n-   **d (I):** the number of differencing required to make the time series stationary. This is 0 if the series is already stationary, otherwise check with the [Augmented Dickey-Fuller Test](https://www.statsmodels.org/dev/generated/statsmodels.tsa.stattools.adfuller.html) (ADF).\n-   **q (MA):** the number of lagged forecast errors that should go into the model. Check with the [Autocorrelation plot (ACF)](https://www.statsmodels.org/dev/generated/statsmodels.graphics.tsaplots.plot_acf.html?highlight=plot_acf#statsmodels.graphics.tsaplots.plot_acf).\n\nOr you can spare yourself all this work and implement an [auto-ARIMA](https://alkaline-ml.com/pmdarima/index.html), which finds the best hyperparameters for you. (But, as with [TPOT, the library used in my previous project](/bootcamp3), my advice is to be cautious with auto-ML and instead have fun learning how to tune the hyperparameters yourself.)\n\nI tried both the manual and auto-ARIMA models on my data and eventually got a **MAPE score of 0.22** (2.2%), which means that the model is 97.8% accurate in predicting the next 365 observations, and a **MAE score of 6.2**, which means that the model predicts the temperatures with an error of 6.2°C. (Fun fact: at first I got an infinite MAPE score -- I learned that's because of negative values, so I had to convert the temperature from Celsius to Kelvin.)\n\nFriday Lightning Talk\n---------------------\n\nAfter I had put in so many hours in my ARIMA models, of course I wanted to talk about that part of my project. But about one hour before the presentations, I discovered **Facebook's **[**Prophet**](https://facebook.github.io/prophet/) -- \"a procedure for forecasting time series data based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects. It works best with time series that have strong seasonal effects and several seasons of historical data. Prophet is robust to missing data and shifts in the trend, and typically handles outliers well.\" This sounded impressive, so I tried it out on my dataset and was surprised at how easy, quick, and well it performed -- and how nice the plots looked with [Plotly](https://plotly.com/python/plotly-express/)! So I ended up talking about this magic library that saves you up so much of the preprocessing work and yields at least equally good results as a strong ARIMA model.\n\n[![](https://lorenaciutacu.files.wordpress.com/2020/11/screenshot_2020-11-01-screenshot.png?w=1024)](https://lorenaciutacu.files.wordpress.com/2020/11/screenshot_2020-11-01-screenshot.png)"
    },
    {
      "id": "/2020/10/24/bootcamp4",
      "metadata": {
        "permalink": "/blog/2020/10/24/bootcamp4",
        "source": "@site/blog/2020-10-24-bootcamp4.md",
        "title": "Building a lyrics scraper and classifier in Python",
        "description": "Project completed in week 4 (19.10.-23.10.20) of the Data Science Bootcamp at Spiced Academy in Berlin.",
        "date": "2020-10-24T00:00:00.000Z",
        "formattedDate": "October 24, 2020",
        "tags": [
          {
            "label": "data science",
            "permalink": "/blog/tags/data-science"
          },
          {
            "label": "bootcamp",
            "permalink": "/blog/tags/bootcamp"
          },
          {
            "label": "tutorials",
            "permalink": "/blog/tags/tutorials"
          },
          {
            "label": "Python",
            "permalink": "/blog/tags/python"
          },
          {
            "label": "NLP",
            "permalink": "/blog/tags/nlp"
          }
        ],
        "readingTime": 3.13,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "post",
          "title": "Building a lyrics scraper and classifier in Python",
          "gh-repo": "lorenanda/lyrics-classification",
          "gh-badge": [
            "star",
            "fork",
            "follow"
          ],
          "tags": [
            "data science",
            "bootcamp",
            "tutorials",
            "Python",
            "NLP"
          ],
          "comments": true
        },
        "prevItem": {
          "title": "Forecasting the weather in Berlin with ARIMA models",
          "permalink": "/blog/2020/10/31/bootcamp5"
        },
        "nextItem": {
          "title": "Predicting bike share demand with linear regression",
          "permalink": "/blog/2020/10/17/bootcamp3"
        }
      },
      "content": ">Project completed in week 4 (19.10.-23.10.20) of the Data Science Bootcamp at Spiced Academy in Berlin.\n\nI was super excited about this week, because it was about language models and first steps into NLP, my favorite ML topic! The challenge was to create a Python program that scrapes lyrics from a website, preprocesses them, and predicts the artist from the text.\n\n### Web Scraping\n\nThe first step in this project was to build a web-scraper in Python with BeautifulSoup. I found this to be the most difficult and frustrating part, from finding a good lyrics website that doesn't contain tens of duplicate lyrics, to implementing BeautifulSoup, and transforming the code from a JupyterNotebook into a Python file.\n\n### Text Processing\n\nNext, I had to clean and preprocess the scraped lyrics, in order to feed them to the classification model. I used TextHero and SpaCy to tokenize the text, make the words lowercase, remove punctuation, numbers, and stop words (i.e. filling words or words that don't change the meaning of a sentence, like *the, a, to*). This step was quite easy, mainly because the lyrics were in English. But remember:\n\n> NLP != English NLP\n\nAnother important part of text processing was to check for class imbalance, i.e. if and artist had far more observations than another. Class imbalance can skew the prediction in the way that the model would predict the majority. There are four main ways to deal with class imbalance:\n\n-   **over-sampling**: collect more samples for the minority class.\n-   **under-sampling**: reduce the number of samples in the majority class.\n-   **SMOTE**: generate new samples by interpolation.\n-   **weighting**: assign higher weights to observation from the minority class during training.\n\nIn my case, I had more lyrics by Metallica than by Iron Maiden. I went with under-sampling and, after dropping the duplicates, I randomly removed 15 more songs by Metallica. This way, I ended up with 86 songs by each band.\n\n### Naive Bayes Classifier\n\nNow that that the dataset was made of clean lyrics, it was ready to train a classification model. I used the [Multinomial Naive Bayes Classifier](https://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes) (MNBC), a probabilistic model based on Bayes' Theorem, which assumes strong independence between the features and uses a multinomial distribution for each of the features. MNBC is typically used in text classification for calculating the probability of a word occurring in a text. For this, the words are [vectorized](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer) or [transformed](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html) into numbers. My model achieved an accuracy of **96.7% on the train set **and **68.4% on the test set**. This means that the model overfit.\n\n### Text analysis of Metallica and Iron Maiden lyrics\n\nJust for fun, I also used NLTK, SpaCy, and VADER to explore the lyrics. Specifically, I looks for similarities and differences between the two bands. It turned out that *Iron Maiden* have a richer vocabulary and use more long words than *Metallica*. The latter are slightly more negative, but overall the similarity score of the two bands was 0.99.\n\n| **Feature** | **Metallica** | **Iron Maiden** |\n| lexical richness | 0.05 | 0.08 |\n| long words | 15 | 19 |\n| polarity | -0.06 | 0.08 |\n| subjectivity | 0.50 | 0.51 |\n| negativity | 0.24 | 0.18 |\n| neutrality | 0.63 | 0.69 |\n| positivity | 0.13 | 0.13 |\n| compound sentiment | -1 | -1 |\n\nThe most fun part was identifying named entities in text with SpaCy. Though SpaCy recognized dates and time adverbs pretty well, it mistagged some words in hilarious/creepy ways.\n\n## Friday Lightning Talk\n\nAt the end of this week, I presented my text analysis of lyrics, to give an overview of NLTK and SpaCy functions for linguistic analysis. As usual, I also got inspiration from my colleagues' projects on how to improve the command line interface for my webscraper/artist predictor!\n\n[![](https://lorenaciutacu.files.wordpress.com/2020/10/ironmaiden_wordcould.png?w=369)](https://lorenaciutacu.files.wordpress.com/2020/10/ironmaiden_wordcould.png)"
    },
    {
      "id": "/2020/10/17/bootcamp3",
      "metadata": {
        "permalink": "/blog/2020/10/17/bootcamp3",
        "source": "@site/blog/2020-10-17-bootcamp3.md",
        "title": "Predicting bike share demand with linear regression",
        "description": "Project completed in week 3 (12.10.-16.10.20) of the Data Science Bootcamp at Spiced Academy in Berlin.",
        "date": "2020-10-17T00:00:00.000Z",
        "formattedDate": "October 17, 2020",
        "tags": [
          {
            "label": "data science",
            "permalink": "/blog/tags/data-science"
          },
          {
            "label": "bootcamp",
            "permalink": "/blog/tags/bootcamp"
          },
          {
            "label": "tutorials",
            "permalink": "/blog/tags/tutorials"
          },
          {
            "label": "Python",
            "permalink": "/blog/tags/python"
          }
        ],
        "readingTime": 2.72,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "post",
          "title": "Predicting bike share demand with linear regression",
          "gh-repo": "lorenanda/bike-demand-prediction",
          "gh-badge": [
            "star",
            "fork",
            "follow"
          ],
          "tags": [
            "data science",
            "bootcamp",
            "tutorials",
            "Python"
          ],
          "comments": true
        },
        "prevItem": {
          "title": "Building a lyrics scraper and classifier in Python",
          "permalink": "/blog/2020/10/24/bootcamp4"
        },
        "nextItem": {
          "title": "Predicting Titanic survivors with decision tree classifiers",
          "permalink": "/blog/2020/10/10/bootcamp2"
        }
      },
      "content": ">Project completed in week 3 (12.10.-16.10.20) of the Data Science Bootcamp at Spiced Academy in Berlin.\n\nI really enjoyed this week's project for two main reasons: it involved time series analysis and it solved a real-world practical business problem.\n\n### Exploratory analysis\n\nThe dataset includes 12 variables, of which 3 are dependent, and 1 is actually the one I had to predict: bike count. I started by extracting the month, day, weekday, and hour from the datetime feature, so I ended up with 15 variables to help me predict the bike count.\n\nIt's a common assumption that the more variables you have in a dataset and include in a model, the better the predictions. But usually (as in this case) it's exactly the contrary. The more variables you have, the higher the chance that some of them will be correlated with each other (**multicollinearity**) or just be irrelevant. Also, some variables that seem useless independently might be useful combined. Selecting what features to include in the model can be quite tricky, but a look at the correlation matrix or the `sklearn.feature_selection` module can help!\n\nIn the bike dataset, I noticed a high correlation between temperature, season, windspeed, season, and month, which was not surprising. I used this information, as well as the [Recursive Feature Elimination](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html) (RFE), to select the most relevant variables for the bike count prediction. In the end, I included only five features: *temperature, hour, weather, weekday, *and* holiday*.\n\n### Feature engineering\n\nAfter selecting the features, I had to do a bit of feature engineering to prepare them for Linear Regression. I tried both with scaling and [polynomial expansion](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html?highlight=polynomial#sklearn.preprocessing.PolynomialFeatures). For the Random Forest models, this was not necessary, since the models do feature engineering themselves. The feature engineering part was very important also for fitting the evaluation score used in the Kaggle competition, which in this case was **[Root Mean Squared Log Error (RMSLE)](https://www.kaggle.com/carlolepelaars/understanding-the-metric-rmsle)**. In order to optimize my models against the RMSLE, I took the logarithm of the target variable(*bike count*).\n\n### Regression models\n\nOnce the dirty work was done, I trained and tested four different regression models:\n\n-   **RFR1**: Random Forest Regressor with default hyperparameters\n-   **RFR2**: Random Forest Regressor with the best hyperparameters found with GridSearchCV\n-   **RFR3**: Random Forest Regressor with hyperparamenters recommended by [TPOT](http://epistasislab.github.io/tpot/)\n-   **GBR**: Gradient Boost Regressor\n-   **SVR**: Support Vector Regressor\n-   **LR**: Linear Regression\n\n|  | RFR1 | RFR2 | RFR3 | GBR | SVR | LR |\n| RMSLE | 0.49 | 0.47 | 0.48 | 0.10 | 0.20 | 0.24 |\n| kaggle | -- | 0.49 | -- | -- | -- | -- |\n\nTo get an idea, the best kaggle score on this competition is 0.33, so I think my 0.49 is not that bad. But of course, as always, there's room for improvement and I might try to do just that over the weekend.\n\nFriday Lightning Talk\n---------------------\n\nAt our weekly project review, I talked about the Random Forest models that I implemented, explained the choice of hyperparameters and the different results I got. As usual, listening to other people's talks gave me more ideas on how to improve my models or other things to try out! That's the beauty and art of Machine Learning -- there are many ways to look at data, combine information, and solve a problem!"
    },
    {
      "id": "/2020/10/10/bootcamp2",
      "metadata": {
        "permalink": "/blog/2020/10/10/bootcamp2",
        "source": "@site/blog/2020-10-10-bootcamp2.md",
        "title": "Predicting Titanic survivors with decision tree classifiers",
        "description": "Project completed in week 2 (05.10.-09.10.20) of the Data Science Bootcamp at Spiced Academy in Berlin.",
        "date": "2020-10-10T00:00:00.000Z",
        "formattedDate": "October 10, 2020",
        "tags": [
          {
            "label": "data science",
            "permalink": "/blog/tags/data-science"
          },
          {
            "label": "bootcamp",
            "permalink": "/blog/tags/bootcamp"
          },
          {
            "label": "tutorials",
            "permalink": "/blog/tags/tutorials"
          },
          {
            "label": "Python",
            "permalink": "/blog/tags/python"
          }
        ],
        "readingTime": 3.06,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "post",
          "title": "Predicting Titanic survivors with decision tree classifiers",
          "gh-repo": "lorenanda/titanic",
          "gh-badge": [
            "star",
            "fork",
            "follow"
          ],
          "tags": [
            "data science",
            "bootcamp",
            "tutorials",
            "Python"
          ],
          "comments": true
        },
        "prevItem": {
          "title": "Predicting bike share demand with linear regression",
          "permalink": "/blog/2020/10/17/bootcamp3"
        },
        "nextItem": {
          "title": "Creating animated scatterplots with Python",
          "permalink": "/blog/2020/10/03/bootcamp1"
        }
      },
      "content": ">Project completed in week 2 (05.10.-09.10.20) of the Data Science Bootcamp at Spiced Academy in Berlin.\n\nOn the second week of the bootcamp, we started with Machine Learning (ML). If you think about it, ML surrounds us in everyday life: Netflix recommending you movies you might like, your smartphone camera detecting faces, self-driving cars recognizing passengers on the street, bank detecting credit card fraud -- these are all applications of ML. They can be split into three main ML categories:\n\n-   **Classification**: Logistic Regression, Decision Trees, Random Forest\n-   **Regression**: Linear Regression, Regression Trees, SVR, Forecasting\n-   **Unsupervised**: PCA, Clustering, t-SNE, Matrix factorization\n\nThis week we focused only on classification and applied logistic regression, decision tree, and random forest models on the Titanic dataset to predict passenger survival.\n\n-   **Logistic Regression** separates the data points into two classes by a curve. It's a fast and performant model, but the data might require prior extensive feature engineering (e.g. encoding categorical values numerically). Logistic Regression is rooted in statistics and dates back to 1958.\n-   **Decision Trees** classify the data by \"asking questions\" about different features in the dataset, just like in a guessing game. You can specify the depth of the tree (on how many levels it should go), depending on the desired complexity.\n-   **Random Forest** is basically an ensemble of decision trees. This model doesn't require feature engineering, but tweaking the hyperparameters. The downside of Decision Trees, and therefore of Random Forest, is that they are prone to overfitting. Decision Trees/Random Forest are rather rooted in computer science and date back to 1986/1995.\n\n> 'Logistic regression' is a terrible term. It should be called 'binary classification with a sigmoid function'.\n>\n> *ONE OF OUR TEACHERS*\n\nNow, a few words about why** Feature Engineering** is so important (and annoying). An ML model like Logistic Regression doesn't understand words (e.g. *female, good, orange*), so we need to transform them into numbers. But depending on the data type, we use different Encoders:\n\n-   for **ordinal **data (if one value is better than another, e.g.: *good-great-excellent*, *child-teenager-adult-senior*): [KBinsDiscretizer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.KBinsDiscretizer.html), [LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html#)\n-   for **nominal** data (if the values are equal or don't have an inherent value, e.g.: *male-female*, *Berlin-Vienna-Bucharest*): [OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)\n\nIn the Titanic dataset, there were several columns that needed to be transformed in different ways, while leaving the others untouched. The best way to do this is with a [ColumnTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html), which applies all individual transformations in one step.\n\n### Evaluating Classifiers\n\nAfter hours of wrangling the data and encoding it properly (e.g. extracting the honorifics from the passenger names), I applied the three models on the Titanic dataset and achieved the following accuracyscores:\n\n|  | **Logistic Regression** | **Decision Tree** | **Random Forest** |\n| *train set* | 79.1 % | 80.3 % | 94.7 % |\n| *test set* | 81.6 % | 76.5 % | 81.0 % |\n\nIt's interesting to note that the Logistic Regression and Random Forest achieved similar scores on the test set (81%), but Logistic Regression performed worse, whereas Random Forest performed better on the train set. This means that Random Forest overfit (it learned too much detail from the train set and couldn't generalized on the test set), whereas Logistic Regression learned less, but could apply that knowledge better on the test set. Another important point is that in Logistic Regression feature engineering is decisive, whereas Random Forest is more of a black box.\n\nI'd say my scores are not that bad, but of course there's room for improvement, by adding/removing/encoding other features or increasing the depth of the tree, for example.\n\nFriday Lightning Talk\n---------------------\n\nAs last Friday, we had to give a talk on the weekly project or a topic related to it and I presented my Decision Tree and Random Forest Classifiers.\n\n[![](https://lorenaciutacu.files.wordpress.com/2020/10/decision_tree.png?w=1024)](https://lorenaciutacu.files.wordpress.com/2020/10/decision_tree.png)"
    },
    {
      "id": "/2020/10/03/bootcamp1",
      "metadata": {
        "permalink": "/blog/2020/10/03/bootcamp1",
        "source": "@site/blog/2020-10-03-bootcamp1.md",
        "title": "Creating animated scatterplots with Python",
        "description": "Project completed in week 1 (28.09.-02.10.20) of the Data Science Bootcamp at Spiced Academy in Berlin.",
        "date": "2020-10-03T00:00:00.000Z",
        "formattedDate": "October 3, 2020",
        "tags": [
          {
            "label": "data science",
            "permalink": "/blog/tags/data-science"
          },
          {
            "label": "bootcamp",
            "permalink": "/blog/tags/bootcamp"
          },
          {
            "label": "tutorials",
            "permalink": "/blog/tags/tutorials"
          },
          {
            "label": "Python",
            "permalink": "/blog/tags/python"
          }
        ],
        "readingTime": 2.29,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "post",
          "title": "Creating animated scatterplots with Python",
          "share-description": "Learn how animate seaborn/matplotlib scatterplots with imageio.",
          "gh-repo": "lorenanda/animated-scatterplot",
          "gh-badge": [
            "star",
            "fork",
            "follow"
          ],
          "tags": [
            "data science",
            "bootcamp",
            "tutorials",
            "Python"
          ],
          "readtime": true
        },
        "prevItem": {
          "title": "Predicting Titanic survivors with decision tree classifiers",
          "permalink": "/blog/2020/10/10/bootcamp2"
        },
        "nextItem": {
          "title": "5 eye-opening books about AI ethics",
          "permalink": "/blog/2020/08/19/books-ethics-ai"
        }
      },
      "content": ">Project completed in week 1 (28.09.-02.10.20) of the Data Science Bootcamp at Spiced Academy in Berlin.\n\nOur first bootcamp project was creating an animated scatterplot, using the libraries `matplotlib` or `seaborn` and `imageio`. The scatterplot illustrates the relationship between life expectancy and fertility rate of world's countries from 1960 to 2015, based on the [Gapminder data set](https://www.gapminder.org/tag/download-data/):\n\n||country|year|population|life_expectancy|fertility_rate|continent|\n|---|---|---|---|---|---|---|\n|0|Afghanistan|1800|3280000.0|28.21|7.0|Asia|\n\nThe animated scatterplot is basically made of several overlapping static plots. The animation consists of **four steps**:\n\n**1\\. Create static scatterplots for each year in the data set.**\n\nThescatterplots depict `life_expectancy` on the x axis and `fertility_rate` on the y rate. To make the plots even more insightful, the size of the points illustrates the `population` number and the color of the points illustrates the `continent`.\n\n```python\nimport matplotlib.pyplot as plt \n%matplotlib inline\nimport seaborn as sns\n\nsns.scatterplot(x='life_expectancy',\n                y='fertility_rate',\n                hue='continent',\n                size='population',\n                sizes=(10, 1000),\n                legend=False,\n                data=gapminder_df.loc[gapminder_df['year']==year],\n                alpha=0.7,\n                palette='Set2')\n    \nplt.title(f'{year}', loc='center', fontsize=20, color='black', fontweight='bold')\n\nplt.xlabel('Life expectancy')\nplt.ylabel('Fertility rate')\n```\n\n**2\\. Export the scatterplot images to a designated folder.**\n\n\n```python\nimport imageio\nimport os\n\nimages = []\nfolder='/path/to/folder/images'\n\nif not os.path.exists(folder):\n    os.mkdir(folder)\n```\n\n**3\\. Join the individual images in chronological order.**\n\n```python\nfilename = f'lifeexp_{year}.png'\nplt.savefig(os.path.join(folder,filename))\nimages.append(imageio.imread(os.path.join(folder,filename)))\n```\n\n**4\\. Export the scatterplots sequence as a gif.**\\\nThe `fps` (frames per second) parameter sets the speed of the animation.\n\n```python\nimageio.mimsave(os.path.join(folder,'scatterplot.gif'), images, fps=20)\n```\n\nNow, putting everything together, here's the full code and the animated scatterplot:\n\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt \n%matplotlib inline\nimport seaborn as sns\nimport imageio\nimport os\n\nimages = []\nfolder='/home/lorena/Documents/bootcamp/W1/images'\n\nif not os.path.exists(folder):\n    os.mkdir(folder)\n\nfor year in range(1960, 2016):\n    \n    plt.axis((0, 100, 0, 10))\n    \n    sns.scatterplot(x='life_expectancy',\n                    y='fertility_rate',\n                    hue='continent',\n                    size='population',\n                    sizes=(10, 1000),\n                    legend=False,\n                    data=gapminder_df.loc[gapminder_df['year']==year],\n                    alpha=0.7,\n                    palette='Set2')\n    \n    plt.title(f'{year}', loc='center', fontsize=20, color='black', fontweight='bold')\n    #plt.title(f'inspired by Hans Rosling', loc='right', fontsize=10, color='grey', style='italic', pad=-20)\n\n    #plt.legend(bbox_to_anchor=(0.74, 0.85), loc='center')\n\n    plt.xlabel('Life expectancy')\n    plt.ylabel('Fertility rate')\n    #plt.annotate({country}, )\n    \n    filename = f'lifeexp_{year}.png'\n    \n    plt.savefig(os.path.join(folder,filename))\n    \n    images.append(imageio.imread(os.path.join(folder,filename)))\n    \n    plt.figure()\n    \nimageio.mimsave(os.path.join(folder,'scatterplot.gif'), images, fps=20)\n```\n\n![animated scatterplot](https://github.com/lorenanda/animated-scatterplot/raw/main/scatterplot.gif)\n\n### Friday Lightning Talk\n\nEach week, we get a main dataset and several tasks to apply the concepts learned throughout the week. On Fridays, we present in 5 minutes a particular finding from our weekly challenge project, a chart, new library, (un)solved bugs, or anything that is worth sharing and helpful for others. This Friday, I chose to talk about five new **bash commands** for checking the installed Python libraries, their versions and dependencies.\n\n| function|command|\n|---|---|\n| to list all installed libraries | `pip list` |\n| to list only outdated libraries | `pip list -o` (or `--outdated`) |\n| to list only the latest / up to date libraries | `pip list -u` (or `--uptodate`) |\n| to show all information about a library | `pip show <package-name>` |\n| to list all libraries installed in a specific environment | `conda list -n <environment-name>` |"
    },
    {
      "id": "/2020/08/19/books-ethics-ai",
      "metadata": {
        "permalink": "/blog/2020/08/19/books-ethics-ai",
        "source": "@site/blog/2020-08-19-books-ethics-ai.md",
        "title": "5 eye-opening books about AI ethics",
        "description": "Artificial Intelligence is undoubtedly an exciting field. We are making machines think like humans, mimic our actions, and solve problems more efficiently than us, at this at an unprecedented level and speed. But beyond the hype, I can’t help but wonder",
        "date": "2020-08-19T00:00:00.000Z",
        "formattedDate": "August 19, 2020",
        "tags": [
          {
            "label": "books",
            "permalink": "/blog/tags/books"
          },
          {
            "label": "AI",
            "permalink": "/blog/tags/ai"
          }
        ],
        "readingTime": 1.725,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "post",
          "title": "5 eye-opening books about AI ethics",
          "tags": [
            "books",
            "AI"
          ],
          "comments": true
        },
        "prevItem": {
          "title": "Creating animated scatterplots with Python",
          "permalink": "/blog/2020/10/03/bootcamp1"
        },
        "nextItem": {
          "title": "Women in Data Symposium - Skills to Last a Decade",
          "permalink": "/blog/2020/07/31/womenindata-symposium"
        }
      },
      "content": "Artificial Intelligence is undoubtedly an exciting field. We are making machines think like humans, mimic our actions, and solve problems more efficiently than us, at this at an unprecedented level and speed. But beyond the hype, I can’t help but wonder: does more efficiently mean better? Are some of our actions worthy of being mimicked? And should some of our ways of thinking about the world even be reinforced? If, like me, you’re concerned about these issues, here are five books to educate yourself on:\n\n<figure>\n    <img src='https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1499698329l/34964830.jpg' alt='missing' width=\"200\" height=\"300\"/>\n    <figcaption><strong><i>Automating Inequality</i> by Virginia Eubanks</strong> is a thorough investigation of data-based discrimination in healthcare, housing, and child welfare services in the USA. Through first-hand accounts and emotional personal stories, she reveals how technology affects human rights and works to the disadvantage of the poor and working-class people.</figcaption>\n</figure>\n\n<figure>\n    <img src='https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1456091964l/28186015.jpg' alt='missing' width=\"200\" height=\"300\"/>\n    <figcaption><strong><i>Weapons of Math Destruction</i> by Cathy O'Neill</strong> explains what it means to live in the age of the algorithm, where mathematical models trained on Big Data have the power to decide our future in an instant, reinforce discrimination, and even undermine democracy.</figcaption>\n</figure>\n \n<figure>\n    <img src='https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1492944248l/34762552.jpg' alt='missing' width=\"200\" height=\"300\"/>\n    <figcaption><strong><i>Algorithms of Oppression</i> by Safiya Umoja Noble</strong> exposes how negative biases against women of color are embedded in search engine results and algorithms, through an analysis of text, media searches, and paid online ads.</figcaption>\n</figure>\n\n<figure>\n    <img src='https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1519083351l/38212110._SY475_.jpg' alt='missing' width=\"200\" height=\"300\"/>\n    <figcaption><strong><i>Technically Wrong</i> by Sara Wachter-Boetcher</strong> demystifies the tech industry, unraveling underlying sexism, bias, and threats in popular health, dating, and shopping apps.</figcaption>\n</figure>\n\n<figure>\n    <img src='https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1576795927l/42527493._SY475_.jpg' alt='missing' width=\"200\" height=\"300\"/>\n    <figcaption><strong><i>Race After Technology</i> by Ruha Benjamin</strong> illuminates how hyped technologies can advance white supremacy and social injustice, thus reinforcing racial discrimination.</figcaption>\n</figure>\n\n\nWith great power comes great responsibility. AI has been entrusted with tremendous control in many areas of our everyday life, but the responsibility for its actions has been left out of discussion, or debated at most. These five books prove the urgency of having a serious global discussion and taking measures about the ethics and responsibility for AI uses!"
    },
    {
      "id": "/2020/07/31/womenindata-symposium",
      "metadata": {
        "permalink": "/blog/2020/07/31/womenindata-symposium",
        "source": "@site/blog/2020-07-31-womenindata-symposium.md",
        "title": "Women in Data Symposium - Skills to Last a Decade",
        "description": "Women in Data is an international non-profit organization dedicated to increasing diversity in data-related careers by organizing conferences, networking events, and mentorship programs around the world. The Berlin Chapter of WiD was just founded in May this year and I was very excited to join as a Marketing Coordinator.",
        "date": "2020-07-31T00:00:00.000Z",
        "formattedDate": "July 31, 2020",
        "tags": [
          {
            "label": "data science",
            "permalink": "/blog/tags/data-science"
          },
          {
            "label": "conferences",
            "permalink": "/blog/tags/conferences"
          }
        ],
        "readingTime": 0.685,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "post",
          "title": "Women in Data Symposium - Skills to Last a Decade",
          "tags": [
            "data science",
            "conferences"
          ],
          "comments": true
        },
        "prevItem": {
          "title": "5 eye-opening books about AI ethics",
          "permalink": "/blog/2020/08/19/books-ethics-ai"
        },
        "nextItem": {
          "title": "4 tips for creating a great data science portfolio",
          "permalink": "/blog/2020/07/29/tips-for-datascience-portfolio"
        }
      },
      "content": "[Women in Data](https://www.womenindata.org/) is an international non-profit organization dedicated to increasing diversity in data-related careers by organizing conferences, networking events, and mentorship programs around the world. The **Berlin Chapter of WiD** was just founded in May this year and I was very excited to join as a Marketing Coordinator.\n\nThis week, **27-31 July 2020**, we hosted our first event: the **online symposium \"Skills To Last A Decade\"**. Sixteen incredible women working in tech and data roles, mainly in Germany, shared their knowledge and advice in 14 talks on different topics, from data journalism to Artifical Intelligence and career skills.\n\nI had the chance to moderate two insightful talks: **Diana Nanova'**s [\"Celebrating a decade of data with BigQuery Machine Learning\"](http://youtube.com/) and **Kamila Hankiewicz's** [\"How Do Machines Learn To Understand Humans?](http://youtube.com/)\"\n\nYou can watch all the recorded talks **here**. If you want to stay updated about future events of WiD Berlin, follow me on [Instagram](https://www.instagram.com/datalingo/)."
    },
    {
      "id": "/2020/07/29/tips-for-datascience-portfolio",
      "metadata": {
        "permalink": "/blog/2020/07/29/tips-for-datascience-portfolio",
        "source": "@site/blog/2020-07-29-tips-for-datascience-portfolio.md",
        "title": "4 tips for creating a great data science portfolio",
        "description": "It is a truth universally acknowledged that a data scientist in possession of a good portfolio must be in want of a job. A curated selection of your projects is the best way to showcase your work, interests, and thinking to potential employers. But what makes a \"good portfolio\"?",
        "date": "2020-07-29T00:00:00.000Z",
        "formattedDate": "July 29, 2020",
        "tags": [
          {
            "label": "data science",
            "permalink": "/blog/tags/data-science"
          },
          {
            "label": "thoughts",
            "permalink": "/blog/tags/thoughts"
          }
        ],
        "readingTime": 3.745,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "post",
          "title": "4 tips for creating a great data science portfolio",
          "tags": [
            "data science",
            "thoughts"
          ],
          "comments": true
        },
        "prevItem": {
          "title": "Women in Data Symposium - Skills to Last a Decade",
          "permalink": "/blog/2020/07/31/womenindata-symposium"
        },
        "nextItem": {
          "title": "The Pitfalls of Deep Learning (WAD talk)",
          "permalink": "/blog/2020/05/26/wad-pitfalls-deeplearning"
        }
      },
      "content": "It is a truth universally acknowledged that a data scientist in possession of a good portfolio must be in want of a job. A curated selection of your projects is the best way to showcase your work, interests, and thinking to potential employers. But what makes a \"good portfolio\"?\n\nFrom my experience and discussions with colleagues, I summed up **four tips** that you should apply to make your data science portfolio stand out from the crowd.\n\n## 1. Be original\nWhen you started learning data science, you've most probably learned to predict [Boston house prices](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html), classify [Iris flowers](https://archive.ics.uci.edu/ml/datasets/iris) or [handwritten digits](https://www.tensorflow.org/datasets/catalog/mnist), and wrangle features associated with [Titanic survivors](https://www.kaggle.com/c/titanic). Though these projects are a good start for learning the basic concepts of data science, they have become so common that they don't impress anyone anymore. \n\nSo instead of investing more time on these datasets, pick a new one of your own interest, apply different models and answer questions that you'd find insightful. I personally focused on projects that reflect my interest in [Linguistics](https://github.com/lorenanda/world-languages) and [NLP](https://github.com/lorenanda/speech-emotion-recognition) – you can explore data related to your experience or the industry you'd like to work in. \n\n[Kaggle](https://www.kaggle.com/) is a great resource of datasets for all sorts of topics, but if you want to be even more original, you can request access to some research datasets (e.g. [JAFFE](https://zenodo.org/record/3451524)), export your personal data from an app (e.g. [Goodreads](https://help.goodreads.com/s/article/How-do-I-import-or-export-my-books-1553870934590)) or create your own dataset.\n\n## 2. Pitch your work\nThe hard work is done: you've analysed the data, made predictions and maybe some pretty charts, and uploaded your Jupyter notebook to GitHub in a repository with a descriptive name.\n\nYou would probably not buy a book or read a paper without checking out its blurb or abstract first, to see if it's interesting and worth your time. Same goes for data science projects: don't expect people to browse through all the files or read the whole Jupyter notebook in your repository – a project portfolio is not a school assignment, it's a pitch! This means you have to give your audience a taste of your work and the value they can gain from it.\n\nThis is what the README is for. In this markdown file, you should include a **description** of the project (what, why, and how you did it), **results** (key findings), **setup** configuration (how the users can test the project, libraries used), and at least one **image** (insightful charts or screencasts). Also, you should adapt the writing style and structure to the target audience of your project. \n\nFor example, for my research project on [psych-verbs](https://github.com/lorenanda/psych-verbs) I wrote a paper-like README targeted at academics/fellow linguists, whereas for my [movie recommender system](https://github.com/lorenanda/movie-recommender) I wrote an informal short description and included a screencast, aimed at a general audience.\n\n## 3. Highlight your best projects\nThe point of a data science portfolio is to give employers an idea of your potential and prove that you can complete a project from idea to presentation, thus displaying both hard skills (coding and analysis) and soft skills (communication and presentation). As in almost all areas of life, I believe it's important to focus on quality over quantity.\n\nIt's better to have only two projects that are really well-done than tens of repositories with incomplete projects or code errors. Also, people tend to get stressed out and lose interest if they are given too many options (i.e. [the paradox of choice](https://www.ted.com/talks/barry_schwartz_the_paradox_of_choice?language=en)). That's why it's important to curate your work.\n\nOn GitHub, up to six repositories are displayed on your main profile. You can either create public repositories of only your best projects or [pin the ones](https://docs.github.com/en/github/setting-up-and-managing-your-github-profile/customizing-your-profile/pinning-items-to-your-profile) you want to highlight. \n\n## 4. Make it personal\nAt the end of the day, you are more than your work. You are not only a data scientist, but maybe also a photographer, a former lawyer, a musician, a defi enthusiast – whatever it is, don't be afraid to show this side of you. All your experiences, interests, and hobbies shape your personality and give you a unique perspective in data science.\n\nOn GitHub you can easily personalize your profile by adding a [special profile README](https://www.aboutmonica.com/blog/how-to-create-a-github-profile-readme) and a custom status. You could share what you are currently working on, what languages or tools you're learning, and how people can get in touch with you. Here you can let your creativity flow and personality shine!\n\nCreating a good data science portfolio takes time. Don't rush it, take your time to learn and polish both your coding and presentation skills. Good luck!"
    },
    {
      "id": "/2020/05/26/wad-pitfalls-deeplearning",
      "metadata": {
        "permalink": "/blog/2020/05/26/wad-pitfalls-deeplearning",
        "source": "@site/blog/2020-05-26-wad-pitfalls-deeplearning.md",
        "title": "The Pitfalls of Deep Learning (WAD talk)",
        "description": "We Are Developers is the world’s largest conference for software development. It’s a great opportunity for software engineers to connect with tech companies and learn about interesting advances in technology. This year, WAD was supposed to take place in Berlin, but due to COVID-19, the event was moved online for a Live Week (25-29 May) of exciting talks on different tech topics.",
        "date": "2020-05-26T00:00:00.000Z",
        "formattedDate": "May 26, 2020",
        "tags": [
          {
            "label": "data science",
            "permalink": "/blog/tags/data-science"
          },
          {
            "label": "conferences",
            "permalink": "/blog/tags/conferences"
          }
        ],
        "readingTime": 2.54,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "post",
          "title": "The Pitfalls of Deep Learning (WAD talk)",
          "tags": [
            "data science",
            "conferences"
          ],
          "comments": true
        },
        "prevItem": {
          "title": "4 tips for creating a great data science portfolio",
          "permalink": "/blog/2020/07/29/tips-for-datascience-portfolio"
        },
        "nextItem": {
          "title": "Diversity in IT Jobs (WAD talk)",
          "permalink": "/blog/2020/05/25/wad-diversity-it"
        }
      },
      "content": "We Are Developers is the world’s largest conference for software development. It’s a great opportunity for software engineers to connect with tech companies and learn about interesting advances in technology. This year, WAD was supposed to take place in Berlin, but due to COVID-19, the event was moved online for a Live Week (25-29 May) of exciting talks on different tech topics.\n\nOn the second day, I tuned in to watch **Adrian Spataru‘s and Bohdan Andrusyak‘s talk *The pitfalls of Deep Learning – when Neural Networks are not the solution***.\n\nAdrian and Bogdan are data scientists specialized in time series and computer vision, and NLP respectively. They also organize the Machine Learning Meetup in Graz, Austria.\n\nFor a start, they presented a comparison between Machine Learning (ML) and Deep Learning (DL):\n\n- ML goes from data to feature extraction to a ML model and finally to an output. Examples of ML are SVM, gradient boosting, and linear regression.\n- DL goes from data to DL (with very powerful feature extractors) and to the output. Examples of DL are convolutional neural networks, recurrent neural networks,  and transformers.\n\nNext, they gave three examples of DL applications:\n\n- **self-driving cars**: arguably wouldn’t be possible without DL, which are very good feature extractors.\n- **language translation**: DL is very very good at representation learning and has high computational power to find representations faster.\n- **music generation**: DL allows to model complex sequence data, which makes it possible to generate high dimensional data.\n\nAmazing as DL may sound, it has also produced failures, such as:\n\n- **criminal identification**: In 2019, Amazon’s facial recognition system Rekognition was tested on the faces of 24 professional football players and it shockingly matched almost 1 in 6 players with a mugshot in the police database.\n- **mistranslation**: A Zalando ad for lingerie which had in its original text the cuddle word “baby” was wrongly translated (and targeted) as “kid”.\n- **false claims**: Many companies claim (or naively conclude) to have used DL to solve problems that actually are not possible (yet), like predicting IQ, sexuality, or even whether a person is a potential terrorist.\n\n\nThe key idea of the presentation were the five issues with DL:\n\n- **data quantity and quality**: DL models can be only as good as the data they use – if with insufficient and unclean data they could only produce skewed results.\n- **tabular data (spreadsheets)**: is limited and as an alternative, tree-based models outperform DL, especially with good feature engineering.\n- **model explainability**: automatic feature extraction gives less control over the model.\n- **model complexity**: sometimes complex models can’t be used in real-life systems.\n- **resources**: the more parameters the model has, the more expensive it becomes.\n\n\nIn conclusion, those excited about integrating DL solutions in businesses are advised to not jump into it, but first think in terms of business gain: whether the performance boost that DL would provide is worth for the business. It’s more advisable to start with benchmarking and ML and see where they can go from there."
    },
    {
      "id": "/2020/05/25/wad-diversity-it",
      "metadata": {
        "permalink": "/blog/2020/05/25/wad-diversity-it",
        "source": "@site/blog/2020-05-25-wad-diversity-it.md",
        "title": "Diversity in IT Jobs (WAD talk)",
        "description": "We Are Developers is the world's largest conference for software development. It's a great opportunity for software engineers to connect with tech companies and learn about interesting advances in technology. This year, it was supposed to take place in Berlin, but due to COVID-19, it was moved online for a Live Week (25-29 May) of exciting talks on different tech topics.",
        "date": "2020-05-25T00:00:00.000Z",
        "formattedDate": "May 25, 2020",
        "tags": [
          {
            "label": "data science",
            "permalink": "/blog/tags/data-science"
          },
          {
            "label": "conferences",
            "permalink": "/blog/tags/conferences"
          }
        ],
        "readingTime": 1.525,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "post",
          "title": "Diversity in IT Jobs (WAD talk)",
          "tags": [
            "data science",
            "conferences"
          ],
          "comments": true
        },
        "prevItem": {
          "title": "The Pitfalls of Deep Learning (WAD talk)",
          "permalink": "/blog/2020/05/26/wad-pitfalls-deeplearning"
        },
        "nextItem": {
          "title": "10 data science books to read during lockdown",
          "permalink": "/blog/2020/05/05/datascience-books-for-lockdown"
        }
      },
      "content": "[**We Are Developers**](https://www.wearedevelopers.com/talents/) is the world's largest conference for software development. It's a great opportunity for software engineers to connect with tech companies and learn about interesting advances in technology. This year, it was supposed to take place in Berlin, but due to COVID-19, it was moved online for a [Live Week (25-29 May)](https://www.wearedevelopers.com/live) of exciting talks on different tech topics.\n\nOn the first day, I tuned in to watch **Christoph Pirringer**'s talk on *Diversity in Development: Which hurdles can we overcome right now to open up development?*\n\nChristoph Pirringer is the founder and CEO of [CodeFactory](https://codefactory.wien/), a Vienna-based coding school targeted at newbies and career-changers. What makes it stand out from other bootcamps is the **\"Diversity in Development\" **project, which aims to raise awareness about people with hearing impairments in IT and help them and companies integrate better in the tech workforce.\n\nIt's a valuable project, since there are 23 million deaf and hearing-impaired people in the EU, 10.000 of which in Austria. Even though in theory they have open access to work and education, in practice there are not enough measures taken to adapt to their needs and most companies don't know how to deal with it. As a consequence, deaf people are basically excluded from these opportunities.\n\nThe talk addressed some common **challenges** of working with people with hearing impairments. The main one is, obviously, **communication** -- this has to be based on written language, which can work just fine through emails and chats, but it gets tricky in meetings. A solution is having an interpreter on-site to translate into sign language. This has proven to be efficient in Pirringer's experience, so that CodeFactory went on to offer workshops for companies on sign language and how to include people with hearing impairments in their teams.\n\nI was impressed by this approach and wish more companies would take measures to accommodate people with different abilities!"
    },
    {
      "id": "/2020/05/05/datascience-books-for-lockdown",
      "metadata": {
        "permalink": "/blog/2020/05/05/datascience-books-for-lockdown",
        "source": "@site/blog/2020-05-05-datascience-books-for-lockdown.md",
        "title": "10 data science books to read during lockdown",
        "description": "cover",
        "date": "2020-05-05T00:00:00.000Z",
        "formattedDate": "May 5, 2020",
        "tags": [
          {
            "label": "data science",
            "permalink": "/blog/tags/data-science"
          },
          {
            "label": "books",
            "permalink": "/blog/tags/books"
          }
        ],
        "readingTime": 2.515,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "post",
          "title": "10 data science books to read during lockdown",
          "tags": [
            "data science",
            "books"
          ],
          "comments": true
        },
        "prevItem": {
          "title": "Diversity in IT Jobs (WAD talk)",
          "permalink": "/blog/2020/05/25/wad-diversity-it"
        },
        "nextItem": {
          "title": "Der Stand der Bibliotheken in Deutschland",
          "permalink": "/blog/2020/04/23/weltbuchtag"
        }
      },
      "content": "![cover](blog_images/books_datascience.jpg)\n\nIf you're reading this post around its publishing date in April 2020, chances are you're at home, responsibly isolating yourself amid COVID-19 curfews around the world. What a better time than now to read actual books on data-related topics, that you've been postponing for too long in favor of short articles...\n\nTo give you some food for thought for these days, I curated a **top 10** list of books straight from my bookshelves:\n\n**1. *Logicomix* by A. Doxiadis/C. Papadimitrou/A. Papados/A. Di Nonna** is a graphic novel about the life and work of philosopher Bertrand Russell. It's an insightful visual journey into his brilliant mind, the logical foundations of maths and modern philosophy.\n\n**2. *OpenIntro Statistics* by D. Diez/C. Barr/M. Cetinkaya-Rundel** is a manual of statistics, for data types to logistic regression, with applications in R. After every chapter, there are practice exercises with solutions. It's also [available online for free](https://www.openintro.org/book/os/), along with the datasets used in the book.\n\n**3. *Naked Statistics* by C. Wheelan** is a funny book about stats with examples of applications in real life. The concepts are explained in a simple and entertaining way, making stats and maths fun.\n\n**4. *How to Lie with Statistics* by D. Huff** is a classic bestseller on the (tricky) uses of stats. Published in 1954, the methods of distorting data representations for sales interests are still valid today. Read it if you want to not be fooled -- or if you want to fool people yourself.\n\n**5. *Factfulness* by H. Rosling** is an eye-opening book about global trends, social inequality, and how data representation can help make sense of the world. It presents ten instincts people have when interpreting data, and how to overcome them in order to evaluate situations more clearly. I could write a whole post about my admiration for Hans Rosling! Till then, you can also check out his [Gapminder](https://www.gapminder.org/) project and visualization tool.\n\n**6. *Python for Data Analysis* by W. McKinney** provides a detailed overview on all steps involved in data analysis, from library installation and data wrangling to time series and visualization. Some code information is outdated, but it explains well the steps and workflow of data science projects.\n\n**7. *Data Science for Dummies* by Lillian Pierson** gives you an overview of the skills you need as a data scientist, most common models, data visualization techniques, and business applications of data science.\n\n**8. *Data Science from Scratch with Python* by P. Morgan** is a concise introduction to machine learning algorithms, with short practice exercises in Python.\n\n**9. Hello World by H. Fry** is about the use of algorithms in different areas of everyday life, from cars and art to crime and medicine. It also presents cases about the repercussions of data bias, along with thoughts about the future applications of algorithms.\n\n**10. *Gödel, Escher, Bach* by D. Hofstadter** is \"a metaphorical fugue on minds and machines in the spirit of Lewis Carroll\". That sums up the essence of this extraordinary book, it delves into concepts used in computer science, linguistics, philosophy, and AI. I just started reading it and I'm captivated! Curious what I'll learn in the following 700 pages...\n\nGet a cup of tea and enjoy the read!"
    },
    {
      "id": "/2020/04/23/weltbuchtag",
      "metadata": {
        "permalink": "/blog/2020/04/23/weltbuchtag",
        "source": "@site/blog/2020-04-23-weltbuchtag.md",
        "title": "Der Stand der Bibliotheken in Deutschland",
        "description": "Am 23. April 2020 feiern alle Bücherwürmer den Welttag des Buches! Letztes Jahr habe ich zu diesem Anlass eine Infografik zum Stand der Buchproduktion in Österreich erstellt. Dieses Jahr liegt mein Fokus auf der Situation der Bibliotheken in Deutschland.",
        "date": "2020-04-23T00:00:00.000Z",
        "formattedDate": "April 23, 2020",
        "tags": [
          {
            "label": "data science",
            "permalink": "/blog/tags/data-science"
          },
          {
            "label": "analytics",
            "permalink": "/blog/tags/analytics"
          },
          {
            "label": "DataStudio",
            "permalink": "/blog/tags/data-studio"
          },
          {
            "label": "deutsch",
            "permalink": "/blog/tags/deutsch"
          }
        ],
        "readingTime": 1.375,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "post",
          "title": "Der Stand der Bibliotheken in Deutschland",
          "tags": [
            "data science",
            "analytics",
            "DataStudio",
            "deutsch"
          ],
          "comments": true
        },
        "prevItem": {
          "title": "10 data science books to read during lockdown",
          "permalink": "/blog/2020/05/05/datascience-books-for-lockdown"
        },
        "nextItem": {
          "title": "Goodreads Reading Challenge 2019",
          "permalink": "/blog/2019/12/30/goodreads-reading-challenge"
        }
      },
      "content": "Am 23. April 2020 feiern alle Bücherwürmer den [Welttag des Buches](https://www.welttag-des-buches.de/)! Letztes Jahr habe ich zu diesem Anlass eine Infografik zum Stand der Buchproduktion in Österreich erstellt. Dieses Jahr liegt mein Fokus auf der Situation der Bibliotheken in Deutschland.\n\nAuf der [Webseite des Statistischen Bundesamtes](https://www-genesis.destatis.de/genesis/online?levelindex=3&levelid=1587381269102&downloadname=&operation=ergebnistabelleDiagramm&option=diagramm&diagrammTyp=0&zeichnePunkte=false#abreadcrumb) habe ich Daten zu Haupt- und Zweigstellen der Bibliotheken in Deutschland von 2000 bis 2018 gefunden. Ich habe die Daten heruntergeladen, analysiert und sie auf einem [Dashboard in Google Data Studio ](https://datastudio.google.com/s/hjMWSwuxXN0)visualisiert. Hier sind die wichtigsten Erkenntnisse, die ich aus der Datenanalyse gewonnen habe:\n\n-   In Deutschland gibt es **8.152 öffentliche Bibliotheken** (Haupt- und Zweigstellen) mit **113.083 Büchern** und **745 wissenschaftliche Bibliotheken** mit **260.060 Büchern**. Das bedeutet im Durchschnitt 13 Bücher pro öffentliche bzw. 349 pro wissenschaftliche Bibliothek, oder ein **Verhältnis von 1:26**. Die wissenschaftliche Bibliotheken sind also selten, aber groß.\n\n![cover](blog_images/weltbuchtag2.jpg)\n\n-   **Bayern** hat die meisten Bibliotheken (**1853**) und **Bremen** die wenigsten (**13**). Wenn wir aber diese Zahlen im Verhältnis zur Bevölkerung jedes Bundeslands betrachten, schneidet **Rheinland-Pfalz** am besten (ca. **1 Bibliothek zu 6.000 Personen)**, im Gegensatz zu **Berlin** (ca. **1 Bibliothek zu 55.000 Personen**).\n\n![cover](blog_images/weltbuchtag3.jpg)\n\nAnzahl von öffentlichen Bibliotheken nach Bundesland (2018)\n\n-   Im Jahr 2018 gab es in **Nordrhein-Westfalen **die meisten **Bibliotheksbesuche** (24.702) und **Entleiher** (1.554). Das entspricht ungefähr 475 Besuche pro Woche. Im Gegensatz dazu sind in **Saarland** nur 642 Bibliotheksbesuche und 40 Entleiher.\n-   In allen Bundeländern betragen die **Entleiher weniger als 1%** der Bevölkerung.\n-   Deutschlandweit nimmt der Anzahl von Bibliotheken, Entleiher und Bibliotheksbesuche ab. Es gibt **23% wenigere Bibliotheken** und **15% wenigere Entleiher** als vor 20 Jahren.\n\nZusammenfassend lässt sich einen negativen Trend in der Anzahl von Bibliotheken, Bibliotheksbesuche und Entleiher in Deutschland feststellen. Das ist enttäuschend, weil Bibliotheken nicht nur eine allumfassende Bildungsfunktion erfüllen, sondern zugleich die Geschichte und Kultur einer Stadt prägen. Also meine Empfehlung für eine entspannende Aktivität: in einer Bibliothek ein Buch lesen oder sogar Filme ausleihen."
    },
    {
      "id": "/2019/12/30/goodreads-reading-challenge",
      "metadata": {
        "permalink": "/blog/2019/12/30/goodreads-reading-challenge",
        "source": "@site/blog/2019-12-30-goodreads-reading-challenge.md",
        "title": "Goodreads Reading Challenge 2019",
        "description": "As the year is almost over, it's time to review my one and only resolution that I've been sticking to for the past four years: the Goodreads Reading Challenge. For 2019 I set quite a reasonable goal of reading 24 books, an average of two per month.",
        "date": "2019-12-30T00:00:00.000Z",
        "formattedDate": "December 30, 2019",
        "tags": [
          {
            "label": "analytics",
            "permalink": "/blog/tags/analytics"
          },
          {
            "label": "books",
            "permalink": "/blog/tags/books"
          },
          {
            "label": "data science",
            "permalink": "/blog/tags/data-science"
          },
          {
            "label": "DataStudio",
            "permalink": "/blog/tags/data-studio"
          }
        ],
        "readingTime": 2.265,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "post",
          "title": "Goodreads Reading Challenge 2019",
          "tags": [
            "analytics",
            "books",
            "data science",
            "DataStudio"
          ],
          "comments": true
        },
        "prevItem": {
          "title": "Der Stand der Bibliotheken in Deutschland",
          "permalink": "/blog/2020/04/23/weltbuchtag"
        },
        "nextItem": {
          "title": "Web-scraping IMDb with R",
          "permalink": "/blog/2019/03/29/web-scraping-imdb-with-r"
        }
      },
      "content": "As the year is almost over, it's time to review my one and only resolution that I've been sticking to for the past four years: the [Goodreads Reading Challenge](https://www.goodreads.com/user_challenges/14685444). For 2019 I set quite a reasonable goal of reading 24 books, an average of two per month.\n\nTo combine my love for literature and data analysis, I've created [a dataset](https://docs.google.com/spreadsheets/d/1YFqRSg43u0TD5btmiEQxc7nhHomnzRujoiSaBMRFIvE/edit?usp=sharing) with key information about the books I've read this year and visualized it in an interactive [dashboard with Google Data Studio.](https://datastudio.google.com/s/pxR01oyZLF4)\n\n[![](https://lorenaciutacu.files.wordpress.com/2019/12/screenshot_dashboard_gr.png?w=1024)](https://datastudio.google.com/s/pxR01oyZLF4)\n\nSurprisingly, I've exceeded my goal by a whopping 42%, totaling **34 books**. In total I've read almost **9000 pages** of authors from **15 different countries**. My average book rating is 4, which means that mostly good books have fallen into my hands, or that I'm quite generous with ratings, but also that I keep on reading and eventually rate books that I like.\n\n**January** was my peak reading month (9 books), whereas **April and May** were the lowest (1 book each). This looks like extreme reading habits, but it reflects the pace of my life throughout the year. January and September have been pretty quiet with a routine, which allowed me to indulge in reading. In contrast, April brought me a promotion at work and May a new home, so the new responsibilities and moving around left me almost no time to read for pleasure.\n\nAs in the past years, I've read mainly authors from the **USA** (12), followed by **Germany** (7) and **France** (3). However, I've read almost equally in **English** and **German** (15 and 14 books, respectively). I definitely want to explore more of the literary world, so if you have any recommendations, please drop me a comment here or on [goodreads](https://www.goodreads.com/user/show/12918860-ana).\n\nAs for the format, more than a third of the books I've read are **audiobooks (27%)** and **ebooks (12%)**. Yes, the digital format is my secret to reading at all times possible! I like to listen to audiobooks while cleaning the house, cooking, or commuting (especially at night after the training when I'm too tired to read a single line). I switch to ebooks usually when I travel, since a kindle doesn't take up much space and I can move more lightly. However, I still prefer paperback books above all -- there's something about holding a book and flipping through the pages smelling of fresh print or old library.\n\nAn interesting observation is that, though I've read more **fiction (59%)** than **nonfiction (35%)**, 8 of my 12 favorite books are non-fiction (and 3 of them deal with economics), whereas all 4 of my least liked books are fiction.\n\nNow, for the last days of December, I still have a couple of pages left of *Flights* by the Nobel prize winner Olga Tokarczuk. For 2020 I already have a long reading list that I'm looking forward to.\n\nWish you a goodread year and insightful datasets!"
    },
    {
      "id": "/2019/03/29/web-scraping-imdb-with-r",
      "metadata": {
        "permalink": "/blog/2019/03/29/web-scraping-imdb-with-r",
        "source": "@site/blog/2019-03-29-web-scraping-imdb-with-r.md",
        "title": "Web-scraping IMDb with R",
        "description": "Web scraping is a method of automatically gathering data from websites in a structured manner and storing it into a local database or spreadsheet. Why would you do this? Because you're lazy. Or because it's really impossible to copy-paste all the data you need from the website.",
        "date": "2019-03-29T00:00:00.000Z",
        "formattedDate": "March 29, 2019",
        "tags": [
          {
            "label": "data science",
            "permalink": "/blog/tags/data-science"
          },
          {
            "label": "tutorials",
            "permalink": "/blog/tags/tutorials"
          },
          {
            "label": "R",
            "permalink": "/blog/tags/r"
          }
        ],
        "readingTime": 4.345,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "post",
          "title": "Web-scraping IMDb with R",
          "tags": [
            "data science",
            "tutorials",
            "R"
          ],
          "comments": true
        },
        "prevItem": {
          "title": "Goodreads Reading Challenge 2019",
          "permalink": "/blog/2019/12/30/goodreads-reading-challenge"
        },
        "nextItem": {
          "title": "Analyzing my reading habits from Goodreads data",
          "permalink": "/blog/2018/10/21/tableau-visualisation-goodreads"
        }
      },
      "content": "**Web scraping** is a method of automatically gathering data from websites in a structured manner and storing it into a local database or spreadsheet. Why would you do this? Because you're lazy. Or because it's really impossible to copy-paste all the data you need from the website.\n\nSome popular use-cases of web scraping are price comparison sites of products from different companies, lead generation from collected contact information, trend analysis of popular topics in a certain location. I simply wanted to see what are the most popular movies of 2018 and what features they have.\n\nThere are many **tools for web scraping**: browser plug-ins (e.g., [Webscraper](https://www.webscraper.io/)) and software (e.g., [Parsehub](https://www.parsehub.com/)) are easy to use and don't require coding.\n\nHowever, if you need more advanced scraping settings and have basic coding skills, I recommend the Python libraries [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/) or [Selenium](https://selenium-python.readthedocs.io/), and the R package [rvest](https://cran.r-project.org/web/packages/rvest/README.html). The latter is the one I used for scraping IMDb, and you can find the commented code on [my GitHub](https://github.com/lorenanda/imdb/blob/master/imdb_top_2018.R).\n\nBefore I proceed to the fun part, note that the legality of web scraping is not clearly defined around the world, so you should check the website's terms of use before scraping it!\n\nNow let's dive in. I wanted my data analysis to answer three questions:\n\n-   What are the most successful movies released in 2018?\n-   What genres do the popular movies belong to?\n-   What is the duration of the most popular movies?\n\n## Top popular movies\n\nI used [IMDb](https://www.imdb.com/search/title?year=2018) as a reference, because it contains all the information I need. On the website I selected the movies released between 01.01.-31.12.2018, sorted by popularity, and limited my search to the first page, so the top 50 movies.\n\n```r\nlibrary(rvest)\nurl <- \"https://www.imdb.com/search/title?year=2018\"\nimdb <- read_html(url)\nhead(imdb)\n```\n\nThe **top 5 popular movies in 2018** were:\n\n1.  *Aquaman*\n2.  *Green Book*\n3.  *Bohemian Rhapsody*\n4.  *Spider-Man: Into the Spider-Verse*\n5.  *Avengers: Infinity War*\n\n## Top movie genres\n\nScraping the genre tags of each movie is pretty straightforward:\n\n```r\ngenre_data_html <- html_nodes(imdb, \".genre\")\ngenre_data <- html_text(genre_data_html)\nhead(genre_data)\n```\n\nHowever, this returns a list of genres for each movie, because the movies are labeled with multiple genres. The text data needs to be cleaned a bit:\n\n```r\n#remove the \\n in front of the genres\ngenre_data <- gsub(\"\\n\", \"\", genre_data)\n\n#remove the spaces between genres\ngenre_data <- gsub(\" \", \"\", genre_data)\n```\n\nNow, another tricky thing is that the genres of each movie are enumerated alphabetically, not in order of importance. To simplify my work, I selected only the first genre:\n\n```r\n#display only the first genre in the list\ngenre_data <- gsub(\",.*\", \"\", genre_data)\n```\nAt a glance, I noticed that 3 out 5 are action-hero movies, so I visualized closer at the **genre distribution:**\n\n```r\n#plot the number of movies by genre\nlibrary(ggplot2)\nggplot(imdb_df, aes(x=genre_data)) +\ngeom_bar(color=\"purple\", fill=\"green\", alpha=0.3) +\nggtitle(\"Number of movies by genre\") +\nxlab(\"Genre\") + ylab(\"Number of movies\")\n```\n\n![genres_count](https://raw.githubusercontent.com/lorenanda/imdb/master/genres_count.png)\n\nMy initial observation was confirmed: **Action** and **Drama** are the most popular genres, followed by **Biography**. I guess most people enjoy, on one hand, movies that transport them into wild worlds and simulate experiences out of the ordinary, and on the other hand, movies that depict dramatic life stories and relate to some extent to their real life.\n\n## Top movies duration\n\nNext, I analyzed the **distribution of movie duration:**\n\n```r\n#plot the movies by runtime\nbarplot(table(imdb_df$Runtime))\nhist(imdb_df$Runtime)\nggplot(imdb_df, aes(x=runtime_data)) +\ngeom_histogram(color=\"purple\", fill=\"green\", alpha=0.3) +\nggtitle(\"Distribution of movie runtimes\") +\nxlab(\"Minutes\") + ylab(\"Number of movies\")\n```\n\nThe plot shows that most popular movies last on average **104 minutes** (median 117 minutes). The **longest movie** is *Avengers: Infinity War *(149 minutes) and the **shortest movie** (excluding TV-shows) is *A.I. Rising* (85 minutes). From the histogram it is clear that the bars on the left represent the TV-shows (under 60 minutes).\n\n![hist_runtime](https://raw.githubusercontent.com/lorenanda/imdb/master/hist_runtime.png)\n\nI also analyzed the runtime distribution by genre. First, I aggregated the movies by genre:\n\n```r\n#group movies by genre\nlibrary(dplyr)\ngenre_cat = group_by(imdb_df, Genre)\ngenre_runtime = summarize(genre_cat, Minutes=mean(Runtime))\nplot(genre_runtime)\n```\n\nThen, I visualized the average duration for each genre:\n\n```r\ncounts = table(genre_runtime$Genre, genre_runtime$Minutes)\nggplot(data=genre_runtime, aes(x=Genre, y=Minutes)) +\ngeom_bar(stat = \"identity\", color=\"purple\", fill=\"green\", alpha=0.3) +\nggtitle(\"Mean movie duration by genre\")\n```\n\nI found that among genres **Biographies are longest** (on average **127 minutes**) and **Crimes are shortest** (on average **85 minutes**). \n\nThis is not entirely surprising, since I think that, first, it is quite a challenge to pack a lifetime in a biographical movie, and second, there's only so much nerve-wrecking tension a person can take following a crime.\n\nHowever, I was expecting the average duration of **Animations** to be shorter than **110 minutes**, because they are produced mainly for children, who have a short attention span and low patience to sit through a two-hour movie. But then again, we are talking about the most popular movies of last year on IMDb, which means that adults made up the large audience.\n\n![mean_genre_runtime](https://raw.githubusercontent.com/lorenanda/imdb/master/mean_genre_runtime.png)\n\n## Conclusion\n\nThis is a simple web scraping project that can reveal a lot of information about people's movie preferences. It would be interesting to also analyze the total gross and see which movies and genres have sold best in 2018. Now **you** could try to scrape and analyze this information with your preferred tool and let me know what you found out!\n\nWith all this in mind, I'm heading to watch the 36th movie and only documentary in IMDb's Top 2018: *Free Solo*. Some realistic action and drama, for once."
    },
    {
      "id": "/2018/10/21/tableau-visualisation-goodreads",
      "metadata": {
        "permalink": "/blog/2018/10/21/tableau-visualisation-goodreads",
        "source": "@site/blog/2018-10-21-tableau-visualisation-goodreads.md",
        "title": "Analyzing my reading habits from Goodreads data",
        "description": "Goodreads is an amazing platform for tracking and organizing your reading, discovering new books, and sharing reviews or recommandations with friends.",
        "date": "2018-10-21T00:00:00.000Z",
        "formattedDate": "October 21, 2018",
        "tags": [
          {
            "label": "data science",
            "permalink": "/blog/tags/data-science"
          },
          {
            "label": "analytics",
            "permalink": "/blog/tags/analytics"
          },
          {
            "label": "Tableau",
            "permalink": "/blog/tags/tableau"
          }
        ],
        "readingTime": 3.105,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "post",
          "title": "Analyzing my reading habits from Goodreads data",
          "tags": [
            "data science",
            "analytics",
            "Tableau"
          ],
          "comments": true
        },
        "prevItem": {
          "title": "Web-scraping IMDb with R",
          "permalink": "/blog/2019/03/29/web-scraping-imdb-with-r"
        },
        "nextItem": {
          "title": "Talking about psych-verbs @ 5th Linguistics Meetup in Potsdam",
          "permalink": "/blog/2018/08/04/poster-linguistik-meetup-potsdam"
        }
      },
      "content": "[Goodreads](https://www.goodreads.com) is an amazing platform for tracking and organizing your reading, discovering new books, and sharing reviews or recommandations with friends. \n\nOn my quest to becoming a data scientist, I've recently played with [my goodreads](https://www.goodreads.com/lorenasbooks) data to get some insights into my reading habits in the past six years. You can have a look [in the GitHub repo](https://github.com/lorenanda/goodreads) at the full dataset and basic analysis with Python. For a quick and colorful overview, I create two interactive dashboards: [one in Google Data Studio](https://datastudio.google.com/embed/reporting/1G4jH00ImVcFU1c8X_wFyWRNP2SN6v5WH/page/Ivkh) and one in [Tableau](https://public.tableau.com/profile/lorena.ciutacu).\n\n\n## Export Goodreads library\n\nTo export your Goodreads library, go to *My Books > Tools > Import and Export > Export Library*.\n\nThe exported data include:\n- book titles\n- authors' names\n- publishing year\n- number of pages\n- average book rating on goodreads\n- personal book rating.\n\nFor extra fun, I added two dimensions to the dataset:\n- the author's country of origin\n- the language in which I've read the book.\n\n## Books read per country & continent\n\nFirst of all, I mapped the books I've read to have an overview of **my global library**. The color gradient represents the count of books read from that country -- the darker the color, the more books. In the Tableau viz you can hover over each country to see the exact number of books.\n\n![map](https://lorenaciutacu.files.wordpress.com/2018/10/map-e1539944679944.jpg?w=736)\n\nSince I've graduated a bilingual Romanian-English highschool and a BA in German and French Translation, I've mostly read American, British, Romanian, French, and German literature. And also a good share of Russian classics to sharpen my heart. Looking at the map made me realize what a small part of the world how little I've actually read. Even of European literature, I've only read works from some of the most prominent countries, not to mention the other continents.\n\nMoreover, this indicates the American influence on the book market, which also reveals the power of translation, looking at what authors are promoted and given a voice across borders by translating their works into widely spoken languages. Consequently, I've decided to read at least one book from each country in the world, so please let me know if you have any recommandations.\n\n## Books read per year & month\n\nNext, I was interested to see **how many books I've read every year, monthly.** Seems like 2016 was my bookaholic year, with a whopping 37 books, of which 8 read in July. That was time when I was writing my BA thesis, travelling often for a long-distance relationship, and spending many nights paralyzed with anxiety, so naturally I turned to books for inspiration, distraction, and consolation. Based on the pattern of the past six years, Tableau helped me predict that in 2018 I will read 17 books or 7087 pages, and in 2019 25 books or 8576 pages. Let's see how that goes...\n\n![yearly book count](https://lorenaciutacu.files.wordpress.com/2018/10/yearly-book-count.jpg?w=736)\n\n![reading timeline](https://lorenaciutacu.files.wordpress.com/2018/10/reading-timeline.jpg?w=736)\n\n## Book ratings\n\nI've rated most of the books I've read with 4 stars out of 5, which indicates that (a) I've read mostly subjectively good books, (b) I'm generous with rating, (c) I only bothered to rate books that I really liked, (d) I only continued and finished reading books that I liked.\n\n![histogram my rating](https://lorenaciutacu.files.wordpress.com/2018/10/histogram-my-rating.jpg?w=736)\n\n## Books read by language\n\nI'm a polyglot and reading literature is my favorite way to learn languages. Whenever possible, I prefer to read books in their original language or, if I don't speak that language or I can't get the original edition, then in a related language or ultimately in English. This is in fact the **language in which I've read** more than half of the books in the past six years, even more than in my native language Romanian.\n\n![languages](https://lorenaciutacu.files.wordpress.com/2018/10/languages-e1539946580647.jpg?w=736)\n\nThis is my first Tableau viz and it was exciting to learn the software with the goodreads data."
    },
    {
      "id": "/2018/08/04/poster-linguistik-meetup-potsdam",
      "metadata": {
        "permalink": "/blog/2018/08/04/poster-linguistik-meetup-potsdam",
        "source": "@site/blog/2018-08-04-poster-linguistik-meetup-potsdam.md",
        "title": "Talking about psych-verbs @ 5th Linguistics Meetup in Potsdam",
        "description": "Wednesday marked the first day of the last summer month and I celebrated by presenting a poster at the 5th Linguistics Meetup Berlin-Potsdam, hosted at the Potsdam University.",
        "date": "2018-08-04T00:00:00.000Z",
        "formattedDate": "August 4, 2018",
        "tags": [
          {
            "label": "linguistics",
            "permalink": "/blog/tags/linguistics"
          },
          {
            "label": "conferences",
            "permalink": "/blog/tags/conferences"
          }
        ],
        "readingTime": 0.36,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "post",
          "title": "Talking about psych-verbs @ 5th Linguistics Meetup in Potsdam",
          "gh-repo": "lorenanda/psych-verbs",
          "gh-badge": [
            "star",
            "fork",
            "follow"
          ],
          "tags": [
            "linguistics",
            "conferences"
          ],
          "comments": true
        },
        "prevItem": {
          "title": "Analyzing my reading habits from Goodreads data",
          "permalink": "/blog/2018/10/21/tableau-visualisation-goodreads"
        },
        "nextItem": {
          "title": "Talking about autism in Austrian newspapers @ 8th ULAB Conference in Edinburgh",
          "permalink": "/blog/2018/04/11/ulab-conference-edinburgh"
        }
      },
      "content": "Wednesday marked the first day of the last summer month and I celebrated by presenting a poster at the **5th Linguistics Meetup Berlin-Potsdam**, hosted at the Potsdam University. \n\nWhereas last year I gave a [talk about semantic dementia](/linguistik-meetup-berlin), this year I contributed with a poster about my ongoing Master's dissertation on **Romanian psych-verbs**.\n\nUpdate: You can read more about the research method and data analysis of this project in [this blog post](/romanian-psych-verbs-study)."
    },
    {
      "id": "/2018/04/11/ulab-conference-edinburgh",
      "metadata": {
        "permalink": "/blog/2018/04/11/ulab-conference-edinburgh",
        "source": "@site/blog/2018-04-11-ulab-conference-edinburgh.md",
        "title": "Talking about autism in Austrian newspapers @ 8th ULAB Conference in Edinburgh",
        "description": "I've just come back, tired but inspired, from the ULAB Conference in Edinburgh, where for three days I've learned about research projects on various linguistics topics of students from Britain mainly.",
        "date": "2018-04-11T00:00:00.000Z",
        "formattedDate": "April 11, 2018",
        "tags": [
          {
            "label": "linguistics",
            "permalink": "/blog/tags/linguistics"
          },
          {
            "label": "conferences",
            "permalink": "/blog/tags/conferences"
          }
        ],
        "readingTime": 3.13,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "post",
          "title": "Talking about autism in Austrian newspapers @ 8th ULAB Conference in Edinburgh",
          "tags": [
            "linguistics",
            "conferences"
          ],
          "comments": true
        },
        "prevItem": {
          "title": "Talking about psych-verbs @ 5th Linguistics Meetup in Potsdam",
          "permalink": "/blog/2018/08/04/poster-linguistik-meetup-potsdam"
        },
        "nextItem": {
          "title": "Learnings from the 11th Meeting of Patholinguistics in Potsdam",
          "permalink": "/blog/2017/11/20/patholinguistics-conference-potsdam"
        }
      },
      "content": "I've just come back, tired but inspired, from the **[ULAB Conference](http://lingstudents.co.uk/ulab2018/)**** in Edinburgh**, where for three days I've learned about research projects on various linguistics topics of students from Britain mainly.\n\n## Programme\n\nThe [programme](http://lingstudents.co.uk/wp-content/uploads/2018/03/ULAB2018_programme.pdf) included **three keynotes** (Prof. Antonella Sorace on adult bilingualism,  Prof. Frederik Kortlandt on relative chronology, and Prof. Simon King on personalised speech communication aids), **poster sessions** and** talks** on sociolinguistics, discourse analysis, computational linguistics, typology, and psycholinguistics. Oh! and of course, there were social events: a walk to Arthur's seat, ceilidh, dinner, city tour, pub quiz... So I had the chance to meet many amazing people who inspired me to pursue research and also have fun along the way 🙂 Hopefully again at the next ULAB'19 hosted by the University of London at Queen Mary!\n\n![goodies bag](blog_images/ulabBag.jpg)\n\n## My talk\n\nOn the second day, I gave a **talk** on the use of the word **'autistic' as figure of speech in Austrian newspapers.\n\n![ulab talk](blog_images/ulabTalk.jpg)\n\nThis was a paper that I wrote for a seminar on critical discourse analysis as an undergrad at the University of Vienna. I had noticed how often people would say that someone or something is/ seems autistic, with no relation to the condition of autism spectrum disorder. I found this word used quite often not only is speech, but also in the media. After the initial reaction \"What is this even supposed to mean!?\" I set out to look closer at the contexts in which this adjective in used and by what group of people. I chose the three most read Austrian newspapers from different categories:\n\n-   *Heute* is a tabloid found free in subway/ bus stations, in which you can read about trivial topics, local or big news but in a simple language and short sentences. The majority of [readers ](http://www.media-analyse.at/table/2838)are 40-49 years old and have an apprenticeship as their highest education.\n-   *Kronen Zeitung* is slightly higher quality than Heute, most readers are 50-59 years old and have also an apprenticeship as their highest education.\n-   *derStandard* is a quality newspaper in which you can read mainly about (inter)national events, politics, economics, culture, opinion and analysis. Most readers are 20-29 years old and have a University degree.\n\nI searched the online archive between 2014-2016 of each newspaper with the keyword *autistisch**, *excluded the articles that referred to ASD, and ended up with 4 articles from KZ and 19 from DS in which the word was used as figure of speech. Here are some examples (my translation from German):\n\n-   \"[Hungary's autistic refugee policy](http://www.krone.at/473701)\" → isolated, independent, closed\n-   \"autistic health minister\" → cold, isolated person\n-   \"It is an autistic time, because people move around in their own universe.\" → asocial behavior\n-   \"[the Shard remains a strange solitary which spikes up autistically](https://derstandard.at/1342139077111/Modern-oder-Mordor)\" → tall building, solitary, outsider\n-   \"[there was always the impression of an autistic nation](https://derstandard.at/2000014844421/Es-geht-nicht-nur-um-Musik-Konferenz-analysierte-den-Song)\" → isolated, unwelcoming\n-   \"[soon everyday object won't just stay autistically in the corner and wait for signal](https://derstandard.at/1392687503825/Softwarebrueder-fuer-das-geloggte-Leben)\" → subordinate, dependent, isolated\n-   \"[to describe this novel as borderline autistic is not really wrong](http://derstandard.at/2000024799189/Salman-Rushdie-Erzaehlen-als-letztes-Mittel)\" → still not sure what this is supposed to mean\n\nAfter I analysed the 23 articles, I found that 'autistic' was used in various types of texts (reviews, interviews, reports), in various non-medical contexts (politics, art, music, literature, science, architecture) to illustrate asocial behavior, (self-)isolation, distance, or disinterest. Most articles were published in DS, the newspaper with the youngest readership and highest educational level, and generally by persons from the upper social class. Thus, the word 'autistic' is used as a figure of speech with a negative connotation, and to display an elevated vocabulary of the speaker. I find this usage problematic because it reduces the complex ASD to a few negative characteristics, it objectifies persons with ASD, thus communicating a superficial view of ASD which leads to marginalization and oppression, where 'autistic' becomes a synonym for 'outsider', and 'atypical'. Just something to think about..."
    },
    {
      "id": "/2017/11/20/patholinguistics-conference-potsdam",
      "metadata": {
        "permalink": "/blog/2017/11/20/patholinguistics-conference-potsdam",
        "source": "@site/blog/2017-11-20-patholinguistics-conference-potsdam.md",
        "title": "Learnings from the 11th Meeting of Patholinguistics in Potsdam",
        "description": "Last Saturday 18th November I attended again the Autumn Meeting of Patholinguistics in Potsdam, in a wonderful new location Am Neuen Palais. The theme of this edition was \"Diagnosis and treatment of dysphonia\" (also known as hoarseness), a condition of the vocal chords or the larynx which makes the voice sound breathy, strained, raspy, or lower in volume or pitch.",
        "date": "2017-11-20T00:00:00.000Z",
        "formattedDate": "November 20, 2017",
        "tags": [
          {
            "label": "linguistics",
            "permalink": "/blog/tags/linguistics"
          },
          {
            "label": "conferences",
            "permalink": "/blog/tags/conferences"
          }
        ],
        "readingTime": 4.11,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "post",
          "title": "Learnings from the 11th Meeting of Patholinguistics in Potsdam",
          "tags": [
            "linguistics",
            "conferences"
          ],
          "comments": true
        },
        "prevItem": {
          "title": "Talking about autism in Austrian newspapers @ 8th ULAB Conference in Edinburgh",
          "permalink": "/blog/2018/04/11/ulab-conference-edinburgh"
        },
        "nextItem": {
          "title": "Expolingua Berlin",
          "permalink": "/blog/2017/11/17/expolingua-berlin"
        }
      },
      "content": "Last Saturday **18th November** I attended [again](/patholinguistics-meeting-potsdam) the [**Autumn Meeting of Patholinguistics**](https://www.herbsttreffen.vpl-ev.de/) in Potsdam, in a wonderful new location Am Neuen Palais. The theme of this edition was **\"Diagnosis and treatment of dysphonia\"** (also known as hoarseness), a condition of the vocal chords or the larynx which makes the voice sound breathy, strained, raspy, or lower in volume or pitch.\n\n\n## Talks\n\nAs I've learned from the **keynote lecture of Susanne Voigt-Zimmermann** (who btw had the terrible idea of \"warming up our vocal chords\" before her talk by making us stand up and sing *Bunt sind schon die Wälder...*), there is a triad of **symptoms of dysphonia**: \n* changes in voice timbre, \n* limited voice performance, \n* discomfort when speaking. \n\nThe **causes of dysphonia** are of two types: \n* organic (e.g. inflammation, polyps, cysts, nodules) \n* functional (e.g. poor muscles, stress, vocal strain if you sing/talk for a long  time or cheer too enthusiastically). \n\nYou may have dysphonia and not even be aware of it. To find out, you'd need a **laryngoscopy**, which is a procedure performed by an otolaryngologist or a speech-language pathologist to visualize the vibration and closure of your vocal fold and assess the damage.\n\nIn most cases, dysphonia can be treated with voice therapy or surgery, though it's more difficult to diagnose and treat in **children**, because the vocal chords are still developing and voices are \"growing up\" as well. Though it's unhealthy, a hoarse voice could play well for artistic purposes, just listen to Tom Waits or Asaf Avidan or that suspenseful voice of movie trailers.\n\n## Workshop\n\nThe most insightful part, and actually the main reason I attended the Meeting, was the **workshop** *Language initiation in children with autism,* held by the speech-language therapist Kristin Snippe. We started by writing on post-its keywords for \"typically autistic\" language, then she discussed and illustrated them with examples from her practice. \n\nOne of the earliest indicators of ASD is **delayed language development**, as children may say the first words after the age of 4-5. They usually speak in **short and simple sentences** limited to the basic subject-object-verb structure, and may **reverse the 1st and 2nd pronouns** (I/me, you) or refer to themselves in the 2nd person (e.g. *You want juice*. instead of *I want juice.*). After the initial stage of language acquisition, it's quite common for **language development to stagnate or regress.** \n\nAnother common feature is **echolalia**, a stereotypic behavior when the child repeats what he hears, either immediately (e.g. Dad asks *Do you want more rice?* and the child says *Do you want more rice.*) or delayed (e.g. the child memorizes a question/sentence/conversation and repeats it later or on another occasion). For example, a parent complained that she couldn't speak on the phone in the presence of her autistic son because he would repeat the entire conversation afterwards. \n\nIt may sound annoying, but echolalia can serve many different functions: learning by imitation, mapping questions to situations (e.g. *Are you hungry? Enjoy your meal!* for dinner), affirmative answer to a question, establish contact with the interlocutor by repeating her words, time to process the interlocutor's words before replying, or self-stimulating behavior because certain words sound funny/interesting. \n\nI learned about three **therapeutical approaches** for initiating communication in children with autism:\n\n-   **Behavioral (Verbal Behavior Therapy, Applied Behavioral Therapy)**: The child is taught the meaning and use of words, for example to say *water* when she sees different bottles of water but also when she is thirsty. The therapy is structures, with a set of tasks or activities, and the therapist uses prompts, repetition, rewards, and reinforcement in teaching.\n-   **Incidental Teaching:** The child is let to initiate interaction and the therapist plays along. It takes place in the natural environment, it's unstructured, but requires creativity and spontaneity for the therapist to create \"teachable moments\" with what's at hand. For example, a boy in Ms. Snippe's therapy was carrying objects from one room to another, uninterested in other activities, so she stepped in his \"game\" and started carrying toys as well, but stopped repeatedly along the way to indicate that she is tired, so that the boy would push her or point to the room.\n-   **Picture Exchange Communication System (PECS)**: The child communicates using picture cards of objects, foods, places etc. For example, the child would hand you a picture of a glass of water to \"say\" that he's thirsty. Gradually, it's possible to combine picture and verb cards to make sentences. For example, to say *I want to go with dad in the park,* the child would arrange cards with the verbs *want* and *go* and pictures of his dad and a park.\n\nChildren with autism/Asperger's also have difficulties with **pragmatics** and **non-verbal communication** (e.g. gestures, facial expressions, tone of voice, turn-taking, irony/sarcasm, assumptions, allusions, eye contact). These can be trained, for example by simulating social situations or analyzing the non-verbal communication in stories/movies.\n\n## Poster session\nIn the poster session I learned about ongoing research on dysphagia, working memory in patients with aphasia, and therapy for dysprosody in patients with apraxia of speech. Since I didn't know much about voice disorders before, this Meeting was very informative and exciting, especially because it presented mainly applied research. Looking forward to next year!"
    },
    {
      "id": "/2017/11/17/expolingua-berlin",
      "metadata": {
        "permalink": "/blog/2017/11/17/expolingua-berlin",
        "source": "@site/blog/2017-11-17-expolingua-berlin.md",
        "title": "Expolingua Berlin",
        "description": "Today I attended the long-awaited international fair for languages and cultures ExpoLingua in Berlin. It's a must event for anyone interested in language learning and teaching, because for two days (17-18 November) it brings 150+ exhibitors (language schools, travel operators, cultural institutes, publishing houses, translators, associations) from 30+ countries and offers workshops, presentations, crash-courses in different languages, performances, and moderated panel discussions, from five categories: languages and cultures, presentations for teachers, learn & travel, tests, and career.",
        "date": "2017-11-17T00:00:00.000Z",
        "formattedDate": "November 17, 2017",
        "tags": [
          {
            "label": "linguistics",
            "permalink": "/blog/tags/linguistics"
          },
          {
            "label": "conferences",
            "permalink": "/blog/tags/conferences"
          }
        ],
        "readingTime": 1.535,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "post",
          "title": "Expolingua Berlin",
          "tags": [
            "linguistics",
            "conferences"
          ],
          "comments": true
        },
        "prevItem": {
          "title": "Learnings from the 11th Meeting of Patholinguistics in Potsdam",
          "permalink": "/blog/2017/11/20/patholinguistics-conference-potsdam"
        },
        "nextItem": {
          "title": "Talking about the brains of interpreters @ 2nd Conference for Student Research in Berlin",
          "permalink": "/blog/2017/09/22/conference-student-research-berlin"
        }
      },
      "content": "Today I attended the long-awaited international fair for languages and cultures [**ExpoLingua** ](http://expolingua.com/en/)in Berlin. It's a *must* event for anyone interested in language learning and teaching, because for two days (**17-18 November**) it brings 150+ exhibitors (language schools, travel operators, cultural institutes, publishing houses, translators, associations) from 30+ countries and offers workshops, presentations, crash-courses in different languages, performances, and moderated panel discussions, from five categories: languages and cultures, presentations for teachers, learn & travel, tests, and career.\n\nSo on Friday morning, for a start, I attended two crash courses in Hebrew and Indonesian, where I learned to introduce myself and write a few words. Honestly, the lessons weren't really efficient because the courses took place in a corner next to a couple of exhibition stands, so the chatting in the background was disturbing and apart from that the instructors didn't bring enough handouts.  Nevertheless, I got a taste of these languages and I think I'll continue learning Hebrew! My next stop was a talk about the language and culture of Latin America, with a focus on Colombia, then a Henna workshop, which I enjoyed most.\n\nAfterwards I did a tour of the exhibitor stands, each luring visitors with goodie bags and colorful flyers about volunteering opportunities in Argentina, language trips in Japan, study programs in the UK, and many other countries. I discovered a cool app [Beelinguapp ](http://www.beelinguapp.com/)for learning languages with audiobooks, karaoke animations, and side-by-side reading; learned about the exam and registration process in the [German Association of Interpreters and Translators](http://www.bdue.de/); added some Spanish [crime comics](http://www.compactverlag.de/) to my reading list; learned a bit of [Ido](http://www.idolinguo.de/), a constructed language derived from Esperanto; \"visited\" Guatemala, the special guest country; and finally stopped for a coffee at ExpoCafé.\n\nAt the end of the day, I felt like I've traveled around the world in a couple of hours and it was inspiring to see the linguistic and cultural diversity of our world!"
    },
    {
      "id": "/2017/09/22/conference-student-research-berlin",
      "metadata": {
        "permalink": "/blog/2017/09/22/conference-student-research-berlin",
        "source": "@site/blog/2017-09-22-conference-student-research-berlin.md",
        "title": "Talking about the brains of interpreters @ 2nd Conference for Student Research in Berlin",
        "description": "I am typing this post from a coffee shop around Humboldt University of Berlin, sipping the fifth coffee today and leafing through my notes from the 2nd Conference for Student Research (Forschung Vermitteln-Communicating Research).",
        "date": "2017-09-22T00:00:00.000Z",
        "formattedDate": "September 22, 2017",
        "tags": [
          {
            "label": "linguistics",
            "permalink": "/blog/tags/linguistics"
          },
          {
            "label": "conferences",
            "permalink": "/blog/tags/conferences"
          }
        ],
        "readingTime": 2.1,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "post",
          "title": "Talking about the brains of interpreters @ 2nd Conference for Student Research in Berlin",
          "tags": [
            "linguistics",
            "conferences"
          ],
          "comments": true
        },
        "prevItem": {
          "title": "Expolingua Berlin",
          "permalink": "/blog/2017/11/17/expolingua-berlin"
        },
        "nextItem": {
          "title": "The effects of anesthesia on language",
          "permalink": "/blog/2017/09/11/effects-of-anesthesia-on-language"
        }
      },
      "content": "I am typing this post from a coffee shop around Humboldt University of Berlin, sipping the fifth coffee today and leafing through my notes from the [**2nd Conference for Student Research** (Forschung Vermitteln-Communicating Research)](https://www.hu-berlin.de/de/einrichtungen-organisation/verwaltung/bolognalab/zweite-konferenz-fuer-studentische-forschung). \n\n## Program\n\nIn the past two days (21-22.9.2017) I've attended 16 out of **80 talks** and **52 posters** of (under)graduate students from all over the world. The diverse programme included sessions on biology & health, film & media, environment, computer science, law, technology, sociology, and not least linguistics. I'm always eager to learn from other disciplines, so I skipped the language-related talks to attend some about winery, immunoglobin therapy for neuromuscular dystrophy, heart rate variability, the complex symptoms of Tourette syndrome, a sociological film analysis of *Her...*\n\n## My poster – exploring the brains of interpreters\n\nThe highlight for me was the first poster session, where I had the chance to expose my first **conference poster.** I presented my BA thesis about **what's going on the brain of simultaneous interpreters,** specifically what brain areas are activated in simultaneous interpreting (SI) and how it affects cognitive functions. \n\n![poster](blog_images/posterTranslatorsBrain.jpg)\n\nTo get the bigger picture, I did a systematic literature review and [ALE meta-analysis](https://www.brainmap.org/ale/) of neuroimaging studies carried on professional/long-term SIs. This revealed that SI into the native language engages the following brain areas:\n* the **prefrontal cortex** (responsible for planning and speech production)\n* the **fronto-parietal** regions\n* the **auditory cortex** (responsible for auditory attention and categorization)\n* the **right caudate nucleus** (involved in learning, motor control, and some domain-general executive functions). \n\nA surprising finding was the altered white matter in the most anterior and posterior parts of the **corpus callosum**, which connects and enables communication between the two hemispheres. \n\nThese findings indicate that for professional SIs the task of switching between languages becomes automated and they rely on more non-linguistic skills than on language control, but also that language proficiency causes plastic adaptations in regions involved in motor aspects of speech and in interhemispheric information transfer. In short, we could say that SI is not only about language, but also about sound and movement.\n\n\n## The end\n\nI think the best part of the conference was the poster session, when I had the opportunity to make contacts from all over the world, to exchange ideas and tips with like-minded people about research projects, funding opportunities or campus life at different institutions, but also about personal interests. \n\nOverall, the conference was well organized and the team created a nice atmosphere, even a small concert on the first evening. The next Conference for Student Research will be hosted by the University of Bochum around September 2018, I'm already looking forward to it!"
    },
    {
      "id": "/2017/09/11/effects-of-anesthesia-on-language",
      "metadata": {
        "permalink": "/blog/2017/09/11/effects-of-anesthesia-on-language",
        "source": "@site/blog/2017-09-11-effects-of-anesthesia-on-language.md",
        "title": "The effects of anesthesia on language",
        "description": "Imagine you go for a nap, only to wake up speaking in a foreign language!",
        "date": "2017-09-11T00:00:00.000Z",
        "formattedDate": "September 11, 2017",
        "tags": [
          {
            "label": "linguistics",
            "permalink": "/blog/tags/linguistics"
          }
        ],
        "readingTime": 2.88,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "post",
          "title": "The effects of anesthesia on language",
          "tags": [
            "linguistics"
          ],
          "comments": true
        },
        "prevItem": {
          "title": "Talking about the brains of interpreters @ 2nd Conference for Student Research in Berlin",
          "permalink": "/blog/2017/09/22/conference-student-research-berlin"
        },
        "nextItem": {
          "title": "Talking about semantic dementia @ 4th Linguistics Meetup in Berlin",
          "permalink": "/blog/2017/08/04/linguistik-meetup-berlin"
        }
      },
      "content": "**Imagine you go for a nap, only to wake up speaking in a foreign language!**\n\nIn 1987, a Turkish patient established in the USA for many years unerwent a surgery, after which he expressed himself by writing  or speaking in English (Akpek et al., 2002). But 24--28 hours later, he suddenly switched back to speaking Turkish. The surprising thing, for both the patient and the medical staff, was that the man apparently hasn't been aware of speaking English, though he could remember everything that had happened during that time.\n\nAnother \"intriguing case of transient language disturbance following anaesthesia\" was observed in a 54-year-old man undergoing a common surgery for knee joint injury in 1999 (Ward & Marshall, 1999). The patient had local analgesia in his left hand and general anaesthesia with midazolam 2 mg, propofol 180 mg and fentanyl 75 mg. After the 20-minute surgery, he was transferred to the recovery room. Shortly afterwards, the nurse alerted the doctors that the patient was speaking in Spanish, though apparently he could understand when he was spoken to in English. To make things even creepier, he then started having some twiching movements in one arm and staring at the ceiling, remaining unresponsive in either Spanish or English. All this time, all vital signs were normal. The anaesthetist was also puzzled and \"in an effort to at least appear to be doing something, he requested a blood sugar measurement\", which indicated low sugar level. The patient was administered sugar by the nurses, then started acting normally and speaking in English.\n\nA few years later, a 68-year-old Czechoslovakian man who had lived outside his country for years, but used to communicating in English, underwent a surgery during which, although awakened enough to open his eyes and hear the voices of the medical staff, he did not respond to their English commands to move his foot (Akpek et al., 2002).  \n\nSo, what is going on in this phenomenon known as **pathological language switching** (PLS)!? In the case described by Ward and Marshall (1999), the authors suppose that a temporal lobe seizure was induced by hypoglycemia and during the post-seizure state, the patient's normal speech was inhibited, allowing the second language to emerge. One possible explanation for switching from a foreign to the native language is that the native language is mostly stored in implicit memory systems of the subcortical regions, whereas acquired languages are learned by explicit rules and stored more diffusely in the cerebral cortex (Akpek et al., 2002). Another explanation is that the anesthetic might affect differently the neocortical areas representing the native (L1) and non-native language (L2), inhibiting L1 and facilitating L2 (Sharwood & Perry, 2005). However, this seems implausible because, although the L2 area in some bilinguals extends beyond the L1 area, the significance of this partial overlap is not clear.\n\nThe effects of anesthesia on linguistic centers in the brain seem to involve very complex mechanisms that are not yet understood, but case studies like these could shed some light on this phenomenon. And don't forget: **when things go wrong, make an effort to at least appear to be doing something**.\n\n* * * * *\n\nAkpek, E. A., Sulemanji, D. S., & Arslan, I. G. (2002). Effects of Anesthesia on Linguistic Skills: Can Anesthesia Cause Language Switches? *Anesthesia and Analgesia, 95, *1127*.*\n\nSharwood Smith, M. A. & Perry, R. (2005). Transient fixation on a non-native language associated with anaesthesia. *Anesthesia, 60,* 712-726.\n\nWard, M. E. & Marshall, J. C. (1999). 'Speaking in tongues'. Paradoxical fixation on a non-native language following anaesthesia. *Anaesthesia*, *54*, 1201--1203."
    },
    {
      "id": "/2017/08/04/linguistik-meetup-berlin",
      "metadata": {
        "permalink": "/blog/2017/08/04/linguistik-meetup-berlin",
        "source": "@site/blog/2017-08-04-linguistik-meetup-berlin.md",
        "title": "Talking about semantic dementia @ 4th Linguistics Meetup in Berlin",
        "description": "It's a hot morning on August 4th in Berlin, summer holidays are already on... But at the Institute of German Language and Linguistics of Humboldt Uni there's much ado about something: the 4th Linguistics Meetup Berlin-Potsdam!",
        "date": "2017-08-04T00:00:00.000Z",
        "formattedDate": "August 4, 2017",
        "tags": [
          {
            "label": "linguistics",
            "permalink": "/blog/tags/linguistics"
          },
          {
            "label": "conferences",
            "permalink": "/blog/tags/conferences"
          }
        ],
        "readingTime": 3.92,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "post",
          "title": "Talking about semantic dementia @ 4th Linguistics Meetup in Berlin",
          "tags": [
            "linguistics",
            "conferences"
          ],
          "comments": true
        },
        "prevItem": {
          "title": "The effects of anesthesia on language",
          "permalink": "/blog/2017/09/11/effects-of-anesthesia-on-language"
        },
        "nextItem": {
          "title": "Research findings about bilingualism in 2016",
          "permalink": "/blog/2016/12/21/bilingualism-research-2016"
        }
      },
      "content": "**It's a hot morning on August 4th in Berlin, summer holidays are already on... But at the [Institute of German Language and Linguistics](https://www.linguistik.hu-berlin.de/en/standardseite-en?set_language=en) of Humboldt Uni there's much ado about something: the [4th Linguistics Meetup Berlin-Potsdam](https://meetup.junge-sprachwissenschaft.de/)!**\n\n\nThis annual event brings together BA/MA/PhD students of Linguistics from Humboldt-, Free-, and Technical Universities of Berlin and Potsdam University, who present their research interests, projects, or dissertations. It's a great opportunity to get out of our own uni-caves, exhange ideas and get feedback from other students, while honing our presentation skills. \n\n## Program\n\nThe meetup doesn't set a theme, talks and posters are accepted from all branches of linguistics. Corpus analyses of 18th century texts and fundraising letters, learning/teaching German as second/foreign language, mnemonics for language learning, typology, voice therapy, medical and law terminology, and clinical linguistics were presented in the **4 posters and 12 talks** that made up this year's [programme](https://meetup.junge-sprachwissenschaft.de/dateien/Tagungsheft_LinguistikMeetup2017.pdf). \n\n## My talk\n\nI participated with a talk about the **impairment of concrete and abstract words in semantic dementia** (SD). My interest in this topic was sparked a couple of months ago in a seminar on the development of the mental lexicon. Since I'm fascinated by all (language) disorders, I chose to write the term paper on SD. This Meetup was a good occasion to discuss with other students some open questions in the literature and future research.\n\n### What is semantic dementia?\n\nFor a start, SD is a progressive neurodegenerative disease of Alzheimer's type (AD), caused by brain cells dying off. Unlike AD, SD appears earlier in life (45-65 years old), originating in the temporal lobes and more often in the left one, which is known to be reponsible for language comprehension. A person with SD would speak fluently, but wouldn't make a lot of sense, specifically they'd have difficulties naming or recognizing objects (e.g., see a cup and not know what it is or how to use it), and in time would use more general terms instead of specific names (e.g., *animal* instead of *cat* or *thing* for different objects).\n\n### Semantic dementia in language\n\nAn interesting aspect is that word categories seem to be differently impaired. SD patients are better at:\n* **non-living things** (e.g., *hammer*) than living things (e.g., *dog*), \n* **words from different categories** (e.g., *banana, nose*) than from the same category (e.g., *shoes, gloves*), \n* **concrete words** (e.g., *cabbage, bus*) than abstract words (e.g., *freedom, honesty*).\n\nThe last point is known as **concreteness effect** and holds also for the general population. Here however there are some curious cases of **reverse concreteness effects in SD**, where patients know abstract words better than concrete words, but up to date there's no clear explanation for this. Also unclear is to what extent are different categories impaired, for example whether *body parts* are better preserved than *tools*.\n\n### Cases of semantic dementia\n\nThe Monty Python star [**Terry Jones**](https://www.theguardian.com/society/2017/apr/16/monty-python-terry-jones-learning-to-live-with-dementia) was diagnosed with frontotemporal dementia in 2015, and at the Bafta ceremony 2016 he could only articulate \"quieten down\". \n\nAnother case study that impressed me is that of **Iris Murdoch**, the prolific British writer, best known for the novel *The sea, the Sea* which won the Booker Prize in 1978. However, her last novel *Jackson's Dilemma* (1995) was surprisingly weak and critics commented on her simple, at times incoherent writing style. Shortly afterwards, Iris Murdoch was diagnosed with AD, confirmed after her death in 1999. What does this have to do with literature? Apparently her \"poor writing style\" actually revealed signs of her cognitive decline. A team of [researchers](https://academic.oup.com/brain/article-lookup/doi/10.1093/brain/awh341) carried out a computer text analysis of three of her novels: *Under the Net* (1954), *The Sea, the Sea* (1978), and *Jackson's Dilemma* (1995). They found that the last novel had a limited vocabulary, with highly frequent and more repeated words, compared to the other two. These are characterictics of SD and AD, which unfortunately were not recognized at the time. \n\nThis study suggests how **creative writing** could be an insightful assessment of people with or even at risk of dementia. By analyzing various word features (e.g., concreteness, imageability, emotion, frequency, semantic category), along with research on the neural representation of words, could indicate earlier signs of different forms of dementia.\n\n## Conclusion\nAfter my 25-minute talk, it was time for more coffee, more networking, and a nice picnic in the park outside the Humboldt-Uni at the end of the day...\n\n---\n\nSome references:\n\n-   Cherkow, H. & Bub, D. (1990). Semantic memory loss in dementia of Alzheimer's type. *Brain, 113*, 397--417.\n-   Garrard, P. et al. (2005). The effects of very early Alzheimer's disease on the characteristics of writing by a renowned author. *Brain, 128*, 250--260.\n-   Hoffman, P. & Lambon Ralph, M. A. (2011). Reverse concreteness effects are not a typical feature of semantic dementia: Evidence for the hub-and-spoke model of conceptual representation. *Cerebral Cortex, 21*(9), 2103--2112.\n-   Jefferies, E. et al. (2009). Comprehension of concrete and abstract words in semantic dementia. *Neuropsychology, 23*, 492--499."
    },
    {
      "id": "/2016/12/21/bilingualism-research-2016",
      "metadata": {
        "permalink": "/blog/2016/12/21/bilingualism-research-2016",
        "source": "@site/blog/2016-12-21-bilingualism-research-2016.md",
        "title": "Research findings about bilingualism in 2016",
        "description": "Nowadays the majority of the world's population is bilingual/multilingual, mainly due to English as lingua-franca and international policies on foreign language education. In the past year many researchers have been investigating the cognitive aspects of foreign language learning and bilingualism, and here are some of their discoveries.",
        "date": "2016-12-21T00:00:00.000Z",
        "formattedDate": "December 21, 2016",
        "tags": [
          {
            "label": "linguistics",
            "permalink": "/blog/tags/linguistics"
          }
        ],
        "readingTime": 2.445,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "post",
          "title": "Research findings about bilingualism in 2016",
          "tags": [
            "linguistics"
          ],
          "comments": true
        },
        "prevItem": {
          "title": "Talking about semantic dementia @ 4th Linguistics Meetup in Berlin",
          "permalink": "/blog/2017/08/04/linguistik-meetup-berlin"
        },
        "nextItem": {
          "title": "Xenolinguistics by D.R. Slattery",
          "permalink": "/blog/2016/12/21/xenolinguistics-book-review"
        }
      },
      "content": "**Nowadays the majority of the world's population is bilingual/multilingual, mainly due to English as lingua-franca and international policies on foreign language education. In the past year many researchers have been investigating the cognitive aspects of foreign language learning and bilingualism, and here are some of their discoveries.**\n\nLearning a foreign language is known to be an efficient (and fun) way to train your brain. Even just a[ one-week](https://www.sciencedaily.com/releases/2016/04/160427164131.htm) crash-course can boost your cognitive abilities (such as attention span) and if you want to maintain and enhance them -- keep on [learning regularly](https://www.sciencedaily.com/releases/2016/04/160427164131.htm)! But while some people seem to have a talent for learning foreign languages, others struggle for years to reach fluency. [It turns out ](https://www.sciencedaily.com/releases/2016/01/160120202512.htm)that there are indeed some innate neural aspects which support language learning, which can be seen in resting-state connectivity: strong connections between the left anterior insula/frontal operculum and the left superior temporal gyrus enhance speaking proficiency, while strong connectivity between the visual word form area and a different area of the left superior temporal gyrus in the left temporal lobe enhance reading comprehension. Moreover, [variations of the COMT gene](https://www.sciencedaily.com/releases/2016/06/160613153358.htm) can alter the white matter as a result of language learning. If you're not among the lucky ones with a \"polyglot brain\", you could try [neurosensory auditory stimulation ](https://www.sciencedaily.com/releases/2016/03/160321081417.htm)to reset your ears back to the \"critical period\" of language learning. Learning a new language is worth it not only for international communication, but also for training your [brain's elasticity and its ability to process information](https://www.sciencedaily.com/releases/2016/09/160902111425.htm) in the long term. If you're curious to see what actually happens in the brain when learning a language, researchers have managed for the first time to [capture images ](https://www.sciencedaily.com/releases/2016/06/160629125943.htm)of linguistic information being integrated into the same brain areas for the native language.\n\nIf learning a foreign language seems challenging, think about juggling two (or more) languages in everyday conversations. Early bilinguals, who learn two languages in parallel from birth on, are able to [learn the rules](https://www.sciencedaily.com/releases/2016/05/160516091843.htm) of each language faster than monolinguals. The effects of early bilingualism on the brain can be [observed early on](https://www.sciencedaily.com/releases/2016/04/160404141743.htm): 11 month-olds present brain activity associated with executive functioning, and it turns out they are more open to learning new language sounds. Babies' [executive function](https://www.sciencedaily.com/releases/2016/02/160203110936.htm) also seems to be improved by the age of 3 due to early bilingualism. The cognitive advantages of bilinguals are a hot topic and this year many studies have contributed to its research. [It has been confirmed](https://www.sciencedaily.com/releases/2016/02/160213185925.htm) that bilinguals can control better competing cognitive tasks and show [better attention span and ability to focus](https://www.sciencedaily.com/releases/2016/09/160909112256.htm). Language switching in conversations seems to come easily for bilinguals, and [an explanation](https://www.sciencedaily.com/releases/2016/04/160407083739.htm) is consistency in using one word from one languge for a specific noun, which allows them to avoid the cognitive costs of language switching. [Even speaking two dialects](https://www.sciencedaily.com/releases/2016/04/160427151051.htm) seems to confer the same cognitive advantages as those observed in bilinguals of different languages. New insights into the neural underpinnings of bilingualism show that speakers [use different neural networks](https://www.sciencedaily.com/releases/2016/10/161026113837.htm) for reading transparent languages (pronounced as they are written) and opaque languages (pronounced different than written).\n\nResearch in this field is advancing at a fast pace and here I have mentioned only a fraction of the studies published this year on the topic. Stay tuned for future discoveries!"
    },
    {
      "id": "/2016/12/21/xenolinguistics-book-review",
      "metadata": {
        "permalink": "/blog/2016/12/21/xenolinguistics-book-review",
        "source": "@site/blog/2016-12-21-xenolinguistics-book-review.md",
        "title": "Xenolinguistics by D.R. Slattery",
        "description": "In *Xenolinguistics*, Diana Reed Slattery mixes research on language, psychedelics, and consciousness in a trippy cocktail which will warp your mind like the characters of Glide, the symbolic language she “downloaded” in an altered state of consciousness.",
        "date": "2016-12-21T00:00:00.000Z",
        "formattedDate": "December 21, 2016",
        "tags": [
          {
            "label": "linguistics",
            "permalink": "/blog/tags/linguistics"
          },
          {
            "label": "books",
            "permalink": "/blog/tags/books"
          }
        ],
        "readingTime": 1.34,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "post",
          "title": "Xenolinguistics by D.R. Slattery",
          "tags": [
            "linguistics",
            "books"
          ],
          "comments": true
        },
        "prevItem": {
          "title": "Research findings about bilingualism in 2016",
          "permalink": "/blog/2016/12/21/bilingualism-research-2016"
        },
        "nextItem": {
          "title": "Learning about languages of trauma",
          "permalink": "/blog/2016/11/28/conference-languages-of-trauma"
        }
      },
      "content": "In ***Xenolinguistics*, Diana Reed Slattery** mixes research on language, psychedelics, and consciousness in a trippy cocktail which will warp your mind like the characters of Glide, the symbolic language she “downloaded” in an altered state of consciousness. \n\nAmong the topics discussed in the book are psychonautic practices, the concept of reality and extended perception, neurophenomenology, (un)natural languages, and their relation with culture and nature. The author provides a vibrant account of altered states of consciousness, supported by the works of psychologists, philosophers, psychonauts (I think Terrence McKenna is the most cited) and most importantly by personal introspective reports. \n\nA personal highlight was to see cited two intellectual Romanians way ahead of their time: Mircea Eliade and Basarab Nicolescu – the former best known for his comparative study of religions, the latter for his transdisciplinary approach and the Law of the Included Middle, presented in detail in chapter 5. To enhance the visual journey, the pages include images of the Glide symbols, the artworks of Alex and Allyson Grey, Slattery’s own paintings, and other symbolic images. It was even more exciting to read the recent study by Robin Carhart-Harris et al., who showed for the first time the effects of LSD on the brain, thanks to brain-scanning techniques. \n\nAs for the linguistic part, Neiloufar Family et al. showed that “LSD seems to effect the mind’s semantic networks, or how words and concepts are stored in relation to each other. When LSD makes the network activation stronger, more words from the same family of meanings come to mind.” \n\nThis book offers an enriching experience to artists, psychonauts, and scholars alike."
    },
    {
      "id": "/2016/11/28/conference-languages-of-trauma",
      "metadata": {
        "permalink": "/blog/2016/11/28/conference-languages-of-trauma",
        "source": "@site/blog/2016-11-28-conference-languages-of-trauma.md",
        "title": "Learning about languages of trauma",
        "description": "The conference Languages of trauma, which took place between 25-26 November in Berlin, brought together scholars of cultural/film/media studies to discuss about the body/psyche, historiography, traumatology, and visual media -- and everything in between. Here I will talk only about two of the seven talks, which made a bigger impression on me -- namely on the theme Audio-visualization of trauma and history of (psycho)traumatology, jointly discussed by Julia Köhne and Anne Freese.",
        "date": "2016-11-28T00:00:00.000Z",
        "formattedDate": "November 28, 2016",
        "tags": [
          {
            "label": "linguistics",
            "permalink": "/blog/tags/linguistics"
          },
          {
            "label": "conferences",
            "permalink": "/blog/tags/conferences"
          }
        ],
        "readingTime": 3.46,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "post",
          "title": "Learning about languages of trauma",
          "tags": [
            "linguistics",
            "conferences"
          ],
          "comments": true
        },
        "prevItem": {
          "title": "Xenolinguistics by D.R. Slattery",
          "permalink": "/blog/2016/12/21/xenolinguistics-book-review"
        },
        "nextItem": {
          "title": "Learnings from the 10th Meeting of Patholinguistics in Potsdam",
          "permalink": "/blog/2016/11/23/patholinguistics-meeting-potsdam"
        }
      },
      "content": "The [conference **Languages of trauma**](https://www.culture.hu-berlin.de/de/institut/kollegium/1688201/veranstaltungen/flyer_languages-of-trauma-berlin-25-26-nov-2016.pdf), which took place between 25-26 November in Berlin, brought together scholars of cultural/film/media studies to discuss about the body/psyche, historiography, traumatology, and visual media -- and everything in between. Here I will talk only about two of the seven talks, which made a bigger impression on me -- namely on the theme **Audio-visualization of trauma and history of (psycho)traumatology,** jointly discussed by **Julia Köhne** and **Anne Freese.**\n\nFirst, **Julia Köhne** talked about PTSD in soldiers of the Second World War, on the basis of video medical documentation from that time. One recording showed a post-war soldier on a mattress in an examining room having what seemed like convulsions. His movements were interpreted as re-enactments of the scenes he lived in the war, including re-enactments of the victims, which arguably represent the guilt of the soldier. It is also relevant that at that point, after the war the soldier was minimally verbal. In a way, it appears as though the body took over.\n\nThis illustrates the limitation of verbal language, in that while words and grammar provide us the tools to express thoughts, feelings, real or imaginary experiences, there is a finite number of words, which define things/situations/feelings/concepts existent in, or \"confirmed\" by the culture/society. We have words like \"very\", \"extremely\", \"fantastic\", \"horrible\" etc., but they might be insufficient for the description of a high impact experience. Then how do you describe something for which there is no word? And if you can't even \"put it into words\", how do you validate your experience?\n\nThere is a concept of \"Language of the unspeakable\", primarily grounded in psychonautics, which refers to the impossibility of a psychonaut to put into words the experience lived in an altered state of consciousness. In this sense, the war experience might be similar. Of course, new words can be created, but their definition still has to resort to already existent words. Or, as in the presented video, the experience is re-enacted (and possibly given a name). In any case, the impact of trauma on language is an interesting topic.\n\n**Anne Freese** took the story further and talked about how PTSD came into being, specifically PTSD in the context of the Vietnam War movement. Even though the term \"trauma\" was well-known in those times, the concept of PTSD was not yet established. But an alarming number of soldiers were experiencing (and reporting) symptoms of trauma and stress after the end of the war. This was a signal for some medical doctors, who took the initiative to clearly define this condition as PTSD and include it in the DSM.\n\nThe interesting point is that initially a differentiation between PTSD caused by catastrophies generated by humans or by the environment was considered. However, the APA chose to not include this explicit differentiation when the term PTSD was first introduced in the DSM-III in 1980. In time, the medical perspective on PTSD shifted from anatomical to psychological, and the diagnostic criteria revised in every edition. For example, the term \"complex PTSD\" is not recognized by the DSM, but will be included in the ICD-11.\n\nThis raised the question whether it is right to leave the term \"trauma\" to be defined by the medical community, and especially by the DSM, which is increasingly [dominating ](http://www.tandfonline.com/doi/full/10.1080/19419899.2015.1024470)the medical discourse. The implication is that also the concept of (ab)normality is (re)defined, which directly affects, in turn, the social discourse about the diagnosed individuals and their acceptance in the society. Michel Foucault notes how this system works: \"...if you are not like everybody else, then you are abnormal, if you are abnormal, then you are sick.\n\nThese three categories, not being like everybody else, not being normal, and being sick are in fact very different but have been reduced to the same thing\". Efforts to change are made by (self-)advocacy groups. One example is the \"Drop the D\" movement, which aims to get the term \"disorder\" removed from the name of conditions such as PTSD or [ASD](https://www.autismspectrum.org.au/dropthedfromASD), because it stigmatizes the diagnosed individuals, defining them as \"disordered\".\n\nAn interesting idea expressed by **Thomas Elsaesser** at the end was that trauma arises from socially raised expectations that are suddenly violated (something along the lines: \"You think it's inconceivable, then you have 9/11.\")"
    },
    {
      "id": "/2016/11/23/patholinguistics-meeting-potsdam",
      "metadata": {
        "permalink": "/blog/2016/11/23/patholinguistics-meeting-potsdam",
        "source": "@site/blog/2016-11-23-patholinguistics-meeting-potsdam.md",
        "title": "Learnings from the 10th Meeting of Patholinguistics in Potsdam",
        "description": "Waking up at 6 a.m. on a Saturday, dressing up to step out in a freezing morning of November, grabbing quickly a double espresso and hopping on a train... only motivated to attend the 10th Herbsttreffen Patholinguistik, an annual event about research on neurology, methods, and therapy of various speech-language impairments (e.g., dyslexia, aphasia, dysphagia, specific language impairment) which takes place at Campus Griebnitzsee of Potsdam University, Germany. This year's theme was \"Linguistics meets speech-language therapy\", covered in talks, posters, workshops, and a photo exhibition.",
        "date": "2016-11-23T00:00:00.000Z",
        "formattedDate": "November 23, 2016",
        "tags": [
          {
            "label": "linguistics",
            "permalink": "/blog/tags/linguistics"
          },
          {
            "label": "conferences",
            "permalink": "/blog/tags/conferences"
          }
        ],
        "readingTime": 4.36,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "post",
          "title": "Learnings from the 10th Meeting of Patholinguistics in Potsdam",
          "tags": [
            "linguistics",
            "conferences"
          ],
          "comments": true
        },
        "prevItem": {
          "title": "Learning about languages of trauma",
          "permalink": "/blog/2016/11/28/conference-languages-of-trauma"
        },
        "nextItem": {
          "title": "Plaidoyer pour l’écriture manuscrite",
          "permalink": "/blog/2015/05/20/plaidoyer-pour-ecriture-manuscrite"
        }
      },
      "content": "Waking up at 6 a.m. on a Saturday, dressing up to step out in a freezing morning of November, grabbing quickly a double espresso and hopping on a train... only motivated to attend the [10th Herbsttreffen Patholinguistik](https://www.herbsttreffen.vpl-ev.de/), an annual event about research on neurology, methods, and therapy of various speech-language impairments (e.g., dyslexia, aphasia, dysphagia, specific language impairment) which takes place at Campus Griebnitzsee of Potsdam University, Germany. This year's theme was \"**Linguistics meets speech-language therapy**\", covered in talks, posters, workshops, and a photo exhibition. \n\n## Workshop\n\nI attended the workshop **\"Diagnosis of specific language impairment (SLI) in multilingual children\"** held by the speech-language therapist Dorothea Posse. SLI is a disorder in which a child doesn't develop language typically, although the overall development is normal, so the language problems aren't due to physical impairments of the speech apparatus, IQ, hearing loss, brain damage, or developmental disorders (e.g., ASD*).\n\nIt can be tricky to  identify SLI in the beginning, because children obviously would make mistakes in the process of learning a language, like in any other skill. The most common errors (in German) are in marking gender (e.g., *he* instead of *she*) and case (e.g., *him/his*), subject-verb agreement (e.g., *he eat*), use of auxiliary verbs (e.g., *have finished*), irregular verbs (e.g., *eated* instead of *ate*), prepositions, and a limited vocabulary (using more general terms than specific words). If the child makes some errors consistently for a long time, at school age or after the age by which she is supposed to have learned the specific grammar rules, and generally speaks in short, simple sentences, these could be **clinical markers** for SLI. \n\nBut not all children who regularly make these errors have SLI. These problems are very common in **normal bilingual acquisition**, when the two languages have different structures (e.g., word order is SVO in English but SOV in Turkish; the verb *run* is irregular in English, but regular in German). Actually, if you're adult learning a foreign language you probably make the same errors, so in this you're like a big baby trying to acquire language (again). Language errors in mono-/multilingual children can also be due to **environmental factors**, e.g., if at home the parents speaks incorrectly, or if children don't get enough language input in kindergarten. In these two cases, the problems can be solved with qualitative language training (e.g. talking/reading more  to the child, correcting his errors instantly), therapy is not necessary.\n\nHowever, **SLI** requires speech-language therapy. In diagnosing SLI, there are two possible misdiagnoses: **missed identity** (when the diagnosis is normal, but the child actually has SLI) and **mistaken identity** (when the child is diagnosed with SLI, but actually his language is normal). Bilingual children who learn the two languages simultaneously acquire them by the same pattern as monolingual children, so don't worry, the second language doesn't take over the first one, all languages share the cognitive resources. \n\n\nThe **assessment** of SLI in multilingual children (usually between 3 and 7 years old) includes first an **interview** with the parents and caregivers about the age of the first spoken words in each language, the environment, time, quality and quality of the languages that the child is exposed to, family history of language impairments or disorders. The second part is a **language test** for the child which evaluates the comprehension of the meaning of verbs, wh-questions, negations, picture stories testing the use of relative clauses, subject-verb-agreement, word classes, and case marking. The results on the test are compared with the means for typically developed bilingual children. If the child underscored significantly on more than two parts of the test, SLI is diagnosed.\n\n## Poster session\n\nLater in the poster session of the conference I learned about a study by G. Bruno, F. Adani, and R. Lassotta who investigated the comorbidity of ASD and LI. Children with typical development, with SLI and with ASD with/without LI were shown pictures of three animals (e.g., a deer tickling a frog tickling another deer) and were asked to identify one of the animals, either in subject relative clauses (SRC, e.g., *Wo ist das Reh, das den Frosch kitzelt? */ *Where is the deer that tickles the frog?*) or in object relative clauses (ORC, e.g., *Wo ist das Reh, das der Frosch kitzelt? */ *Where is the deer that the frog tickles?*) by pressing a red button (left animal) or blue button (right animal). ORC are more difficult and are processed slower than SRC, because they require more working memory to mentally rearrange the subject and object in the language specific word order. Their results showed that children with ASD and LI answered only ~20% of ORC correctly, compared to children with ASD without LI who answered ~90%. This suggests that ASD alone is not responsible for language difficulties, but LI is. (As a side note, the poster won the first prize at the conference and actually inspired a group project I did for a uni course a couple of months later 🙂 )\n\n---\n\n**Bibliography** recommended by Dorothea Posse:\n\n-   Armon-Lotem, S. & de Jong, J. (2015). *Assessing multilingual children: Disentangling Bilingualism from Language Impairment* (Vol. 13). Multilingual Matters.\n-   Grimm, A. & Schulz, P. (2014). Specific language impairment and second language acquisition: The risk of over- and underdiagnosis. *Child Indicators Research, 7*(4), 821-841.\n-   Rothweiler, M., Chilla, S., & Clahsen, H. (2012). Subject-verb agreement in specific language impairment: A study of monolingual and bilingual German-speaking children. *Bilingualism: Language and Cognition, 15*(1), 39-57.\n-   Snippe, K. (2015). *Autismus*: *Wege in die Sprache*. Schulz-Kirchner."
    },
    {
      "id": "/2015/05/20/plaidoyer-pour-ecriture-manuscrite",
      "metadata": {
        "permalink": "/blog/2015/05/20/plaidoyer-pour-ecriture-manuscrite",
        "source": "@site/blog/2015-05-20-plaidoyer-pour-ecriture-manuscrite.md",
        "title": "Plaidoyer pour l’écriture manuscrite",
        "description": "La suppression de l’apprentissage de l’écriture manuscrite dans les écoles en Finlande a déclanché un débat animé dans l’Europe entière. Il est évident que depuis l’utilisation de l’ordinateur, l’écriture cursive disparaît peu à peu au profit des claviers. Mais devons-nous capituler face à la technologie et demander aux enfants de refermer les cahiers et ouvrir les laptops ?",
        "date": "2015-05-20T00:00:00.000Z",
        "formattedDate": "May 20, 2015",
        "tags": [
          {
            "label": "linguistics",
            "permalink": "/blog/tags/linguistics"
          },
          {
            "label": "francais",
            "permalink": "/blog/tags/francais"
          },
          {
            "label": "thoughts",
            "permalink": "/blog/tags/thoughts"
          }
        ],
        "readingTime": 1.555,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "post",
          "title": "Plaidoyer pour l’écriture manuscrite",
          "tags": [
            "linguistics",
            "francais",
            "thoughts"
          ],
          "comments": true
        },
        "prevItem": {
          "title": "Learnings from the 10th Meeting of Patholinguistics in Potsdam",
          "permalink": "/blog/2016/11/23/patholinguistics-meeting-potsdam"
        }
      },
      "content": "La suppression de l’apprentissage de l’écriture manuscrite dans les écoles en Finlande a déclanché un débat animé dans l’Europe entière. Il est évident que depuis l’utilisation de l’ordinateur, l’écriture cursive disparaît peu à peu au profit des claviers. Mais devons-nous capituler face à la technologie et demander aux enfants de refermer les cahiers et ouvrir les laptops ? \n\nL’écriture manuscrite est premièrement une expression de la personnalité individuelle, un acte intime, qui peut offrir des indices sur le caractère d’une personne. La supression (in)voluntaire de la prise de note manuelle aboutirait à l’uniformisation des enfants et, encore plus grave, à l’inhibition de leur originalité.\n\nDeuxièmement, la technologie est omniprésente dans nos vies. Nous sommes la génération numérique, affrontée de toute part par des écrans. J’obsèrve que le contact avec le monde n’est plus exploratif, mais intermedié par des touches de commandes. Le cache-cache a été remplacé par Angry Birds, les jeux imaginatifs perdent de terrain face aux applications pour les tablettes. L’écriture exclusivement sur le clavier ne ferait que renforcer la dépendance à la technologie, qui peut avoir des répercussions serieuses sur les enfants.\n\nA fortiori, la science est de mon côté. Je suis consciente du fait que l’écriture manuscrite est déjà pour beaucoup d’enfants une corvée, qui apparement ralenti leur flux d’idées. Toutefois, le syndrome de la page blanche invoqué par les petits écrivains est une excuse audacieuese. En fait, les études scientifiques en prouvent le contraire : le processus de former et d’attacher des lettres entraîne la motricité fine. De plus, en écrivant à main plusieurs zones du cerveau sont activées qu’en tapant sur une clavier, et par conséquant l’activité cérébrale est plus intensive.\n\nEn conclusion, l’importance de la forme traditionelle de communication, l’écriture manuscrite, ne doit pas être sous-estimée. Reléguer la plume et le cahier aux oubliettes, c’est se sousmettre à la technologie, tout en robotisant nos descendants."
    }
  ]
}