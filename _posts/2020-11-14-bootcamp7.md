---
layout: post
title: Building a Slackbot for live tweets with Python
share-description: Learn how to build a dockerized ETL pipeline with Postgres, MongoDB, and analyze tweets sentiment with Python.
gh-repo: lorenanda/tweets-docker-pipeline
gh-badge: [star, fork, follow]
tags: [data science, bootcamp, projects]
comments: true
---

>Project completed in week 7 (09.11.-13.11.20) of the Data Science Bootcamp at Spiced Academy in Berlin.

This week's project was the most complex and difficult so far. The challenge was to create a database of tweets, along with their sentiment score, and post tweets in a Slack channel. This pipeline had to be done with [Docker Compose](https://docs.docker.com/compose/) and included six steps:

1. Collect tweets with the hashtag *#OnThisDay*
2. Store the collected tweets in a [MongoDB](https://www.mongodb.com/) database
3. Extract tweets from the database
4. Process the tweets (clean the text, analyse sentiment)
5. Load the cleaned tweets and their sentiment score in a [Postgres](https://www.postgresql.org/) database
6. Extract and post tweets with positive sentiment in a [Slack](https://slack.com/intl/en-de/) channel

![](https://lorenaciutacu.files.wordpress.com/2020/11/20201113_1724016109705519843710741-e1605456158415.jpg?w=1024)

In this post, I'll show you how I set up each step.

<details>
    <summary><strong>Table of contents</strong></summary>
    <a href= "#creating-a-docker-compose-pipeline">Creating a Docker-Compose pipeline</a><br>
    <a href= "#collecting-tweets">Collecting tweets</a><br>
    <a href= "#storing-tweets-in-mongodb">Storing tweets in MongoDB</a><br>
    <a href= "#etl-ing-tweets-from-mongodb-to-postgres">ETL-ing tweets from MongoDB to Postgres</a><br>
    <a href= "#creating-a-slackbot">Creating a Slackbot</a><br>
    <a href= "#friday-lightning-talk">Friday Lightning Talk/a>
</details>

## Creating a Docker-Compose pipeline

The image above represents the pipeline I had to build with [Docker-Compose](https://docs.docker.com/compose/). Each rectangle represents a container, i.e. a part of the whole project. I had a pipeline with five containers: `tweet_container`, `postgres_container`, `mongo_container`, `etl_container`, and `slackbot_container`.

```yml
version: '3'

services:
  tweet_container:
    build: tweet_collector/
    depends_on:
      - postgres_container
      - mongo_container
    volumes:
      - ./tweet_collector/:/app
  
  postgres_container:
    build: postgresdb
    image: postgres:13.0
    ports:
      - 5555:5432
    environment:
      - POSTGRES_USER=your_user
      - POSTGRES_PASSWORD=your_password

  mongo_container:
    build: mongodb
    image: mongo
    ports:
      - 27018:27018
    volumes:
      - ./mongodb:/app
  
  etl_container:
    build: etl_job/
    depends_on:
      - postgres_container
      - mongo_container
    volumes:
      - ./etl_job/:/app

  slackbot_container:
    build: slackbot/
    depends_on:
      - mongo_container
      - postgres_container
    volumes:
      - ./slackbot/:/app
```
## Collecting tweets

[](https://github.com/lorenanda/tweets-docker-pipeline/tree/main/docker-compose/tweet_collector)

The first step was to collect tweets using the [Twitter API](https://developer.twitter.com/en/docs/twitter-api) and to process them with the [tweepy](https://www.tweepy.org/) library. It took me only a couple of minutes to register an app on Twitter and get my credentials (API key and access token). Then, I wrote a function in Python to stream live tweets with the hashtag *#OnThisDay* and collect the tweet text, the user's handle, their location, and description.

```python
from tweepy import OAuthHandler, Stream, API
from tweepy.streaming import StreamListener
tweet = {
    'username': t['user']['screen_name'],
    'text': t['text'],
    'followers_count': t['user']['followers_count'],
    'location':t['user']['location'],
    'description':t['user']['description']
}

stream_listener = StreamListener()
stream = tweepy.Stream(auth=api.auth, listener=stream_listener)
stream.filter(track=['OnThisDay'])
```

## Storing tweets in MongoDB

This step builds the second Docker container in my pipeline. The collected tweet information is then stored in MongoDB using the [pymongo](https://pymongo.readthedocs.io/en/stable/) library. MongoDB is a non-relational database (no-SQL) that stores data in JSON-like documents. Since the tweet data is collected as key-value pairs, MongoDB is a good way to store this information.

MongoDB is a non-relational database (NoSQL) that stores data in JSON-like documents. Since our tweets are returned in JSON format, MongoDB is the ideal database to store them in and the **MongoDB node** allows us to connect to the database. Before configuring the node, you need to create a MongoDB instance, set up a cluster, create a database and a collection within it.

1.  [Create a MongoDB account](https://account.mongodb.com/account/register)
2.  Set up a cluster: *cloud.mongodb.com  Clusters  Create New Cluster*
3.  Create a database: *Cluster  Collections  Create Database*
4.  Create a collection: *Cluster  Collections  Database  Create Collection*
5.  Create a field: *Collection  Insert document  Type the field "text" below "_id"*
6.  Allow access to the database: *Project  Security  Network Access  IP Access List  Add your IP address.*
7.  Connect to the database from your terminal:\
    *mongo "mongodb+srv://YourClusterName.mongodb.net/YourDatabaseName" --username YourUsername*

## ETL-ing tweets from MongoDB to Postgres

ETL stands for Extract, Transform, Load -- basically copying data from one or more sources into a destination system which represents the data differently from the source. This process represents the third container in my pipeline. 

[see here](https://github.com/lorenanda/tweets-docker-pipeline/tree/main/docker-compose/etl_job)

### Extracting tweets from MongoDB
To extract the tweet texts from MongoDB, I used the library `pymongo`.

```python
def extract_tweets():
    tweets  = list(db.onthisday.find())
    if tweets:
        t = random.choice(tweets)
        logging.critical("Random tweet: "+t["text"])
        return t
```
### Transforming tweets with sentiment scores

```python
def transform_tweets(tweet):
    #clean text
    tweet_text = tweet['text'].replace("\'","")

    #sentiment analysis
    sia = SentimentIntensityAnalyzer()
    tweet_sia = sia.polarity_scores(tweet_text)['compound']
    tweet_blob = TextBlob(tweet_text).sentiment
    return tweet_sia
```
### Loading tweets into PostgreSQL

```python
def load_tweets(tweet, sentiment):
    insert_query = """
    INSERT INTO tweets VALUES ('{tweet["text"]}', {tweet_sia});
    """
    engine.execute(insert_query)
    # Print out the extracted tweet
    logging.critical(f'Tweet {tweet["text"]} loaded into Postgres.')
```

 then evaluated the sentiment score of each tweet with [VADER](https://pypi.org/project/vaderSentiment/) and [TextBlob](https://textblob.readthedocs.io/en/dev/index.html), and finally loaded this new data (tweet and sentiment score) into a Postgres database using [SQLAlchemy](https://www.sqlalchemy.org/).

Next, we want to insert the newly set data values into a Postgres database. First, you need to [install Postgres](https://www.postgresql.org/download/), then create a database and a table for tweets. The process is quite similar to the MongoDB setup and you can do this from your terminal:

1.  Connect to Postgres: `psql`
2.  Create a database: `createdb twitter`
3. Go into the created database: `psql twitter`
4. Create columns in the database. The columns have to be named like the values defined in the Set node, in order to be matched. `CREATE TABLE tweets (text varchar(280), score numeric(4,3), magnitude numeric(4,3));`

## Creating a Slackbot
[](https://github.com/lorenanda/tweets-docker-pipeline/tree/main/docker-compose/slackbot)

This was the coronation of a week's work: making a shareable product out of the data engineering process. I followed the helpful instructions in [this article](https://slack.com/intl/en-de/help/articles/115005265703-Create-a-bot-for-your-workspace) and, after some time of tweaking the connection to the Postgres tweets database, my bot sent a message in our #channel about a historical event that happened on that day.

[![](https://lorenaciutacu.files.wordpress.com/2020/11/20201115_1759485409966206226208131.jpg?w=734)](https://lorenaciutacu.files.wordpress.com/2020/11/20201115_1759485409966206226208131.jpg)

But don't get excited too fast! I still have some things to figure out, like how to make the bot include the sentiment score along with the tweet, how to schedule it to post in a given time interval, and maybe how to pick only the last or most positive tweet. I'll try to solve these issues using [Airflow](https://airflow.apache.org/) and dive deeper into data engineering. Though it's a complicated process, I realize I really enjoyed building data pipelines!

## Friday Lightning Talk

This week I focused on the Transform part of the project, which was about Sentiment Analysis. When we learned about the VADER library, some colleagues asked how the training data is collected and who rates the training texts. To answer these questions, I decided to present one of my dearest personal projects in which I did exactly that: I created a list of emotion verbs and asked native speakers to rate their valence, arousal, and duration. You can read more about this project (and data analysis) [on GitHub](https://github.com/lorenanda/psych-verbs) and in this [blog post]({ poster-linguistik-meetup-potsdam/ }) from a student conference.