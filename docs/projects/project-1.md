---
sidebar_position: 1
---

# Speech emotion recognition

:::note this project boils down to
neural networks, voice processing, and drama
:::

![github stars](https://img.shields.io/github/stars/lorenanda/speech-emotion-recognition?style=social)
![github forks](https://img.shields.io/github/forks/lorenanda/speech-emotion-recognition?style=social)
![github last commit](https://img.shields.io/github/last-commit/lorenanda/speech-emotion-recognition?style=social)

## Tech stack 

![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)
![tensorflow](https://img.shields.io/badge/TensorFlow-FF6F00?style=for-the-badge&logo=tensorflow&logoColor=white)
![pandas](https://img.shields.io/badge/pandas-%23150458.svg?style=for-the-badge&logo=pandas&logoColor=white)
![scikitlearn](https://img.shields.io/badge/scikit--learn-%23F7931E.svg?style=for-the-badge&logo=scikit-learn&logoColor=white)
![googlecolab](https://img.shields.io/badge/Colab-F9AB00?style=for-the-badge&logo=googlecolab&color=525252)

## How it works

![project image](https://github.com/lorenanda/speech-emotion-recognition/raw/main/speech_emotion_recognition/images/project_pipeline.png)

## Next steps

- [ ] Try other models (not necessarily neural networks).
- [ ] Extract other audio features to see if they are better predictors than the MFCC.
- [ ] Train on larger data sets, since 1500 files and only 200 samples per emotion is not enough.
- [ ] Train on natural data, i.e. on recordings of people speaking in unstaged situations, so that the emotional speech sounds more realistic.
- [ ] Train on more diverse data, i.e. on recordings of people of different cultures and languages. This is important because the expression of emotions varies across cultures and is influenced also by individual experiences.
- [ ] Combine speech with facial expressions and text (speech-to-text) for multimodal sentiment analysis.
